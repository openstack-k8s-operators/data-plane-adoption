= Development environment

The Adoption development environment utilizes
https://github.com/openstack-k8s-operators/install_yamls[install_yamls]
for CRC VM creation and for creation of the VM that hosts the source
Wallaby (or OSP 17.1) OpenStack in Standalone configuration.

== Environment prep

Get dataplane adoption repo:
[,bash]
----
git clone https://github.com/openstack-k8s-operators/data-plane-adoption.git ~/data-plane-adoption
----

Get install_yamls:

[,bash]
----
git clone https://github.com/openstack-k8s-operators/install_yamls.git ~/install_yamls
----

Install tools for operator development:

[,bash]
----
cd ~/install_yamls/devsetup
make download_tools
----

== CRC deployment
=== CRC environment with network isolation

[,bash]
----
cd ~/install_yamls/devsetup
PULL_SECRET=$HOME/pull-secret.txt CPUS=12 MEMORY=40000 DISK=100 make crc

eval $(crc oc-env)
oc login -u kubeadmin -p 12345678 https://api.crc.testing:6443

make crc_attach_default_interface
----

'''

=== CRC environment with Openstack ironic

[IMPORTANT]
This section is specific to deploying Nova with Ironic backend. Skip
it if you want to deploy Nova normally.

Create the BMaaS network (`crc-bmaas`) and virtual baremetal nodes controlled by
a RedFish BMC emulator.

[,bash]
----
cd ~/install_yamls
make nmstate
make namespace
cd devsetup  # back to install_yamls/devsetup
make bmaas BMAAS_NODE_COUNT=2
----

A node definition YAML file to use with the `openstack baremetal
create <file>.yaml` command can be generated for the virtual baremetal
nodes by running the `bmaas_generate_nodes_yaml` make target. Store it
in a temp file for later.

[,bash]
----
make bmaas_generate_nodes_yaml | tail -n +2 | tee /tmp/ironic_nodes.yaml
----

Set variables to deploy edpm Standalone with additional network
(`baremetal`) and compute driver `ironic`.

[,bash]
----
cat << EOF > /tmp/addtional_nets.json
[
  {
    "type": "network",
    "name": "crc-bmaas",
    "standalone_config": {
      "type": "ovs_bridge",
      "name": "baremetal",
      "mtu": 1500,
      "vip": true,
      "ip_subnet": "172.20.1.0/24",
      "allocation_pools": [
        {
          "start": "172.20.1.100",
          "end": "172.20.1.150"
        }
      ],
      "host_routes": [
        {
          "destination": "192.168.130.0/24",
          "nexthop": "172.20.1.1"
        }
      ]
    }
  }
]
EOF
export EDPM_COMPUTE_ADDITIONAL_NETWORKS=$(jq -c . /tmp/addtional_nets.json)
export STANDALONE_COMPUTE_DRIVER=ironic
export NTP_SERVER=pool.ntp.org  # Only necessary if not on the RedHat network ...
export EDPM_COMPUTE_CEPH_ENABLED=false  # Optional
export EDPM_COMPUTE_CEPH_NOVA=false # Optional
export EDPM_COMPUTE_SRIOV_ENABLED=false # Without this the standalone deploy fails when compute driver is ironic.
----

[Note]
===
If `EDPM_COMPUTE_CEPH_ENABLED=false` is set, TripleO configures `Glance` with
`Swift` as a backend.
If `EDPM_COMPUTE_CEPH_NOVA=false` is set, TripleO configures `Nova/Libvirt` with
a local storage backend.
===
'''

== Standalone deployment
Use the https://github.com/openstack-k8s-operators/install_yamls/tree/main/devsetup[install_yamls devsetup]
to create a virtual machine (edpm-compute-0) connected to the isolated networks.

[IMPORTANT]
To use OSP 17.1 content to deploy TripleO Standalone, follow the
https://url.corp.redhat.com/devel-rhoso-adoption[guide for setting up downstream content]
for `make standalone`.

To use Wallaby content instead, run the following:

[,bash]
----
cd ~/install_yamls/devsetup
make standalone
----

To deploy using TLS everywhere enabled, instead run:

[,bash]
---
cd ~/install_yamls/devsetup
TLS_ENABLED=true make standalone
---

== Install the openstack-k8s-operators (openstack-operator)

[,bash]
----
cd ..  # back to install_yamls
make crc_storage
make input
make openstack
----

=== Convenience steps

To make our life easier we can copy the deployment passwords we'll be using
in the https://openstack-k8s-operators.github.io/data-plane-adoption/user/#deploying-backend-services_migrating-databases[backend services deployment phase of the data plane adoption].

[,bash]
----
scp -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100:/root/tripleo-standalone-passwords.yaml ~/
----

If we want to be able to easily run `openstack` commands from the host without
actually installing the package and copying the configuration file from the VM
we can create a simple alias:

[,bash]
----
alias openstack="ssh -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100 OS_CLOUD=standalone openstack"
----

=== Route networks

Route VLAN20 to have access to the MariaDB cluster:

[,bash]
----
EDPM_BRIDGE=$(sudo virsh dumpxml edpm-compute-0 | grep -oP "(?<=bridge=').*(?=')")
sudo ip link add link $EDPM_BRIDGE name vlan20 type vlan id 20
sudo ip addr add dev vlan20 172.17.0.222/24
sudo ip link set up dev vlan20
----

To adopt the Swift service as well, route VLAN23 to have access to the storage
backend services:

[,bash]
----
EDPM_BRIDGE=$(sudo virsh dumpxml edpm-compute-0 | grep -oP "(?<=bridge=').*(?=')")
sudo ip link add link $EDPM_BRIDGE name vlan23 type vlan id 23
sudo ip addr add dev vlan23 172.20.0.222/24
sudo ip link set up dev vlan23
----

=== Snapshot/revert

When the deployment of the Standalone OpenStack is finished, it's a
good time to snapshot the machine, so that multiple Adoption attempts
can be done without having to deploy from scratch.

[,bash]
----
cd ~/install_yamls/devsetup
make standalone_snapshot
----

And when you wish to revert the Standalone deployment to the
snapshotted state:

[,bash]
----
cd ~/install_yamls/devsetup
make standalone_revert
----

Similar snapshot could be done for the CRC virtual machine, but the
developer environment reset on CRC side can be done sufficiently via
the install_yamls `*_cleanup` targets. This is further detailed in
the section:
https://openstack-k8s-operators.github.io/data-plane-adoption/dev/#_reset_the_environment_to_pre_adoption_state[Reset the environment to pre-adoption state]

=== Create a workload to adopt

'''

==== Ironic Steps

[,bash]
----
# Enroll baremetal nodes
make bmaas_generate_nodes_yaml | tail -n +2 | tee /tmp/ironic_nodes.yaml
scp -i $HOME/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa /tmp/ironic_nodes.yaml root@192.168.122.100:
ssh -i $HOME/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100

export OS_CLOUD=standalone
openstack baremetal create /root/ironic_nodes.yaml
export IRONIC_PYTHON_AGENT_RAMDISK_ID=$(openstack image show deploy-ramdisk -c id -f value)
export IRONIC_PYTHON_AGENT_KERNEL_ID=$(openstack image show deploy-kernel -c id -f value)
for node in $(openstack baremetal node list -c UUID -f value); do
  openstack baremetal node set $node \
    --driver-info deploy_ramdisk=${IRONIC_PYTHON_AGENT_RAMDISK_ID} \
    --driver-info deploy_kernel=${IRONIC_PYTHON_AGENT_KERNEL_ID} \
    --resource-class baremetal \
    --property capabilities='boot_mode:uefi'
done

# Create a baremetal flavor
openstack flavor create baremetal --ram 1024 --vcpus 1 --disk 15 \
  --property resources:VCPU=0 \
  --property resources:MEMORY_MB=0 \
  --property resources:DISK_GB=0 \
  --property resources:CUSTOM_BAREMETAL=1 \
  --property capabilities:boot_mode="uefi"

# Create image
IMG=Fedora-Cloud-Base-38-1.6.x86_64.qcow2
URL=https://download.fedoraproject.org/pub/fedora/linux/releases/38/Cloud/x86_64/images/$IMG
curl -o /tmp/${IMG} -L $URL
DISK_FORMAT=$(qemu-img info /tmp/${IMG} | grep "file format:" | awk '{print $NF}')
openstack image create --container-format bare --disk-format ${DISK_FORMAT} Fedora-Cloud-Base-38 < /tmp/${IMG}

export BAREMETAL_NODES=$(openstack baremetal node list -c UUID -f value)
# Manage nodes
for node in $BAREMETAL_NODES; do
  openstack baremetal node manage $node
done

# Wait for nodes to reach "manageable" state
watch openstack baremetal node list

# Inspect baremetal nodes
for node in $BAREMETAL_NODES; do
  openstack baremetal introspection start $node
done

# Wait for inspection to complete
watch openstack baremetal introspection list

# Provide nodes
for node in $BAREMETAL_NODES; do
  openstack baremetal node provide $node
done

# Wait for nodes to reach "available" state
watch openstack baremetal node list

# Create an instance on baremetal
openstack server show baremetal-test || {
    openstack server create baremetal-test --flavor baremetal --image Fedora-Cloud-Base-38 --nic net-id=provisioning --wait
}

# Check instance status and network connectivity
openstack server show baremetal-test
ping -c 4 $(openstack server show baremetal-test -f json -c addresses | jq -r .addresses.provisioning[0])
----

'''

==== Virtual Machine Steps

Create a test VM instance with a test volume attachement:

[,bash]
----
cd ~/data-plane-adoption
bash tests/roles/development_environment/files/pre_launch.bash
----

This also creates a test Cinder volume, a backup from it, and a snapshot of it.

'''

==== Ceph Storage Steps

Confirm the image UUID can be seen in Ceph's images pool.

[,bash]
----
ssh -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100 sudo cephadm shell -- rbd -p images ls -l
----

Create a Barbican secret
```
openstack secret store --name testSecret --payload 'TestPayload'
```

== Performing the Data Plane Adoption

The development environment is now set up, you can go to the https://openstack-k8s-operators.github.io/data-plane-adoption/[Adoption
documentation]
and perform adoption manually, or run the https://openstack-k8s-operators.github.io/data-plane-adoption/dev/#_test_suite_information[test
suite]
against your environment.

== Reset the environment to pre-adoption state

The development environment must be rolled back in case we want to execute another Adoption run.

Delete the data-plane and control-plane resources from the CRC vm

[,bash]
----
oc delete --ignore-not-found=true --wait=false openstackdataplanedeployment/openstack
oc delete --ignore-not-found=true --wait=false openstackdataplanedeployment/openstack-nova-compute-ffu
oc delete --ignore-not-found=true --wait=false openstackcontrolplane/openstack
oc patch openstackcontrolplane openstack --type=merge --patch '
metadata:
  finalizers: []
' || true

while oc get pod | grep rabbitmq-server-0; do
    sleep 2
done
while oc get pod | grep openstack-galera-0; do
    sleep 2
done

oc delete --wait=false pod ovn-copy-data || true
oc delete secret osp-secret || true
----

Revert the standalone vm to the snapshotted state

[,bash]
----
cd ~/install_yamls/devsetup
make standalone_revert
----

Clean up and initialize the storage PVs in CRC vm

[,bash]
----
cd ..
for i in {1..3}; do make crc_storage_cleanup crc_storage && break || sleep 5; done
----

== Experimenting with an additional compute node

The following is not on the critical path of preparing the development
environment for Adoption, but it shows how to make the environment
work with an additional compute node VM.

The remaining steps should be completed on the hypervisor hosting crc
and edpm-compute-0.

=== Deploy NG Control Plane with Ceph

Export the Ceph configuration from edpm-compute-0 into a secret.

[,bash]
----
SSH=$(ssh -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100)
KEY=$($SSH "cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CONF=$($SSH "cat /etc/ceph/ceph.conf | base64 -w 0")

cat <<EOF > ceph_secret.yaml
apiVersion: v1
data:
  ceph.client.openstack.keyring: $KEY
  ceph.conf: $CONF
kind: Secret
metadata:
  name: ceph-conf-files
  namespace: openstack
type: Opaque
EOF

oc create -f ceph_secret.yaml
----

Deploy the NG control plane with Ceph as backend for Glance and
Cinder. As described in
https://github.com/openstack-k8s-operators/install_yamls/tree/main[the install_yamls README],
use the sample config located at
https://github.com/openstack-k8s-operators/openstack-operator/blob/main/config/samples/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml
but make sure to replace the `_FSID_` in the sample with the one from
the secret created in the previous step.

[,bash]
----
curl -o /tmp/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml https://raw.githubusercontent.com/openstack-k8s-operators/openstack-operator/main/config/samples/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml
FSID=$(oc get secret ceph-conf-files -o json | jq -r '.data."ceph.conf"' | base64 -d | grep fsid | sed -e 's/fsid = //') && echo $FSID
sed -i "s/_FSID_/${FSID}/" /tmp/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml
oc apply -f /tmp/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml
----

A NG control plane which uses the same Ceph backend should now be
functional. If you create a test image on the NG system to confirm
it works from the configuration above, be sure to read the warning
in the next section.

Before beginning adoption testing or development you may wish to
deploy an EDPM node as described in the following section.

=== Warning about two OpenStacks and one Ceph

Though workloads can be created in the NG deployment to test, be
careful not to confuse them with workloads from the Wallaby cluster
to be migrated. The following scenario is now possible.

A Glance image exists on the Wallaby OpenStack to be adopted.

[,bash]
----
[stack@standalone standalone]$ export OS_CLOUD=standalone
[stack@standalone standalone]$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 33a43519-a960-4cd0-a593-eca56ee553aa | cirros | active |
+--------------------------------------+--------+--------+
[stack@standalone standalone]$
----

If you now create an image with the NG cluster, then a Glance image
will exsit on the NG OpenStack which will adopt the workloads of the
wallaby.

[,bash]
----
[fultonj@hamfast ng]$ export OS_CLOUD=default
[fultonj@hamfast ng]$ export OS_PASSWORD=12345678
[fultonj@hamfast ng]$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 4ebccb29-193b-4d52-9ffd-034d440e073c | cirros | active |
+--------------------------------------+--------+--------+
[fultonj@hamfast ng]$
----

Both Glance images are stored in the same Ceph pool.

[,bash]
----
ssh -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100 sudo cephadm shell -- rbd -p images ls -l
Inferring fsid 7133115f-7751-5c2f-88bd-fbff2f140791
Using recent ceph image quay.rdoproject.org/tripleowallabycentos9/daemon@sha256:aa259dd2439dfaa60b27c9ebb4fb310cdf1e8e62aa7467df350baf22c5d992d8
NAME                                       SIZE     PARENT  FMT  PROT  LOCK
33a43519-a960-4cd0-a593-eca56ee553aa         273 B            2
33a43519-a960-4cd0-a593-eca56ee553aa@snap    273 B            2  yes
4ebccb29-193b-4d52-9ffd-034d440e073c       112 MiB            2
4ebccb29-193b-4d52-9ffd-034d440e073c@snap  112 MiB            2  yes
----

However, as far as each Glance service is concerned each has one
image. Thus, in order to avoid confusion during adoption the test
Glance image on the NG OpenStack should be deleted.

[,bash]
----
openstack image delete 4ebccb29-193b-4d52-9ffd-034d440e073c
----

Connecting the NG OpenStack to the existing Ceph cluster is part of
the adoption procedure so that the data migration can be minimized
but understand the implications of the above example.

=== Deploy edpm-compute-1

edpm-compute-0 is not available as a standard EDPM system to be
managed by https://openstack-k8s-operators.github.io/edpm-ansible[edpm-ansible]
or
https://openstack-k8s-operators.github.io/dataplane-operator[dataplane-operator]
because it hosts the wallaby deployment which will be adopted
and after adoption it will only host the Ceph server.

Use the https://github.com/openstack-k8s-operators/install_yamls/tree/main/devsetup[install_yamls devsetup]
to create additional virtual machines and be sure
that the `EDPM_COMPUTE_SUFFIX` is set to `1` or greater.
Do not set `EDPM_COMPUTE_SUFFIX` to `0` or you could delete
the Wallaby system created in the previous section.

When deploying EDPM nodes add an `extraMounts` like the following in
the `OpenStackDataPlaneNodeSet` CR `nodeTemplate` so that they will be
configured to use the same Ceph cluster.

[,bash]
----
    edpm-compute:
      nodeTemplate:
        extraMounts:
        - extraVolType: Ceph
          volumes:
          - name: ceph
            secret:
              secretName: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
----

A NG data plane which uses the same Ceph backend should now be
functional. Be careful about not confusing new workloads to test the
NG OpenStack with the Wallaby OpenStack as described in the previous
section.

=== Begin Adoption Testing or Development

We should now have:

* An NG glance service based on Antelope running on CRC
* An TripleO-deployed glance serviced running on edpm-compute-0
* Both services have the same Ceph backend
* Each service has their own independent database

An environment above is assumed to be available in the
https://openstack-k8s-operators.github.io/data-plane-adoption/user/#adopting-the-image-service_adopt-control-plane[Glance Adoption documentation]. You
may now follow other Data Plane Adoption procedures described in the
https://openstack-k8s-operators.github.io/data-plane-adoption[documentation].
The same pattern can be applied to other services.
