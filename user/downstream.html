<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="generator" content="Asciidoctor 2.0.26"/>
<title>Adopting a Red Hat OpenStack Platform 17.1 deployment</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"/>
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child{border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock pre>code{display:block}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active,#footnotes .footnote a:first-of-type:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"/>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/monokai.min.css"/>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Adopting a Red Hat OpenStack Platform 17.1 deployment</h1>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#rhoso-180-adoption-overview_assembly">1. Red&#160;Hat OpenStack Services on OpenShift 18.0 adoption overview</a>
<ul class="sectlevel2">
<li><a href="#adoption-limitations_planning">1.1. Adoption limitations</a></li>
<li><a href="#adoption-prerequisites_planning">1.2. Adoption prerequisites</a></li>
<li><a href="#adoption-guidelines_planning">1.3. Guidelines for planning the adoption</a></li>
<li><a href="#adoption-process-overview_planning">1.4. Adoption process overview</a></li>
<li><a href="#dcn-adoption-overview_planning">1.5. Overview of Distributed Compute Node adoption</a></li>
<li><a href="#installing-the-systemd-container-package-on-compute-hosts_planning">1.6. Installing the <code>systemd-container</code> package on Compute hosts</a></li>
<li><a href="#identity-service-authentication_planning">1.7. Identity service authentication</a></li>
<li><a href="#configuring-network-for-RHOSO-deployment_planning">1.8. Configuring the network for the Red&#160;Hat OpenStack Services on OpenShift deployment</a>
<ul class="sectlevel3">
<li><a href="#retrieving-the-network-configuration_configuring-network">1.8.1. Retrieving the network configuration from your existing deployment</a></li>
<li><a href="#planning-your-ipam-configuration_configuring-network">1.8.2. Planning your IPAM configuration</a></li>
<li><a href="#configuring-isolated-networks_configuring-network">1.8.3. Configuring isolated networks</a></li>
</ul>
</li>
<li><a href="#adopting-spine-leaf-networks_configuring-network">1.9. Configuring spine-leaf networks for the Red&#160;Hat OpenStack Services on OpenShift deployment</a></li>
<li><a href="#storage-requirements_configuring-network">1.10. Storage requirements</a>
<ul class="sectlevel3">
<li><a href="#storage-driver-certification_storage-requirements">1.10.1. Storage driver certification</a></li>
<li><a href="#block-storage-requirements_storage-requirements">1.10.2. Block Storage service guidelines</a></li>
<li><a href="#block-storage-limitations_storage-requirements">1.10.3. Limitations for adopting the Block Storage service</a></li>
<li><a href="#openshift-preparation-for-block-storage-adoption_storage-requirements">1.10.4. RHOCP preparation for Block Storage service adoption</a></li>
<li><a href="#preparing-block-storage-by-customizing-configuration_storage-requirements">1.10.5. Converting the Block Storage service configuration</a></li>
<li><a href="#changes-to-cephFS-through-NFS_storage-requirements">1.10.6. Changes to CephFS through NFS</a></li>
</ul>
</li>
<li><a href="#red-hat-ceph-storage-prerequisites_configuring-network">1.11. Red Hat Ceph Storage prerequisites</a>
<ul class="sectlevel3">
<li><a href="#completing-prerequisites-for-migrating-ceph-monitoring-stack_ceph-prerequisites">1.11.1. Completing prerequisites for a Red Hat Ceph Storage cluster with monitoring stack components</a></li>
<li><a href="#completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">1.11.2. Completing prerequisites for Red Hat Ceph Storage RGW migration</a></li>
<li><a href="#completing-prerequisites-for-rbd-migration_ceph-prerequisites">1.11.3. Completing prerequisites for a Red Hat Ceph Storage RBD migration</a></li>
<li><a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">1.11.4. Creating an NFS Ganesha cluster</a></li>
</ul>
</li>
<li><a href="#preparing-an-instance-HA-deployment-for-adoption_configuring-network">1.12. Preparing an Instance HA deployment for adoption</a>
<ul class="sectlevel3">
<li><a href="#maintaining-instance-ha-functionality-after-adoption_preparing-instance-HA">1.12.1. Maintaining the Instance HA functionality after adoption</a></li>
<li><a href="#preventing-pacemaker-from-monitoring-compute-nodes_preparing-instance-HA">1.12.2. Preventing Pacemaker from monitoring Compute nodes</a></li>
</ul>
</li>
<li><a href="#comparing-configuration-files-between-deployments_configuring-network">1.13. Comparing configuration files between deployments</a></li>
<li><a href="#preventing-config-loss-when-using-oc-patch_configuring-network">1.14. Preventing configuration loss when using the <code>oc patch</code> command</a></li>
</ul>
</li>
<li><a href="#migrating-tls-everywhere_configuring-network">2. Migrating TLS-e to the RHOSO deployment</a></li>
<li><a href="#migrating-databases-to-the-control-plane_configuring-network">3. Migrating databases to the control plane</a>
<ul class="sectlevel2">
<li><a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">3.1. Retrieving topology-specific service configuration</a></li>
<li><a href="#deploying-backend-services_migrating-databases">3.2. Deploying back-end services</a></li>
<li><a href="#configuring-a-ceph-backend_migrating-databases">3.3. Configuring a Red Hat Ceph Storage back end</a></li>
<li><a href="#stopping-openstack-services_migrating-databases">3.4. Stopping Red&#160;Hat OpenStack Platform services</a></li>
<li><a href="#migrating-databases-to-mariadb-instances_migrating-databases">3.5. Migrating databases to MariaDB instances</a></li>
<li><a href="#migrating-ovn-data_migrating-databases">3.6. Migrating OVN data</a></li>
</ul>
</li>
<li><a href="#adopting-openstack-control-plane-services_configuring-network">4. Adopting Red&#160;Hat OpenStack Platform control plane services</a>
<ul class="sectlevel2">
<li><a href="#adopting-the-identity-service_adopt-control-plane">4.1. Adopting the Identity service</a></li>
<li><a href="#configuring-ldap-with-domain-specific-drivers_adopt-control-plane">4.2. Configuring LDAP with domain-specific drivers</a></li>
<li><a href="#adopting-the-key-manager-service_adopt-control-plane">4.3. Adopting the Key Manager service</a></li>
<li><a href="#adopting-key-manager-service-with-hsm_adopt-control-plane">4.4. Adopting the Key Manager service with HSM integration</a>
<ul class="sectlevel3">
<li><a href="#key-manager-service-hsm-adoption-approaches_hsm-integration">4.4.1. Key Manager service HSM adoption approaches</a></li>
<li><a href="#adopting-the-key-manager-service-with-proteccio-hsm_hsm-integration">4.4.2. Adopting the Key Manager service with Proteccio HSM integration</a></li>
<li><a href="#adopting-key-manager-service-with-hsm-integration_hsm-integration">4.4.3. Adopting the Key Manager service with HSM integration</a></li>
<li><a href="#troubleshooting-key-manager-hsm-adoption_hsm-integration">4.4.4. Troubleshooting Key Manager HSM adoption</a></li>
<li><a href="#troubleshooting-key-manager-proteccio-adoption_hsm-integration">4.4.5. Troubleshooting Key Manager service Proteccio HSM adoption</a></li>
<li><a href="#rolling-back-the-HSM-adoption_hsm-integration">4.4.6. Rolling back the HSM adoption</a></li>
</ul>
</li>
<li><a href="#adopting-the-networking-service_hsm-integration">4.5. Adopting the Networking service</a></li>
<li><a href="#configuring-control-plane-networking-for-spine-leaf_hsm-integration">4.6. Configuring control plane networking for spine-leaf topologies</a></li>
<li><a href="#adopting-the-object-storage-service_hsm-integration">4.7. Adopting the Object Storage service</a></li>
<li><a href="#adopting-the-image-service_hsm-integration">4.8. Adopting the Image service</a>
<ul class="sectlevel3">
<li><a href="#adopting-image-service-with-object-storage-backend_image-service">4.8.1. Adopting the Image service that is deployed with a Object Storage service back end</a></li>
<li><a href="#adopting-image-service-with-block-storage-backend_image-service">4.8.2. Adopting the Image service that is deployed with a Block Storage service back end</a></li>
<li><a href="#adopting-image-service-with-nfs-backend_image-service">4.8.3. Adopting the Image service that is deployed with an NFS back end</a></li>
<li><a href="#adopting-image-service-with-ceph-backend_image-service">4.8.4. Adopting the Image service that is deployed with a Red Hat Ceph Storage back end</a></li>
<li><a href="#verifying-the-image-service-adoption_image-service">4.8.5. Verifying the Image service adoption</a></li>
</ul>
</li>
<li><a href="#adopting-the-placement-service_hsm-integration">4.9. Adopting the Placement service</a></li>
<li><a href="#adopting-the-bare-metal-provisioning-service_hsm-integration">4.10. Adopting the Bare Metal Provisioning service</a>
<ul class="sectlevel3">
<li><a href="#con_bare-metal-provisioning-service-configurations_adopting-bare-metal-provisioning">4.10.1. Bare Metal Provisioning service configurations</a></li>
<li><a href="#deploying-the-bare-metal-provisioning-service_adopting-bare-metal-provisioning">4.10.2. Deploying the Bare Metal Provisioning service</a></li>
</ul>
</li>
<li><a href="#adopting-the-compute-service_hsm-integration">4.11. Adopting the Compute service</a></li>
<li><a href="#adopting-the-block-storage-service_hsm-integration">4.12. Adopting the Block Storage service</a></li>
<li><a href="#adopting-the-openstack-dashboard_hsm-integration">4.13. Adopting the Dashboard service</a></li>
<li><a href="#adopting-the-shared-file-systems-service_hsm-integration">4.14. Adopting the Shared File Systems service</a>
<ul class="sectlevel3">
<li><a href="#preparing-the-shared-file-systems-service-configuration_adopting-shared-file-systems">4.14.1. Guidelines for preparing the Shared File Systems service configuration</a></li>
<li><a href="#deploying-file-systems-service-control-plane_adopting-shared-file-systems">4.14.2. Deploying the Shared File Systems service on the control plane</a></li>
<li><a href="#decommissioning-RHOSP-standalone-Ceph-NFS-service_adopting-shared-file-systems">4.14.3. Decommissioning the Red&#160;Hat OpenStack Platform standalone Ceph NFS service</a></li>
</ul>
</li>
<li><a href="#adopting-the-orchestration-service_hsm-integration">4.15. Adopting the Orchestration service</a></li>
<li><a href="#adopting-the-loadbalancer-service_hsm-integration">4.16. Adopting the Load-balancing service</a></li>
<li><a href="#adopting-telemetry-services_hsm-integration">4.17. Adopting Telemetry services</a></li>
<li><a href="#adopting-autoscaling_hsm-integration">4.18. Adopting autoscaling services</a></li>
<li><a href="#pulling-configuration-from-tripleo-deployment_hsm-integration">4.19. Pulling the configuration from a director deployment</a></li>
<li><a href="#rolling-back-control-plane-adoption_hsm-integration">4.20. Rolling back the control plane adoption</a></li>
</ul>
</li>
<li><a href="#adopting-data-plane_hsm-integration">5. Adopting the data plane</a>
<ul class="sectlevel2">
<li><a href="#stopping-infrastructure-management-and-compute-services_data-plane">5.1. Stopping infrastructure management and Compute services</a></li>
<li><a href="#adopting-compute-services-to-the-data-plane_data-plane">5.2. Adopting Compute services to the RHOSO data plane</a></li>
<li><a href="#configuring-dcn-data-plane-nodesets_data-plane">5.3. Configuring data plane node sets for DCN sites</a></li>
<li><a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">5.4. Performing a fast-forward upgrade on Compute services</a></li>
<li><a href="#adopting-networker-services-to-the-data-plane_data-plane">5.5. Adopting Networker services to the RHOSO data plane</a></li>
<li><a href="#enabling-high-availability-for-instances_data-plane">5.6. Enabling the high availability for Compute instances service</a></li>
<li><a href="#performing-post-adoption-cleanup-of-load-balancers_data-plane">5.7. Post-adoption tasks for the Load-balancing service</a></li>
</ul>
</li>
<li><a href="#migrating-the-object-storage-service_hsm-integration">6. Migrating the Object Storage service to Red&#160;Hat OpenStack Services on OpenShift nodes</a>
<ul class="sectlevel2">
<li><a href="#migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">6.1. Migrating the Object Storage service data from RHOSP to RHOSO nodes</a></li>
<li><a href="#troubleshooting-object-storage-migration_migrate-object-storage-service">6.2. Troubleshooting the Object Storage service migration</a></li>
</ul>
</li>
<li><a href="#ceph-migration_hsm-integration">7. Migrating the Red Hat Ceph Storage cluster</a>
<ul class="sectlevel2">
<li><a href="#ceph-daemon-cardinality_migrating-ceph">7.1. Red Hat Ceph Storage daemon cardinality</a></li>
<li><a href="#migrating-ceph-monitoring_migrating-ceph">7.2. Migrating the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</a>
<ul class="sectlevel3">
<li><a href="#migrating-monitoring-stack-to-target-nodes_migrating-ceph-monitoring">7.2.1. Migrating the monitoring stack to the target nodes</a></li>
</ul>
</li>
<li><a href="#migrating-ceph-mds_migrating-ceph-monitoring">7.3. Migrating Red Hat Ceph Storage MDS to new nodes within the existing cluster</a></li>
<li><a href="#migrating-ceph-rgw_migrating-ceph-monitoring">7.4. Migrating Red Hat Ceph Storage RGW to external RHEL nodes</a>
<ul class="sectlevel3">
<li><a href="#migrating-the-rgw-backends_migrating-ceph-rgw">7.4.1. Migrating the Red Hat Ceph Storage RGW back ends</a></li>
<li><a href="#deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">7.4.2. Deploying a Red Hat Ceph Storage ingress daemon</a></li>
<li><a href="#updating-the-object-storage-endpoints_migrating-ceph-rgw">7.4.3. Create or update the Object Storage service endpoints</a></li>
</ul>
</li>
<li><a href="#migrating-ceph-rbd_migrating-ceph-monitoring">7.5. Migrating Red Hat Ceph Storage RBD to external RHEL nodes</a>
<ul class="sectlevel3">
<li><a href="#migrating-ceph-mgr-daemons-to-ceph-nodes_migrating-ceph-rbd">7.5.1. Migrating Ceph Manager daemons to Red Hat Ceph Storage nodes</a></li>
<li><a href="#migrating-mon-from-controller-nodes_migrating-ceph-rbd">7.5.2. Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</a></li>
</ul>
</li>
<li><a href="#updating-the-cluster-dashboard-configuration_migrating-ceph-rbd">7.6. Updating the Red Hat Ceph Storage cluster Ceph Dashboard configuration</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="rhoso-180-adoption-overview_assembly">1. Red&#160;Hat OpenStack Services on OpenShift 18.0 adoption overview</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Adoption is the process of migrating a Red&#160;Hat OpenStack Platform (RHOSP) 17.1 control plane to Red&#160;Hat OpenStack Services on OpenShift 18.0, and then completing an in-place upgrade of the data plane. You can retain existing infrastructure investments and modernize your RHOSP deployment on a containerized Red Hat OpenShift Container Platform (RHOCP) foundation. To ensure that you understand the entire adoption process and how to sufficiently prepare your RHOSP environment, review the prerequisites, adoption process, and post-adoption tasks.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Read the whole adoption guide before you start
the adoption to ensure that you understand the procedure. Prepare the necessary configuration snippets for each RHOSP service in advance, and test the migration in a representative test environment before you apply it to production.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="adoption-limitations_planning">1.1. Adoption limitations</h3>
<div class="paragraph _abstract">
<p>Before you proceed with the adoption, check which features are Technology Previews or unsupported.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Technology Preview</dt>
<dd>
<div class="paragraph">
<p>The following features are Technology Previews and have not been tested within the context of the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) adoption:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>FC-based drivers for Block Storage service (cinder)</p>
<div class="paragraph">
<p>The following Compute service (nova) features are Technology Previews:</p>
</div>
</li>
<li>
<p>NUMA-aware vswitches</p>
</li>
<li>
<p>PCI passthrough by flavor</p>
</li>
<li>
<p>SR-IOV trusted virtual functions</p>
</li>
<li>
<p>vGPU</p>
</li>
<li>
<p>Emulated virtual Trusted Platform Module (vTPM)</p>
</li>
<li>
<p>UEFI</p>
</li>
<li>
<p>AMD SEV</p>
</li>
<li>
<p>Direct download from Rados Block Device (RBD)</p>
</li>
<li>
<p>File-backed memory</p>
</li>
<li>
<p>Defining a custom inventory of resources in a YAML file, <code>provider.yaml</code></p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Unsupported features</dt>
<dd>
<div class="paragraph">
<p>The adoption process does not support the following features:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Distributed Compute Node (DCN) architecture with storage services at remote or edge sites</p>
</li>
<li>
<p>DNS-as-a-service (designate)</p>
</li>
<li>
<p>Load-balancing service (octavia)</p>
</li>
<li>
<p>Adopting Border Gateway Protocol (BGP) environments to the RHOSO data plane</p>
</li>
<li>
<p>Adopting a Federal Information Processing Standards (FIPS) environment</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="adoption-prerequisites_planning">1.2. Adoption prerequisites</h3>
<div class="paragraph _abstract">
<p>Before you begin the adoption procedure, complete the following prerequisites:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Planning information</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Review the <a href="#adoption-limitations_planning">Adoption limitations</a>.</p>
</li>
<li>
<p>Review the Red Hat OpenShift Container Platform (RHOCP) requirements, data plane node requirements, Compute node requirements, and so on. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/planning_your_deployment/index">Planning your deployment</a>.</p>
</li>
<li>
<p>Review the adoption-specific networking requirements. For more information, see <a href="#configuring-network-for-RHOSO-deployment_planning">Configuring the network for the RHOSO deployment</a>.</p>
</li>
<li>
<p>Review the adoption-specific storage requirements. For more information, see <a href="#storage-requirements_configuring-network">Storage requirements</a>.</p>
</li>
<li>
<p>Review how to customize your deployed control plane with the services that are required for your environment. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_the_red_hat_openstack_services_on_openshift_deployment/index">Customizing the Red Hat OpenStack Services on OpenShift deployment</a>.</p>
</li>
<li>
<p>Familiarize yourself with the following RHOCP concepts that are used during adoption:</p>
<div class="ulist">
<ul>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/overview-of-nodes">Overview of nodes</a></p>
</li>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a></p>
</li>
<li>
<p><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/machine_configuration/index">Machine configuration overview</a></p>
</li>
</ul>
</div>
</li>
<li>
<p>Familiarize yourself with mapping RHOSO versions to OpenStack Operators and OpenStackVersion custom resources (CRs). For more information, see the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/7125383">How RHOSO versions map to OpenStack Operators and OpenStackVersion CRs</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Back-up information</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Back up your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 environment by using one of the following options:</p>
<div class="ulist">
<ul>
<li>
<p>The Relax-and-Recover tool. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/backing_up_and_restoring_the_undercloud_and_control_plane_nodes/assembly_backing-up-the-undercloud-and-the-control-plane-nodes-using-the-relax-and-recover-tool_br-undercloud-ctlplane">Backing up the undercloud and the control plane nodes by using the Relax-and-Recover tool</a> in <em>Backing up and restoring the undercloud and control plane nodes</em>.</p>
</li>
<li>
<p>The Snapshot and Revert tool. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/backing_up_and_restoring_the_undercloud_and_control_plane_nodes/assembly_snapshot-and-revert-appendix_snapshot-and-revert-appendix">Backing up your Red Hat OpenStack Platform cluster by using the Snapshot and Revert tool</a> in <em>Backing up and restoring the undercloud and control plane nodes</em>.</p>
</li>
<li>
<p>A third-party backup and recovery tool. For more information about certified backup and recovery tools, see the <a href="https://catalog.redhat.com/">Red Hat Ecosystem Catalog</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Back up the configuration files from the RHOSP services and director on your file system. For more information, see <a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Compute</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Upgrade your Compute nodes to Red Hat Enterprise Linux 9.2. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html-single/framework_for_upgrades_16.2_to_17.1/index#upgrading-compute-nodes_upgrading-the-compute-node-operating-system">Upgrading all Compute nodes to RHEL 9.2</a> in <em>Framework for upgrades (16.2 to 17.1)</em>.</p>
</li>
<li>
<p>On your Compute hosts, the <code>systemd-container</code> package must be installed and the <code>systemd-machined</code> service must be running. For more information about how to verify that the package is installed and that the service is running, see <a href="#installing-the-systemd-container-package-on-compute-hosts_planning">Installing the <code>systemd-container</code> package on Compute hosts</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">ML2/OVS</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>If you use the Modular Layer 2 plug-in with Open vSwitch mechanism driver (ML2/OVS), migrate it to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/migrating_to_the_ovn_mechanism_driver/index">Migrating to the OVN mechanism driver</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Tools</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>The oc and podman command line tools are installed on your workstation.</p>
</li>
<li>
<p>Make sure to set the correct RHOSO project namespace in which to run commands.</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc project openstack</code></pre>
</div>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">RHOSP 17.1 release</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>The RHOSP 17.1 cloud is updated to the 17.1.4 release or later. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/performing_a_minor_update_of_red_hat_openstack_platform/index">Performing a minor update of Red Hat OpenStack Platform</a>.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">RHOSP 17.1 hosts</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>All control plane and data plane hosts of the RHOSP 17.1 cloud are up and running, and continue to run throughout the adoption procedure.</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="adoption-guidelines_planning">1.3. Guidelines for planning the adoption</h3>
<div class="paragraph _abstract">
<p>When planning to adopt a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 environment, consider the scope of the change. An adoption is similar in scope to a data center upgrade. Different firmware levels, hardware vendors, hardware profiles, networking interfaces, storage interfaces, and so on affect the adoption process and can cause changes in behavior during the adoption.</p>
</div>
<div class="paragraph">
<p>Review the following guidelines to adequately plan for the adoption and increase the chance that you complete the adoption successfully:</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
All commands in the adoption documentation are examples. Do not copy and paste the commands without understanding what the commands do.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>To minimize the risk of an adoption failure, reduce the number of environmental differences between the staging environment and the production sites.</p>
</li>
<li>
<p>If the staging environment is not representative of the production sites or if a staging environment is not available, you must plan to include contingency time in case the adoption fails.</p>
</li>
<li>
<p>Review your custom Red&#160;Hat OpenStack Platform (RHOSP) service configuration at every major release.</p>
<div class="ulist">
<ul>
<li>
<p>Every major release upgrades through multiple OpenStack releases.</p>
</li>
<li>
<p>Each major release might deprecate configuration options or change the format of the configuration.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Prepare a Method of Procedure (MOP) that is specific to your environment to reduce the risk of variance or omitted steps when running the adoption process.</p>
</li>
<li>
<p>You can use representative hardware in a staging environment to prepare a MOP and validate any content changes.</p>
<div class="ulist">
<ul>
<li>
<p>Include a cross-section of firmware versions, additional interface or device hardware, and any additional software in the representative staging environment to ensure that it is broadly representative of the variety that is present in the production environments.</p>
</li>
<li>
<p>Ensure that you validate any Red Hat Enterprise Linux update or upgrade in the representative staging environment.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Use Satellite for localized and version-pinned RPM content where your data plane nodes are located.</p>
</li>
<li>
<p>In the production environment, use the content that you tested in the staging environment.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adoption-process-overview_planning">1.4. Adoption process overview</h3>
<div class="paragraph _abstract">
<p>Familiarize yourself with the steps of the adoption process and the optional post-adoption tasks.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Main adoption process</dt>
<dd>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><a href="#migrating-tls-everywhere_configuring-network">Migrate TLS everywhere (TLS-e) to the Red Hat OpenStack Services on OpenShift (RHOSO) deployment</a>.</p>
</li>
<li>
<p><a href="#migrating-databases-to-the-control-plane_configuring-network">Migrate your existing databases to the new control plane</a>.</p>
</li>
<li>
<p><a href="#adopting-openstack-control-plane-services_configuring-network">Adopt your Red Hat OpenStack Platform 17.1 control plane services to the new RHOSO 18.0 deployment</a>.</p>
</li>
<li>
<p><a href="#adopting-data-plane_adopt-control-plane">Adopt the RHOSO 18.0 data plane</a>.</p>
</li>
<li>
<p><a href="#migrating-the-object-storage-service_adopt-control-plane">Migrate the Object Storage service (swift) to the RHOSO nodes</a>.</p>
</li>
<li>
<p><a href="#ceph-migration_adopt-control-plane">Migrate the Red Hat Ceph Storage cluster</a>.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p><a href="#migrating-ceph-monitoring_migrating-ceph">Migrate the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</a>.</p>
</li>
<li>
<p><a href="#migrating-ceph-mds_migrating-ceph-monitoring">Migrate Red Hat Ceph Storage MDS to new nodes within the existing cluster</a>.</p>
</li>
<li>
<p><a href="#migrating-ceph-rgw_migrating-ceph-monitoring">Migrate Red Hat Ceph Storage RGW to external RHEL nodes</a>.</p>
</li>
<li>
<p><a href="#migrating-ceph-rbd_migrating-ceph-monitoring">Migrate Red Hat Ceph Storage RBD to external RHEL nodes</a>.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>
</dd>
<dt class="hdlist1">Post-adoption tasks</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Optional: Run tempest to verify that the entire adoption process is working correctly. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/validating_and_troubleshooting_the_deployed_cloud/index">Validating and troubleshooting the deployed cloud</a>.</p>
</li>
<li>
<p>After adoption, Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane nodes run Red Hat Enterprise Linux (RHEL) 9.2. The data plane nodes can remain on RHEL 9.2; however, you must perform a system update to use the full feature set from the release, and to align your environment with the maximum support lifecycle of RHOSO.</p>
<div class="ulist">
<ul>
<li>
<p>You can perform a system update any time after you complete the adoption procedure.</p>
</li>
<li>
<p>You can defer the system update to a separate maintenance window.</p>
</li>
<li>
<p>You can perform the system update on one nodeset at a time. For example, you can update one nodeset from RHEL 9.2 to RHEL 9.4 in one maintenance window, and then update a different nodeset in another maintenance window later.</p>
<div class="paragraph">
<p>For more information about updating your environment, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/updating_your_environment_to_the_latest_maintenance_release/index">Updating your environment to the latest maintenance release</a>.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Verify that you migrated all services from the Controller nodes, and then power off the nodes. If any services are still running in the Controller nodes, such as Open Virtual Networking (ML2/OVN), Object Storage service (swift), or Red Hat Ceph Storage, do not power off the nodes.</p>
</li>
<li>
<p>If you enabled the high availability for Compute instances (Instance HA) service, remove the Pacemaker components from your Compute nodes. For more information, see <a href="#enabling-high-availability-for-instances_data-plane">Enabling the high availability for Compute instances service</a>.</p>
</li>
<li>
<p>Enable TLS Everywhere (TLS-e). For more information about enabling TLS-e after completing the adoption, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/configuring_security_services/index#assembly_enabling-TLS-on-a-deployed-RHOSO-environment">Enabling TLS on a deployed RHOSO environment</a> in <em>Configuring security services</em>.</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="dcn-adoption-overview_planning">1.5. Overview of Distributed Compute Node adoption</h3>
<div class="paragraph _abstract">
<p>The process to adopt Distributed Compute Node (DCN) deployment from Red&#160;Hat OpenStack Platform (RHOSP) to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) requires additional adoption tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You must map a multi-stack deployment to multiple node sets.</p>
</li>
<li>
<p>You must map additional networking configurations.</p>
<div class="dlist">
<dl>
<dt class="hdlist1">Multi-stack to multi-node set mapping</dt>
<dd>
<p>In director deployments, DCN environments use multiple Heat stacks:</p>
<div class="ulist">
<ul>
<li>
<p>The Central stack is templating for Controllers and central Compute nodes.</p>
</li>
<li>
<p>An edge stack is templating for Edge Compute nodes in a stack. There is one stack per DCN site.</p>
<div class="paragraph">
<p>When you perform an adoption, map director stacks to <code>OpenStackDataPlaneNodeSet</code> custom resources (CRs):</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. Mapping director stacks to RHOSO nodesets</caption>
<colgroup>
<col style="width: 33.3333%;"/>
<col style="width: 33.3333%;"/>
<col style="width: 33.3334%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">director stack</th>
<th class="tableblock halign-left valign-top">RHOSO nodeset</th>
<th class="tableblock halign-left valign-top">Availability zone</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Central stack (Compute role)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>openstack-edpm</code> or <code>openstack-cell1</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">az-central</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DCN1 stack (ComputeDcn1 role)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>openstack-edpm-dcn1</code> or <code>openstack-cell1-dcn1</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">az-dcn1</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DCN2 stack (ComputeDcn2 role)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>openstack-edpm-dcn2</code> or <code>openstack-cell1-dcn2</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">az-dcn2</p></td>
</tr>
</tbody>
</table>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Keep all node sets in the same Nova cell to maintain unified scheduling through a shared cell. The default cell is <code>cell1</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Key differences from standard adoption</dt>
<dd>
<p>The following table summarizes the differences between standard adoption and DCN adoption:</p>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 2. Comparison of standard and DCN adoption</caption>
<colgroup>
<col style="width: 33.3333%;"/>
<col style="width: 33.3333%;"/>
<col style="width: 33.3334%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Aspect</th>
<th class="tableblock halign-left valign-top">Standard adoption</th>
<th class="tableblock halign-left valign-top">DCN adoption</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Director stacks</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single stack</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multiple stacks (central + edge sites)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Network topology</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Flat L2 networks</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Routed L3 networks with multiple subnets</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Data plane node sets</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single node set</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multiple node sets (one per site minimum)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Network routes</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Usually not required</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required for inter-site connectivity</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Physnets</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single physnet (e.g., <code>datacentre</code>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multiple physnets (e.g., <code>leaf0</code>, <code>leaf1</code>, <code>leaf2</code>)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Availability zones</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Often single AZ</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multiple AZs (one per site)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">OVN bridge mappings</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single mapping</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Site-specific mappings</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Provider networks</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Single segment</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Multi-segment routed provider networks</p></td>
</tr>
</tbody>
</table>
</dd>
<dt class="hdlist1">Requirements for DCN adoption</dt>
<dd>
<p>Before adopting a DCN deployment, ensure you have:</p>
<div class="ulist">
<ul>
<li>
<p>Network topology information for all sites (IP ranges, VLANs, gateways)</p>
</li>
<li>
<p>Inter-site routing configuration (routes between site subnets)</p>
</li>
<li>
<p>Mapping of director roles to availability zones</p>
</li>
<li>
<p>OVN bridge mapping configuration for each site</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The adoption of the control plane must complete before adopting any data plane nodes. However, once the control plane is adopted, the edge site data plane adoptions can proceed in parallel with the central site data plane adoption.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">DCN Adoption workflow overview</dt>
<dd>
<p>The adoption of a Distributed Compute Node (DCN) deployment from Red&#160;Hat OpenStack Platform (RHOSP) to Red&#160;Hat OpenStack Services on OpenShift (RHOSO)</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Control plane adoption</strong>: Adopt all control plane services from the central director stack to the RHOSO control plane. This is identical to standard adoption.</p>
</li>
<li>
<p><strong>Network configuration</strong>: Configure multi-subnet <code>NetConfig</code> and <code>NetworkAttachmentDefinition</code> CRs to support all site networks.</p>
</li>
<li>
<p><strong>Data plane node set creation</strong>: Create separate <code>OpenStackDataPlaneNodeSet</code> CRs for each site, each with site-specific network configurations:</p>
<div class="ulist">
<ul>
<li>
<p>Network subnet references</p>
</li>
<li>
<p>OVN bridge mappings (physnets)</p>
</li>
<li>
<p>Inter-site routing configuration</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>Data plane deployment</strong>: Deploy all node sets. The edge site node sets can be deployed in parallel after the central site control plane is adopted.</p>
</li>
</ol>
</div>
</dd>
</dl>
</div>
<div class="ulist _additional-resources">
<div class="title">Additional resources</div>
<ul>
<li>
<p><a href="#adopting-spine-leaf-networks_planning">Configuring spine-leaf networks for the Red&#160;Hat OpenStack Services on OpenShift deployment</a></p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="installing-the-systemd-container-package-on-compute-hosts_planning">1.6. Installing the <code>systemd-container</code> package on Compute hosts</h3>
<div class="paragraph _abstract">
<p>Before you adopt the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane, you must verify that the <code>systemd-container</code> package is installed and that <code>systemd-machined</code> is running on all the Compute hosts. You must install the <code>systemd-container</code> package on each Compute host that does not have this package.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Log in to the Compute node host as a user with the appropriate permissions.</p>
</li>
<li>
<p>List the instances that are running on the host:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo machinectl list</pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Sample output</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre>MACHINE                  CLASS SERVICE      OS VERSION ADDRESSES
qemu-1-instance-000000b9 vm    libvirt-qemu -  -       -
qemu-2-instance-000000c2 vm    libvirt-qemu -  -       -

2 machines listed.</pre>
</div>
</div>
</dd>
</dl>
</div>
</li>
<li>
<p>Verify that the <code>systemd-machined</code> service is running:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl status systemd-machined.service</pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Sample output</dt>
<dd>
<div class="listingblock">
<div class="content">
<pre>systemd-machined.service - Virtual Machine and Container Registration Service
     Loaded: loaded (/usr/lib/systemd/system/systemd-machined.service; static)
     Active: active (running) since Mon 2025-06-16 11:42:07 EDT; 2min 48s ago
       Docs: man:systemd-machined.service(8)
             man:org.freedesktop.machine1(5)
   Main PID: 136614 (systemd-machine)
     Status: "Processing requests..."
      Tasks: 1 (limit: 838860)
     Memory: 1.4M
        CPU: 33ms
     CGroup: /system.slice/systemd-machined.service
             136614 /usr/lib/systemd/systemd-machined

Jun 16 11:42:07 computehost001 systemd[1]: Starting Virtual Machine and Container Registration Service...
Jun 16 11:42:07 computehost001 systemd[1]: Started Virtual Machine and Container Registration Service.
Jun 16 11:43:44 computehost001 systemd-machined[136614]: New machine qemu-1-instance-000000b9.
Jun 16 11:43:51 computehost001 systemd-machined[136614]: New machine qemu-2-instance-000000c2.</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
If the <code>systemd-machined</code> service is running, skip the rest of this procedure. Ensure that you verify that the <code>systemd-machined</code> service is running each Compute node host in the cluster.
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
<li>
<p>If the <code>systemd-machined</code> service is not running, before you can install the <code>systemd-container</code> package, live migrate all virtual machines from the host. For more information about live migration, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html/performing_a_minor_update_of_red_hat_openstack_platform/assembly_rebooting-the-overcloud_keeping-updated#proc_rebooting-compute-nodes_rebooting-the-overcloud">Rebooting Compute nodes</a> in <em>Performing a minor update of Red Hat OpenStack Platform</em>.</p>
</li>
<li>
<p>Install the <code>systemd-container</code> on the host:</p>
<div class="ulist">
<ul>
<li>
<p>If you upgraded your environment from an earlier version of Red&#160;Hat OpenStack Platform, reboot the Compute host to automatically install the <code>systemd-container</code>.</p>
</li>
<li>
<p>If you deployed a new RHOSO environment, install the <code>systemd-container</code> manually by using the following command. Rebooting the Compute host is not required:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo dnf -y install systemd-container</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If your Compute host is not running a virtual machine, you can install the <code>systemd-container</code> automatically or manually.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Repeat this procedure on each Compute host in the cluster where the <code>systemd-machined</code> service is not running.</p>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="identity-service-authentication_planning">1.7. Identity service authentication</h3>
<div class="paragraph _abstract">
<p>If you have custom policies enabled, complete the following steps for adoption:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Remove custom policies.</p>
</li>
<li>
<p>Run the adoption.</p>
</li>
<li>
<p>Re-add custom policies by using the new SRBAC syntax.</p>
</li>
</ol>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Red Hat does not support customized roles or policies. Syntax errors or misapplied authorization can negatively impact security or usability. If you need customized roles or policies in your production environment, contact Red Hat support for a support exception before you begin the adoption.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After you adopt a director-based OpenStack deployment to a Red&#160;Hat OpenStack Services on OpenShift deployment, the Identity service performs user authentication and authorization by using Secure RBAC (SRBAC). If SRBAC is already enabled, then there is no change to how you perform operations. If SRBAC is disabled, then adopting a director-based OpenStack deployment might change how you perform operations due to changes in API access policies.</p>
</div>
<div class="paragraph">
<p>For more information on SRBAC, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/performing_security_operations/assembly_srbac-in-rhoso_performing-security-services#assembly_srbac-in-rhoso_performing-security-services">Secure role based access control in Red Hat OpenStack Services on OpenShift</a> in <em>Performing security operations</em>.</p>
</div>
</div>
<div class="sect2">
<h3 id="configuring-network-for-RHOSO-deployment_planning">1.8. Configuring the network for the Red&#160;Hat OpenStack Services on OpenShift deployment</h3>
<div class="paragraph _abstract">
<p>When you adopt a new Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment, you must align the network
configuration with the adopted cluster to maintain connectivity for existing
workloads.</p>
</div>
<div class="paragraph">
<p>Perform the following tasks to incorporate the existing network configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Configure Red Hat OpenShift Container Platform (RHOCP) worker nodes to align VLAN tags and IP Address Management (IPAM) configuration with the existing deployment.</p>
</li>
<li>
<p>Configure control plane services to use compatible IP ranges for service and load-balancing IP addresses.</p>
</li>
<li>
<p>Configure data plane nodes to use corresponding compatible configuration for VLAN tags and IPAM.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When configuring nodes and services, the general approach is as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For IPAM, you can either reuse subnet ranges from the existing deployment or, if there is a shortage of free IP addresses in existing subnets, define new ranges for the new control plane services. If you define new ranges, you configure IP routing between the old and new ranges. For more information, see <a href="#planning-your-ipam-configuration_configuring-network">Planning your IPAM configuration</a>.</p>
</li>
<li>
<p>For VLAN tags, always reuse the configuration from the existing deployment.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For more information about the network architecture and configuration, see
<a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/deploying_red_hat_openstack_services_on_openshift/assembly_preparing-rhoso-networks">Preparing networks for Red Hat OpenStack Services on OpenShift</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em> and <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/networking/about-networking">About networking</a> in <em>Networking</em>.
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="retrieving-the-network-configuration_configuring-network">1.8.1. Retrieving the network configuration from your existing deployment</h4>
<div class="paragraph _abstract">
<p>You must determine which isolated networks are defined in your existing
deployment. After you retrieve your network configuration, you have the following information:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A list of isolated networks that are used in the existing deployment.</p>
</li>
<li>
<p>For each of the isolated networks, the VLAN tag and IP ranges used for
dynamic address allocation.</p>
</li>
<li>
<p>A list of existing IP address allocations that are used in the environment.
When reusing the existing subnet ranges to host the new control plane
services, these addresses are excluded from the corresponding allocation
pools.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Find the network configuration in the <code>network_data.yaml</code> file. For example:</p>
<div class="listingblock">
<div class="content">
<pre>- name: InternalApi
  mtu: 1500
  vip: true
  vlan: 20
  name_lower: internal_api
  dns_domain: internal.mydomain.tld.
  service_net_map_replace: internal
  subnets:
    internal_api_subnet:
      ip_subnet: '172.17.0.0/24'
      allocation_pools: [{'start': '172.17.0.4', 'end': '172.17.0.250'}]</pre>
</div>
</div>
</li>
<li>
<p>Retrieve the VLAN tag that is used in the <code>vlan</code> key and the IP range in the
<code>ip_subnet</code> key for each isolated network from the <code>network_data.yaml</code> file.
When reusing subnet ranges from the existing deployment for the new control
plane services, the ranges are split into separate pools for control
plane services and load-balancer IP addresses.</p>
</li>
<li>
<p>Use the <code>tripleo-ansible-inventory.yaml</code> file to determine the list of IP addresses that are already consumed in the adopted environment. For each listed host in the file, make a note of the IP and VIP addresses that are consumed by the node. For example:</p>
<div class="listingblock">
<div class="content">
<pre>Standalone:
  hosts:
    standalone:
      ...
      internal_api_ip: 172.17.0.100
    ...
  ...
standalone:
  children:
    Standalone: {}
  vars:
    ...
    internal_api_vip: 172.17.0.2
    ...</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In this example, the <code>172.17.0.2</code> and <code>172.17.0.100</code> values are
consumed and are not available for the new control plane services until the adoption is complete.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Repeat this procedure for each isolated network and each host in the
configuration.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="planning-your-ipam-configuration_configuring-network">1.8.2. Planning your IPAM configuration</h4>
<div class="paragraph _abstract">
<p>In a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment, each service that is deployed on the Red Hat OpenShift Container Platform (RHOCP)
worker nodes requires an IP address from the IP Address Management (IPAM) pool.
In a Red&#160;Hat OpenStack Platform (RHOSP) deployment, all services that are
hosted on a Controller node share the same IP address.</p>
</div>
<div class="paragraph">
<p>The RHOSO control plane has different requirements for the number of IP
addresses that are made available for services. Depending on the size of the IP
ranges that are used in the existing RHOSO deployment, you might reuse
these ranges for the RHOSO control plane.</p>
</div>
<div class="paragraph">
<p>The total number of IP addresses that are required for the new control plane services in each isolated network is calculated as the sum of the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of RHOCP worker nodes. Each worker node requires 1 IP address in the <code>NodeNetworkConfigurationPolicy</code> custom resource (CR).</p>
</li>
<li>
<p>The number of IP addresses required for the data plane nodes. Each node requires an IP address from the <code>NetConfig</code> CRs.</p>
</li>
<li>
<p>The number of IP addresses required for control plane services. Each service requires an IP address from the <code>NetworkAttachmentDefinition</code> CRs. This number depends on the number of replicas for each service.</p>
</li>
<li>
<p>The number of IP addresses required for load balancer IP addresses. Each service requires a Virtual IP address from the <code>IPAddressPool</code> CRs.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For example, a simple single worker node RHOCP deployment
with Red Hat OpenShift Local has the following IP ranges defined for the <code>internalapi</code> network:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 IP address for the single worker node</p>
</li>
<li>
<p>1 IP address for the data plane node</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> CRs for control plane services:
<code>X.X.X.30-X.X.X.70</code> (41 addresses)</p>
</li>
<li>
<p><code>IPAllocationPool</code> CRs for load balancer IPs: <code>X.X.X.80-X.X.X.90</code> (11
addresses)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This example shows a total of 54 IP addresses allocated to the <code>internalapi</code>
allocation pools.</p>
</div>
<div class="paragraph">
<p>The requirements might differ depending on the list of RHOSP services
to be deployed, their replica numbers, and the number of RHOCP worker nodes and data plane nodes.</p>
</div>
<div class="paragraph">
<p>Additional IP addresses might be required in future RHOSP releases, so you must plan for some extra capacity for each of the allocation pools that are used in the new environment.</p>
</div>
<div class="paragraph">
<p>After you determine the required IP pool size for the new deployment, you can choose to define new IP address ranges or reuse your existing IP address ranges. Regardless of the scenario, the VLAN tags in the existing deployment are reused in the new deployment. Ensure that the VLAN tags are properly retained in the new configuration. For more information, see <a href="#configuring-isolated-networks_configuring-network">Configuring isolated networks</a>.</p>
</div>
<div class="sect4">
<h5 id="using-new-subnet-ranges_ipam-configuration">Configuring new subnet ranges</h5>
<div class="admonitionblock note _abstract">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you are using IPv6, you can reuse existing subnet ranges in most cases. For more information about existing subnet ranges, see <a href="#reusing-existing-subnet-ranges_ipam-configuration">Reusing existing subnet ranges</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can define new IP ranges for control plane services that belong to a different subnet that is not used in the existing cluster. Then you configure link local IP routing between the existing and new subnets to enable existing and new service deployments to communicate. This involves using the director mechanism on a pre-adopted cluster to configure additional link local routes. This enables the data plane deployment to reach out to Red&#160;Hat OpenStack Platform (RHOSP) nodes by using the existing subnet addresses. You can use new subnet ranges with any existing subnet configuration, and when the existing cluster subnet ranges do not have enough free IP addresses for the new control plane services.</p>
</div>
<div class="paragraph">
<p>You must size the new subnet appropriately to accommodate the new control
plane services. There are no specific requirements for the
existing deployment allocation pools that are already consumed by the RHOSP environment.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Defining a new subnet for Storage and Storage management is not supported because Compute service (nova) and Red Hat Ceph Storage do not allow modifying those networks during adoption.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the following procedure, you configure <code>NetworkAttachmentDefinition</code> custom resources (CRs) to use a different subnet from what is configured in the <code>network_config</code> section of the <code>OpenStackDataPlaneNodeSet</code> CR for the same networks. The new range in the <code>NetworkAttachmentDefinition</code> CR is used for control plane services, while the existing range in the <code>OpenStackDataPlaneNodeSet</code> CR is used to manage IP Address Management (IPAM) for data plane nodes.</p>
</div>
<div class="paragraph">
<p>The values that are used in the following procedure are examples. Use values that are specific to your configuration.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure link local routes on the existing deployment nodes for the control plane subnets. This is done through director configuration:</p>
<div class="listingblock">
<div class="content">
<pre>network_config:
  - type: ovs_bridge
    name: br-ctlplane
    routes:
    - ip_netmask: 0.0.0.0/0
      next_hop: 192.168.1.1
    - <strong>ip_netmask: 172.31.0.0/24</strong>
      <strong>next_hop: 192.168.1.100</strong></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ip_netmask</code> defines the new control plane subnet.</p>
</li>
<li>
<p><code>next_hop</code> defines the control plane IP address of the existing data plane node.</p>
<div class="paragraph">
<p>Repeat this configuration for other networks that need to use different subnets for the new and existing parts of the deployment.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the new configuration to every RHOSP node:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud)$ openstack overcloud network provision \
 --output  &lt;deployment_file&gt; \
[--templates &lt;templates_directory&gt;]/home/stack/templates/&lt;networks_definition_file&gt;</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>(undercloud)$ openstack overcloud node provision \
 --stack &lt;stack&gt; \
 --network-config \
 --output &lt;deployment_file&gt; \
[--templates &lt;templates_directory&gt;]/home/stack/templates/&lt;node_definition_file&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Optional: Include the <code>--templates</code> option to use your own templates instead of the default templates located in <code>/usr/share/openstack-tripleo-heat-templates</code>. Replace <code>&lt;templates_directory&gt;</code> with the path to the directory that contains your templates.</p>
</li>
<li>
<p>Replace <code>&lt;stack&gt;</code> with the name of the stack for which the bare-metal nodes are provisioned. If not specified, the default is <code>overcloud</code>.</p>
</li>
<li>
<p>Include the <code>--network-config</code> optional argument to provide the network definitions to the <code>cli-overcloud-node-network-config.yaml</code> Ansible playbook. The <code>cli-overcloud-node-network-config.yaml</code> playbook uses the <code>os-net-config</code> tool to apply the network configuration on the deployed nodes. If you do not use <code>--network-config</code> to provide the network definitions, then you must configure the <code>{{role.name}}NetworkConfigTemplate</code> parameters in your <code>network-environment.yaml</code> file, otherwise the default network definitions are used.</p>
</li>
<li>
<p>Replace <code>&lt;deployment_file&gt;</code> with the name of the heat environment file to generate for inclusion in the deployment command, for example <code>/home/stack/templates/overcloud-baremetal-deployed.yaml</code>.</p>
</li>
<li>
<p>Replace <code>&lt;node_definition_file&gt;</code> with the name of your node definition file, for example, <code>overcloud-baremetal-deploy.yaml</code>. Ensure that the <code>network_config_update</code> variable is set to <code>true</code> in the node definition file.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Network configuration changes are not applied by default to avoid
the risk of network disruption. You must enforce the changes by setting the
<code>StandaloneNetworkConfigUpdate: true</code> in the director configuration files.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Confirm that there are new link local routes to the new subnet on each node. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># ip route | grep 172
172.31.0.0/24 via 192.168.122.100 dev br-ctlplane</code></pre>
</div>
</div>
</li>
<li>
<p>You also must configure link local routes to existing deployment on Red&#160;Hat OpenStack Services on OpenShift (RHOSO) worker nodes. This is achieved by adding <code>routes</code> entries to the <code>NodeNetworkConfigurationPolicy</code> CRs for each network. For example:</p>
<div class="listingblock">
<div class="content">
<pre>  - destination: 192.168.122.0/24
    next-hop-interface: ospbr</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>destination</code> defines the original subnet of the isolated network on the data plane.</p>
</li>
<li>
<p><code>next-hop-interface</code> defines the Red Hat OpenShift Container Platform (RHOCP) worker network interface that corresponds to the isolated network on the data plane.</p>
<div class="paragraph">
<p>As a result, the following route is added to your RHOCP nodes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># ip route | grep 192
192.168.122.0/24 dev ospbr proto static scope link</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Later, during the data plane adoption, in the <code>network_config</code> section of the <code>OpenStackDataPlaneNodeSet</code> CR, add the same link local routes for the new control plane subnet ranges. For example:</p>
<div class="listingblock">
<div class="content">
<pre>  nodeTemplate:
    ansible:
      ansibleUser: root
      ansibleVars:
        additional_ctlplane_host_routes:
        - ip_netmask: 172.31.0.0/24
          next_hop: '{{ ctlplane_ip }}'
        edpm_network_config_template: |
          network_config:
          - type: ovs_bridge
            routes: {{ ctlplane_host_routes + additional_ctlplane_host_routes }}
            ...</pre>
</div>
</div>
</li>
<li>
<p>List the IP addresses that are used for the data plane nodes in the existing deployment as <code>ansibleHost</code> and <code>fixedIP</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>  nodes:
    standalone:
      ansible:
        ansibleHost: 192.168.122.100
        ansibleUser: ""
      hostName: standalone
      networks:
      - defaultRoute: true
        fixedIP: 192.168.122.100
        name: ctlplane
        subnetName: subnet1</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Do not change RHOSP node IP addresses during the adoption process. List previously used IP addresses in the <code>fixedIP</code> fields for each node entry in the <code>nodes</code> section of the <code>OpenStackDataPlaneNodeSet</code> CR.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Expand the SSH range for the firewall configuration to include both subnets to allow SSH access to data plane nodes from both subnets:</p>
<div class="listingblock">
<div class="content">
<pre>  edpm_sshd_allowed_ranges:
  - 192.168.122.0/24
  - 172.31.0.0/24</pre>
</div>
</div>
<div class="paragraph">
<p>This provides SSH access from the new subnet to the RHOSP nodes as well as the RHOSP subnets.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="reusing-existing-subnet-ranges_ipam-configuration">Reusing existing subnet ranges</h5>
<div class="paragraph _abstract">
<p>You can reuse existing subnet ranges if they have enough IP addresses to allocate to the new control plane services. You configure the new control plane services to use the same subnet as you used in the Red&#160;Hat OpenStack Platform (RHOSP) environment, and configure the allocation pools that are used by the new services to exclude IP addresses that are already allocated to existing cluster nodes. By reusing existing subnets, you avoid additional link local route configuration between the existing and new subnets.</p>
</div>
<div class="paragraph">
<p>If your existing subnets do not have enough IP addresses in the existing subnet ranges for the new control plane services, you must create new subnet ranges. For more information, see <a href="#using-new-subnet-ranges_ipam-configuration">Using new subnet ranges</a>.</p>
</div>
<div class="paragraph">
<p>No special routing configuration is required to reuse subnet ranges. However, you must ensure that the IP addresses that are consumed by RHOSP services do not overlap with the new allocation pools configured for Red&#160;Hat OpenStack Services on OpenShift control plane services.</p>
</div>
<div class="paragraph">
<p>If you are especially constrained by the size of the existing subnet, you may
have to apply elaborate exclusion rules when defining allocation pools for the
new control plane services. For more information, see <a href="#configuring-isolated-networks_configuring-network">Configuring isolated networks</a>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="configuring-isolated-networks_configuring-network">1.8.3. Configuring isolated networks</h4>
<div class="paragraph _abstract">
<p>Before you begin replicating your existing VLAN and IPAM configuration in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment, you must have the following IP address allocations for the new control plane services:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>1 IP address for each isolated network on each Red Hat OpenShift Container Platform (RHOCP) worker node. You configure these IP addresses in the <code>NodeNetworkConfigurationPolicy</code> custom resources (CRs) for the RHOCP worker nodes. For more information, see <a href="#configuring-openshift-worker-nodes_isolated-networks">Configuring RHOCP worker nodes</a>.</p>
</li>
<li>
<p>1 IP range for each isolated network for the data plane nodes. You configure these ranges in the <code>NetConfig</code> CRs for the data plane nodes. For more information, see <a href="#configuring-data-plane-nodes_isolated-networks">Configuring data plane nodes</a>.</p>
</li>
<li>
<p>1 IP range for each isolated network for control plane services. These ranges
enable pod connectivity for isolated networks in the <code>NetworkAttachmentDefinition</code> CRs. For more information, see <a href="#configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</a>.</p>
</li>
<li>
<p>1 IP range for each isolated network for load balancer IP addresses. These IP ranges define load balancer IP addresses for MetalLB in the <code>IPAddressPool</code> CRs. For more information, see <a href="#configuring-networking-for-control-plane-services_isolated-networks">Configuring the networking for control plane services</a>.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The exact list and configuration of isolated networks in the following procedures should reflect the actual Red&#160;Hat OpenStack Platform environment. The number of isolated networks might differ from the examples used in the procedures. The IPAM scheme might also differ. Only the parts of the configuration that are relevant to configuring networks are shown. The values that are used in the following procedures are examples. Use values that are specific to your configuration.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="configuring-openshift-worker-nodes_isolated-networks">Configuring isolated networks on RHOCP worker nodes</h5>
<div class="paragraph _abstract">
<p>To connect service pods to isolated networks on Red Hat OpenShift Container Platform (RHOCP) worker nodes that run Red&#160;Hat OpenStack Platform services, physical network configuration on the hypervisor is required.</p>
</div>
<div class="paragraph">
<p>This configuration is managed by the NMState operator, which uses <code>NodeNetworkConfigurationPolicy</code> custom resources (CRs) to define the desired network configuration for the nodes.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>For each RHOCP worker node, define a <code>NodeNetworkConfigurationPolicy</code> CR that describes the desired network configuration. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: v1
items:
- apiVersion: nmstate.io/v1
  kind: NodeNetworkConfigurationPolicy
  spec:
    desiredState:
      interfaces:
      - description: internalapi vlan interface
        ipv4:
          address:
          - ip: 172.17.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.20
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 20
          reorder-headers: true
      - description: storage vlan interface
        ipv4:
          address:
          - ip: 172.18.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.21
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 21
          reorder-headers: true
      - description: tenant vlan interface
        ipv4:
          address:
          - ip: 172.19.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.22
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 22
          reorder-headers: true
    nodeSelector:
      kubernetes.io/hostname: ocp-worker-0
      node-role.kubernetes.io/worker: ""</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For environments that are enabled with border gateway protocol (BGP), you might need to add additional routes in the <code>NodeNetworkConfigurationPolicy</code> CR so that RHOCP worker nodes can reach the Red&#160;Hat OpenStack Platform Controller nodes and Compute nodes over the control plane and internal API networks.</p>
</div>
<div class="paragraph">
<p>When you configure the RHOCP worker nodes network in the <code>NodeNetworkConfigurationPolicy</code> CR, add routes for each of the following networks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>External network (for example, <code>172.31.0.0/24</code>)</p>
</li>
<li>
<p>Control plane network (for example, <code>192.168.188.0/24</code>)</p>
</li>
<li>
<p>BGP main network (for example, <code>99.99.0.0/16</code>)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following example shows the <code>routes.config</code> section from a <code>NodeNetworkConfigurationPolicy</code> CR for a worker node with BGP configured. In this example, <code>100.64.0.17</code> and <code>100.65.0.17</code> are the IP addresses of the leaf switches that are connected to the specific RHOCP node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>    routes:
      config:
      - destination: 99.99.0.0/16
        next-hop-address: 100.64.0.17
        next-hop-interface: enp7s0
        weight: 200
      - destination: 99.99.0.0/16
        next-hop-address: 100.65.0.17
        next-hop-interface: enp8s0
        weight: 200
      - destination: 172.31.0.0/24
        next-hop-address: 100.64.0.17
        next-hop-interface: enp7s0
        weight: 200
      - destination: 172.31.0.0/24
        next-hop-address: 100.65.0.17
        next-hop-interface: enp8s0
        weight: 200
      - destination: 192.168.188.0/24
        next-hop-address: 100.64.0.17
        next-hop-interface: enp7s0
        weight: 200
      - destination: 192.168.188.0/24
        next-hop-address: 100.65.0.17
        next-hop-interface: enp8s0
        weight: 200</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="configuring-networking-for-control-plane-services_isolated-networks">Configuring isolated networks on control plane services</h5>
<div class="paragraph _abstract">
<p>After the NMState operator creates the desired hypervisor network configuration for isolated networks, you must configure the Red&#160;Hat OpenStack Platform (RHOSP) services to use the configured interfaces. You define a <code>NetworkAttachmentDefinition</code> custom resource (CR) for each isolated network. In some clusters, these CRs are managed by the Cluster Network Operator, in which case you use <code>Network</code> CRs instead. For more information, see
<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/networking/cluster-network-operator#nw-cluster-network-operator_cluster-network-operator">Cluster Network Operator</a> in <em>Networking</em>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Define a <code>NetworkAttachmentDefinition</code> CR for each isolated network.
For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: internalapi
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50"
      }
    }</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Ensure that the interface name and IPAM range match the configuration that you used in the <code>NodeNetworkConfigurationPolicy</code> CRs.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Optional: When reusing existing IP ranges, you can exclude part of the range that is used in the existing deployment by using the <code>exclude</code> parameter in the <code>NetworkAttachmentDefinition</code> pool. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: internalapi
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50",
        "exclude": [
          "172.17.0.24/32",
          "172.17.0.44/31"
        ]
      }
    }</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.config.ipam.range_start</code> defines the start of the IP range.</p>
</li>
<li>
<p><code>spec.config.ipam.range_end</code> defines the end of the IP range.</p>
</li>
<li>
<p><code>spec.config.ipam.exclude</code> excludes part of the IP range. This example excludes IP addresses <code>172.17.0.24/32</code> and <code>172.17.0.44/31</code> from the allocation pool.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If your RHOSP services require load balancer IP addresses, define the pools for these services in an <code>IPAddressPool</code> CR. For example:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The load balancer IP addresses belong to the same IP range as the control plane services, and are managed by MetalLB. This pool should also be aligned with the RHOSP configuration.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.70</pre>
</div>
</div>
<div class="paragraph">
<p>Define <code>IPAddressPool</code> CRs for each isolated network that requires load
balancer IP addresses.</p>
</div>
</li>
<li>
<p>Optional: When reusing existing IP ranges, you can exclude part of the range by listing multiple entries in the <code>addresses</code> section of the <code>IPAddressPool</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.64
    - 172.17.0.66-172.17.0.70</pre>
</div>
</div>
<div class="paragraph">
<p>The example above would exclude the <code>172.17.0.65</code> address from the allocation
pool.</p>
</div>
</li>
<li>
<p>For environments that are enabled with border gateway protocol (BGP), add routes to the <code>NetworkAttachmentDefinition</code> CRs so that the pods can communicate with the Red&#160;Hat OpenStack Platform Controller nodes and Compute nodes over the isolated networks. This is similar to the routes that should be added to the <code>NodeNetworkConfigurationPolicy</code> CRs in BGP environments. For more information about isolated networks, see <a href="#configuring-openshift-worker-nodes_isolated-networks">Configuring isolated networks on RHOCP worker nodes</a>. The following example shows a <code>NetworkAttachmentDefinition</code> CR for the storage network with routes:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: storage
  namespace: openstack
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "storage",
      "type": "bridge",
      "isDefaultGateway": false,
      "isGateway": true,
      "forceAddress": false,
      "hairpinMode": true,
      "ipMasq": false,
      "bridge": "storage",
      "ipam": {
        "type": "whereabouts",
        "range": "172.18.0.0/24",
        "range_start": "172.18.0.30",
        "range_end": "172.18.0.70",
        "routes": [
           {"dst": "172.31.0.0/24", "gw": "172.18.0.1"},
           {"dst": "192.168.188.0/24", "gw": "172.18.0.1"},
           {"dst": "99.99.0.0/16", "gw": "172.18.0.1"}
        ]
      }
    }</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="configuring-data-plane-nodes_isolated-networks">Configuring isolated networks on data plane nodes</h5>
<div class="paragraph _abstract">
<p>Data plane nodes are configured by the OpenStack Operator and your <code>OpenStackDataPlaneNodeSet</code> custom resources (CRs). The <code>OpenStackDataPlaneNodeSet</code> CRs define your desired network configuration for the nodes.</p>
</div>
<div class="paragraph">
<p>Your Red&#160;Hat OpenStack Services on OpenShift (RHOSO) network configuration should reflect the existing Red&#160;Hat OpenStack Platform (RHOSP) network setup. You must pull the <code>network_data.yaml</code> files from each RHOSP node and reuse them when you define the <code>OpenStackDataPlaneNodeSet</code> CRs. The format of the configuration does not change, so you can put network templates under <code>edpm_network_config_template</code> variables, either for all nodes or for each node.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure a <code>NetConfig</code> CR with your desired VLAN tags and IPAM configuration. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">spec.networks</dt>
<dd>
<p>Specifies the <code>networks</code> composition. The <code>networks</code> composition must match the source cloud configuration to avoid data plane connectivity downtime.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Optional: In the <code>NetConfig</code> CR, list multiple ranges for the <code>allocationRanges</code> field to exclude some of the IP addresses, for example, to accommodate IP addresses that are already consumed by the adopted environment:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.199
        start: 172.17.0.100
      - end: 172.17.0.250
        start: 172.17.0.201
      cidr: 172.17.0.0/24
      vlan: 20</pre>
</div>
</div>
<div class="paragraph">
<p>This example excludes the <code>172.17.0.200</code> address from the pool.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-spine-leaf-networks_configuring-network">1.9. Configuring spine-leaf networks for the Red&#160;Hat OpenStack Services on OpenShift deployment</h3>
<div class="paragraph _abstract">
<p>When you adopt a Red&#160;Hat OpenStack Platform (RHOSP) deployment with spine-leaf networking, like a Distributed Compute Node (DCN) architecture, you must each L2 network segment with a separate IP subnet and create create routed provider networks. Traffic between sites is routed at L3 through spine routers or similar network infrastructure.</p>
</div>
<div class="paragraph">
<p>You must configure routing for Compute nodes at edge sites to connect with control plane services, such as RabbitMQ or the database at the central site. The cloud will not function correctly without routes configured.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>DHCP relay is not supported in adopted Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environments with spine-leaf topologies. This affects bare-metal provisioning scenarios that use PXE boot.</p>
</div>
<div class="paragraph">
<p>If you need to provision bare-metal nodes at edge sites, use Redfish virtual media or similar BMC virtual media features instead of PXE boot.</p>
</div>
</td>
</tr>
</table>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 3. Example routes required on DCN1 Compute nodes</caption>
<colgroup>
<col style="width: 33.3333%;"/>
<col style="width: 33.3333%;"/>
<col style="width: 33.3334%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Destination network</th>
<th class="tableblock halign-left valign-top">Next hop</th>
<th class="tableblock halign-left valign-top">Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.0.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.10.1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Route to central internalapi</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.20.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.10.1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Route to DCN2 internalapi</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.0.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.10.1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Route to central storage</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.20.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.10.1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Route to DCN2 storage</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>You configure these routes in the <code>edpm_network_config_template</code> within the <code>OpenStackDataPlaneNodeSet</code> custom resource (CR) for each site.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 4. Example network topology for a three-site DCN deployment</caption>
<colgroup>
<col style="width: 25%;"/>
<col style="width: 25%;"/>
<col style="width: 25%;"/>
<col style="width: 25%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Network</th>
<th class="tableblock halign-left valign-top">Central site</th>
<th class="tableblock halign-left valign-top">DCN1 site</th>
<th class="tableblock halign-left valign-top">DCN2 site</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Control plane</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">192.168.122.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">192.168.133.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">192.168.144.0/24</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Internal API</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.0.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.10.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.17.20.0/24</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Storage</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.0.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.10.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.18.20.0/24</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Tenant</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.19.0.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.19.10.0/24</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">172.19.20.0/24</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>When you adopt a spine-leaf deployment, you configure the <code>NetConfig</code> CR with multiple subnets for each service network. Each subnet represents a different site.</p>
</div>
<div class="listingblock">
<div class="title">Example NetConfig with multiple subnets per network</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: subnet1              # Central site
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
    - name: ctlplanedcn1         # DCN1 site
      allocationRanges:
      - end: 192.168.133.120
        start: 192.168.133.100
      cidr: 192.168.133.0/24
      gateway: 192.168.133.1
    - name: ctlplanedcn2         # DCN2 site
      allocationRanges:
      - end: 192.168.144.120
        start: 192.168.144.100
      cidr: 192.168.144.0/24
      gateway: 192.168.144.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1              # Central site
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
    - name: internalapidcn1      # DCN1 site
      allocationRanges:
      - end: 172.17.10.250
        start: 172.17.10.100
      cidr: 172.17.10.0/24
      vlan: 30
    - name: internalapidcn2      # DCN2 site
      allocationRanges:
      - end: 172.17.20.250
        start: 172.17.20.100
      cidr: 172.17.20.0/24
      vlan: 40</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Each network defines multiple subnets, one for each site.</p>
</li>
<li>
<p>Each site uses unique VLAN IDs. In this example, central uses VLANs 20-23, DCN1 uses VLANs 30-33, and DCN2 uses VLANs 40-43.</p>
</li>
<li>
<p>The subnet naming convention typically uses <code>subnet1</code> for the central site and site-specific names like <code>internalapidcn1</code> for edge sites.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Because the sites are geopgraphically distributed, each site requires its own provider network (physnet). The Networking service (neutron) must be configured to recognize all physnets.</p>
</div>
<div class="listingblock">
<div class="title">Example Neutron ML2 configuration for multiple physnets</div>
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">[ml2_type_vlan]
network_vlan_ranges = leaf0:1:1000,leaf1:1:1000,leaf2:1:1000

[neutron]
physnets = leaf0,leaf1,leaf2</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>leaf0</code> corresponds to the central site.</p>
</li>
<li>
<p><code>leaf1</code> corresponds to the DCN1 site.</p>
</li>
<li>
<p><code>leaf2</code> corresponds to the DCN2 site.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When you create routed provider networks in RHOSO, you create network segments that map to these physnets:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Segment for central: <code>physnet=leaf0</code>, subnet=192.168.122.0/24</p>
</li>
<li>
<p>Segment for DCN1: <code>physnet=leaf1</code>, subnet=192.168.133.0/24</p>
</li>
<li>
<p>Segment for DCN2: <code>physnet=leaf2</code>, subnet=192.168.144.0/24</p>
</li>
</ul>
</div>
<div class="ulist _additional-resources">
<div class="title">Additional resources</div>
<ul>
<li>
<p><a href="#configuring-control-plane-networking-for-spine-leaf_troubleshooting-hsm">Configuring control plane networking for spine-leaf topologies</a></p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="storage-requirements_configuring-network">1.10. Storage requirements</h3>
<div class="paragraph _abstract">
<p>Storage in a Red&#160;Hat OpenStack Platform (RHOSP) deployment refers to the following types:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The storage that is needed for the service to run</p>
</li>
<li>
<p>The storage that the service manages</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Before you can deploy the services in Red&#160;Hat OpenStack Services on OpenShift (RHOSO), you must review the storage requirements, plan your Red Hat OpenShift Container Platform (RHOCP) node selection, prepare your RHOCP nodes, and so on.</p>
</div>
<div class="sect3">
<h4 id="storage-driver-certification_storage-requirements">1.10.1. Storage driver certification</h4>
<div class="paragraph _abstract">
<p>Before you adopt your Red&#160;Hat OpenStack Platform 17.1 deployment to a Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment, confirm that your deployed storage drivers are certified for use with RHOSO 18.0.</p>
</div>
<div class="paragraph">
<p>For information on software certified for use with RHOSO 18.0, see the <a href="https://catalog.redhat.com/search?searchType=software&amp;certified_versions=Red%20Hat%20OpenStack%20Services%20on%20OpenShift%2018&amp;p=1">Red Hat Ecosystem Catalog</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="block-storage-requirements_storage-requirements">1.10.2. Block Storage service guidelines</h4>
<div class="paragraph _abstract">
<p>Prepare to adopt your Block Storage service (cinder):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Take note of the Block Storage service back ends that you use.</p>
</li>
<li>
<p>Determine all the transport protocols that the Block Storage service back ends use, such as
RBD, iSCSI, FC, NFS, NVMe-TCP, and so on. You must consider them when you place the Block Storage services and when the right storage transport-related binaries are running on the Red Hat OpenShift Container Platform (RHOCP) nodes. For more information about each storage transport protocol, see <a href="#openshift-preparation-for-block-storage-adoption_storage-requirements">RHOCP preparation for Block Storage service adoption</a>.</p>
</li>
<li>
<p>Use a Block Storage service volume service to deploy each Block Storage service volume back end.</p>
<div class="paragraph">
<p>For example, you have an LVM back end, a Ceph back end, and two entries in <code>cinderVolumes</code>, and you cannot set global defaults for all volume services. You must define a service for each of them:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [lvm]
&lt; . . . &gt;
        ceph:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [ceph]
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
Check that all configuration options are still valid for RHOSO 18.0 version. Configuration options might be deprecated, removed, or added. This applies to both back-end driver-specific configuration options and other generic options.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="block-storage-limitations_storage-requirements">1.10.3. Limitations for adopting the Block Storage service</h4>
<div class="paragraph _abstract">
<p>Before you begin the Block Storage service (cinder) adoption, review the following limitations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>There is no global <code>nodeSelector</code> option for all Block Storage service volumes. You must specify the <code>nodeSelector</code> for each back end.</p>
</li>
<li>
<p>There are no global <code>customServiceConfig</code> or <code>customServiceConfigSecrets</code> options for all Block Storage service volumes. You must specify these options for each back end.</p>
</li>
<li>
<p>Support for Block Storage service back ends that require kernel modules that are not included in Red Hat Enterprise Linux is not tested in Red&#160;Hat OpenStack Services on OpenShift (RHOSO).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="openshift-preparation-for-block-storage-adoption_storage-requirements">1.10.4. RHOCP preparation for Block Storage service adoption</h4>
<div class="paragraph _abstract">
<p>Before you deploy Red&#160;Hat OpenStack Platform (RHOSP) in Red Hat OpenShift Container Platform (RHOCP) nodes, ensure that the networks are ready, that you decide which RHOCP nodes to restrict, and that you make any necessary changes to the RHOCP nodes.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Node selection</dt>
<dd>
<p>You might need to restrict the RHOCP nodes where the Block Storage service volume and backup services run.</p>
<div class="paragraph">
<p>An example of when you need to restrict nodes for a specific Block Storage service is when you deploy the Block Storage service with the LVM driver. In that scenario, the LVM data where the volumes are stored only exists in a specific host, so you need to pin the Block Storage-volume service to that specific RHOCP node. Running the service on any other RHOCP node does not work. You cannot use the RHOCP host node name to restrict the LVM back end. You need to identify the LVM back end by using a unique label, an existing label, or a new label:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc label nodes worker0 lvm=cinder-volumes</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm-iscsi:
          nodeSelector:
            lvm: cinder-volumes
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>For more information about node selection, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If your nodes do not have enough local disk space for temporary images, you can use a remote NFS location by setting the extra volumes feature, <code>extraMounts</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
<dt class="hdlist1">Transport protocols</dt>
<dd>
<p>Some changes to the storage transport protocols might be required for RHOCP:</p>
<div class="ulist">
<ul>
<li>
<p>If you use a <code>MachineConfig</code> to make changes to RHOCP nodes, the nodes reboot.</p>
</li>
<li>
<p>Check the back-end sections that are listed in the <code>enabled_backends</code> configuration option in your <code>cinder.conf</code> file to determine the enabled storage back-end sections.</p>
</li>
<li>
<p>Depending on the back end, you can find the transport protocol by viewing the <code>volume_driver</code> or <code>target_protocol</code> configuration options.</p>
</li>
<li>
<p>The <code>iscsid</code> service, <code>multipathd</code> service, and <code>NVMe-TCP</code> kernel modules start automatically on data plane nodes.</p>
<div class="dlist">
<dl>
<dt class="hdlist1">NFS</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>RHOCP connects to NFS back ends without additional changes.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Rados Block Device and Red Hat Ceph Storage</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>RHOCP connects to Red Hat Ceph Storage back ends without additional changes. You must provide credentials and configuration files to the services.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">iSCSI</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>To connect to iSCSI volumes, the iSCSI initiator must run on the
RHOCP hosts where the volume and backup services run. The Linux Open iSCSI initiator does not support network namespaces, so you must only run one instance of the service for the normal RHOCP usage, as well as
the RHOCP CSI plugins and the RHOSP services.</p>
</li>
<li>
<p>If you are not already running <code>iscsid</code> on the RHOCP nodes, then you must apply a <code>MachineConfig</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-iscsid
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: iscsid.service</code></pre>
</div>
</div>
</li>
<li>
<p>If you use labels to restrict the nodes where the Block Storage services run, you must use a <code>MachineConfigPool</code> to limit the effects of the
<code>MachineConfig</code> to the nodes where your services might run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you are using a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</li>
<li>
<p>For production deployments that use iSCSI volumes, configure multipathing for better I/O.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">FC</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>The Block Storage service volume and Block Storage service backup services must run in an RHOCP host that has host bus adapters (HBAs). If some nodes do not have HBAs, then use labels to restrict where these services run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you have virtualized RHOCP clusters that use FC you need to expose the host HBAs inside the virtual machine.</p>
</li>
<li>
<p>For production deployments that use FC volumes, configure multipathing for better I/O.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">NVMe-TCP</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>To connect to NVMe-TCP volumes, load NVMe-TCP kernel modules on the RHOCP hosts.</p>
</li>
<li>
<p>If you do not already load the <code>nvme-fabrics</code> module on the RHOCP nodes where the volume and backup services are going to run, then you must apply a <code>MachineConfig</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-load-nvme-fabrics
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modules-load.d/nvme_fabrics.conf
          overwrite: false
          # Mode must be decimal, this is 0644
          mode: 420
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,nvme-fabrics</pre>
</div>
</div>
</li>
<li>
<p>If you use labels to restrict the nodes where Block Storage
services run, use a <code>MachineConfigPool</code> to limit the effects of the <code>MachineConfig</code> to the nodes where your services run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you use a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</li>
<li>
<p>Only load the <code>nvme-fabrics</code> module because it loads the transport-specific modules, such as TCP, RDMA, or FC, as needed.</p>
</li>
<li>
<p>For production deployments that use NVMe-TCP volumes, use multipathing for better I/O. For NVMe-TCP volumes, RHOCP uses native multipathing, called ANA.</p>
</li>
<li>
<p>After the RHOCP nodes reboot and load the <code>nvme-fabrics</code> module, you can confirm that the operating system is configured and that it supports ANA by checking the host:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat /sys/module/nvme_core/parameters/multipath</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
ANA does not use the Linux Multipathing Device Mapper, but RHOCP requires <code>multipathd</code> to run on Compute nodes for the Compute service (nova) to be able to use multipathing. Multipathing is automatically configured on data plane nodes when they are provisioned.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Multipathing</dt>
<dd>
<div class="ulist">
<ul>
<li>
<p>Use multipathing for iSCSI and FC protocols. To configure multipathing on these protocols, you perform the following tasks:</p>
<div class="ulist">
<ul>
<li>
<p>Prepare the RHOCP hosts</p>
</li>
<li>
<p>Configure the Block Storage services</p>
</li>
<li>
<p>Prepare the Compute service nodes</p>
</li>
<li>
<p>Configure the Compute service</p>
</li>
</ul>
</div>
</li>
<li>
<p>To prepare the RHOCP hosts, ensure that the Linux Multipath Device Mapper is configured and running on the RHOCP hosts by using <code>MachineConfig</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Includes the /etc/multipathd.conf contents and the systemd unit changes
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-multipathd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/multipath.conf
          overwrite: false
          # Mode must be decimal, this is 0600
          mode: 384
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,defaults%20%7B%0A%20%20user_friendly_names%20no%0A%20%20recheck_wwid%20yes%0A%20%20skip_kpartx%20yes%0A%20%20find_multipaths%20yes%0A%7D%0A%0Ablacklist%20%7B%0A%7D
    systemd:
      units:
      - enabled: true
        name: multipathd.service</code></pre>
</div>
</div>
</li>
<li>
<p>If you use labels to restrict the nodes where Block Storage services run, you need to use a <code>MachineConfigPool</code> to limit the effects of the <code>MachineConfig</code> to only the nodes where your services run. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/nodes/index#nodes-scheduler-node-selectors-about_nodes-scheduler-node-selectors">About node selectors</a>.</p>
</li>
<li>
<p>If you are using a single node deployment to test the process, replace <code>worker</code> with <code>master</code> in the <code>MachineConfig</code>.</p>
</li>
<li>
<p>Cinder volume and backup are configured by default to use multipathing.</p>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect3">
<h4 id="preparing-block-storage-by-customizing-configuration_storage-requirements">1.10.5. Converting the Block Storage service configuration</h4>
<div class="paragraph _abstract">
<p>In your previous deployment, you use the same <code>cinder.conf</code> file for all the services. To prepare your Block Storage service (cinder) configuration for adoption, split this single-file configuration into individual configurations for each Block Storage service service. Review the following information to guide you in coverting your previous configuration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Determine what part of the configuration is generic for all the Block Storage services and remove anything that would change when deployed in Red Hat OpenShift Container Platform (RHOCP), such as the <code>connection</code> in the <code>[database]</code> section, the <code>transport_url</code> and <code>log_dir</code> in the <code>[DEFAULT]</code> sections, the whole <code>[coordination]</code> and <code>[barbican]</code> sections. The remaining generic configuration goes into the <code>customServiceConfig</code> option, or a <code>Secret</code> custom resource (CR) and is then used in the <code>customServiceConfigSecrets</code> section, at the <code>cinder: template:</code> level.</p>
</li>
<li>
<p>Determine if there is a scheduler-specific configuration and add it to the <code>customServiceConfig</code> option in <code>cinder: template: cinderScheduler</code>.</p>
</li>
<li>
<p>Determine if there is an API-specific configuration and add it to the <code>customServiceConfig</code> option in <code>cinder: template: cinderAPI</code>.</p>
</li>
<li>
<p>If the Block Storage service backup is deployed, add the Block Storage service backup configuration options to <code>customServiceConfig</code> option, or to a <code>Secret</code> CR that you can add to <code>customServiceConfigSecrets</code> section at the <code>cinder: template:
cinderBackup:</code> level. Remove the <code>host</code> configuration in the <code>[DEFAULT]</code> section to support multiple replicas later.</p>
</li>
<li>
<p>Determine the individual volume back-end configuration for each of the drivers. The configuration is in the specific driver section, and it includes the <code>[backend_defaults]</code> section and FC zoning sections if you use them. The Block Storage service operator does not support a global <code>customServiceConfig</code> option for all volume services. Each back end has its own section under <code>cinder: template: cinderVolumes</code>, and the configuration goes in the <code>customServiceConfig</code> option or in a <code>Secret</code> CR and is then used in the <code>customServiceConfigSecrets</code> section.</p>
</li>
<li>
<p>If any of the Block Storage service volume drivers require a custom vendor image, find the location of the image in the <a href="https://catalog.redhat.com/search?searchType=software">Red Hat Ecosystem Catalog</a>, and create or modify an <code>OpenStackVersion</code> CR to specify the custom image by using the key from the <code>cinderVolumes</code> section.</p>
<div class="paragraph">
<p>For example, if you have the following configuration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderVolume:
        pure:
          customServiceConfigSecrets:
            - openstack-cinder-pure-cfg
&lt; . . . &gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then the <code>OpenStackVersion</code> CR that describes the container image for that back end looks like the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackVersion
metadata:
  name: openstack
spec:
  customContainerImages:
    cinderVolumeImages:
      pure: registry.connect.redhat.com/purestorage/openstack-cinder-volume-pure-rhosp-18-0'</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The name of the <code>OpenStackVersion</code> must match the name of your <code>OpenStackControlPlane</code> CR.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If your Block Storage services use external files, for example, for a custom policy, or to store credentials or SSL certificate authority bundles to connect to a storage array, make those files available to the right containers. Use <code>Secrets</code> or <code>ConfigMap</code> to store the information in RHOCP and then in the <code>extraMounts</code> key. For example, for Red Hat Ceph Storage credentials that are stored in a <code>Secret</code> called <code>ceph-conf-files</code>, you patch the top-level <code>extraMounts</code> key in the <code>OpenstackControlPlane</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files</code></pre>
</div>
</div>
</li>
<li>
<p>For a service-specific file, such as the API policy, you add the configuration
on the service itself. In the following example, you include the <code>CinderAPI</code>
configuration that references the policy you are adding from a <code>ConfigMap</code>
called <code>my-cinder-conf</code> that has a <code>policy</code> key with the contents of the policy:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderAPI:
        customServiceConfig: |
           [oslo_policy]
           policy_file=/etc/cinder/api/policy.yaml
      extraMounts:
      - extraVol:
        - extraVolType: Ceph
          mounts:
          - mountPath: /etc/cinder/api
            name: policy
            readOnly: true
          propagation:
          - CinderAPI
          volumes:
          - name: policy
            projected:
              sources:
              - configMap:
                  name: my-cinder-conf
                  items:
                    - key: policy
                      path: policy.yaml</code></pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="changes-to-cephFS-through-NFS_storage-requirements">1.10.6. Changes to CephFS through NFS</h4>
<div class="paragraph _abstract">
<p>Before you begin the adoption, review the following information to understand the changes to CephFS through NFS between Red&#160;Hat OpenStack Platform (RHOSP) 17.1 and Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the RHOSP 17.1 deployment uses CephFS through NFS as a back end for Shared File Systems service (manila), you cannot directly import the <code>ceph-nfs</code> service on the RHOSP Controller nodes into RHOSO 18.0. In RHOSO 18.0, the Shared File Systems service only supports using a clustered NFS service that is directly managed on the Red Hat Ceph Storage cluster. Adoption with the <code>ceph-nfs</code> service involves a data path disruption to existing NFS clients.</p>
</li>
<li>
<p>On RHOSP 17.1, Pacemaker manages the high availability of the <code>ceph-nfs</code> service. This service is assigned a Virtual IP (VIP) address that is also managed by Pacemaker. The VIP is typically created on an isolated <code>StorageNFS</code> network. The Controller nodes have ordering and collocation constraints established between this VIP, <code>ceph-nfs</code>, and the Shared File Systems service (manila) share manager service. Prior to adopting Shared File Systems service, you must adjust the Pacemaker ordering and collocation constraints to separate the share manager service. This establishes <code>ceph-nfs</code> with its VIP as an isolated, standalone NFS service that you can decommission after completing the RHOSO adoption.</p>
</li>
<li>
<p>In Red Hat Ceph Storage 7, a native clustered Ceph NFS service has to be deployed on the Red Hat Ceph Storage cluster by using the Ceph Orchestrator prior to adopting the Shared File Systems service. This NFS service eventually replaces the standalone NFS service from RHOSP 17.1 in your deployment. When the Shared File Systems service is adopted into the RHOSO 18.0 environment, it establishes all the existing exports and client restrictions on the new clustered Ceph NFS service. Clients can continue to read and write data on existing NFS shares, and are not affected until the old standalone NFS service is decommissioned. After the service is decommissioned, you can re-mount the same share from the new clustered Ceph NFS service during a scheduled downtime.</p>
</li>
<li>
<p>To ensure that NFS users are not required to make any networking changes to their existing workloads, assign an IP address from the same isolated <code>StorageNFS</code> network to the clustered Ceph NFS service. NFS users only need to discover and re-mount their shares by using new export paths. When the adoption is complete, RHOSO users can query the Shared File Systems service API to list the export locations on existing shares to identify the preferred paths to mount these shares. These preferred paths correspond to the new clustered Ceph NFS service in contrast to other non-preferred export paths that continue to be displayed until the old isolated, standalone NFS service is decommissioned.</p>
</li>
<li>
<p>When you migrate your workloads from the old NFS service, you must ensure that exports are not consumed from both the old NFS service and the new clustered Ceph NFS service at the same time. This simultaneous access to both services is considered dangerous and bypasses the protections for concurrent access that is ensured by the NFS protocol. When you migrate the workloads to use exports from the new NFS service, you must ensure that you migrate the use of each export entirely so that no part of the workload stays connected to the old NFS service.</p>
</li>
<li>
<p>You can no longer control the old Pacemaker-managed <code>ceph-nfs</code> service through the Red&#160;Hat OpenStack Platform director after the control plane adoption is complete. This means that there is no support for updating the NFS Ganesha software, or changing any configuration. While data is protected from server crashes or restarts, high availability and data recovery is still limited, and these maintenance issues are no longer visible to Shared File Systems service.</p>
</li>
<li>
<p>Cloud administrators must ensure a reasonably short window to switch over all end-user workloads to the new NFS service.</p>
</li>
<li>
<p>While the old <code>ceph-nfs</code> service only supported NFS version 4.1 and later, the new clustered NFS service supports NFS protocols 3 and 4.1 and later. Mixing protocol versions with an export results in unintended consequences. You should mount a given share across all clients by using a consistent NFS protocol version.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For more information on setting up a clustered NFS service, see <a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating an NFS Ganesha cluster</a>.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="red-hat-ceph-storage-prerequisites_configuring-network">1.11. Red Hat Ceph Storage prerequisites</h3>
<div class="paragraph _abstract">
<p>Before you migrate your Red Hat Ceph Storage cluster daemons from your Controller nodes, complete the following tasks in your Red&#160;Hat OpenStack Platform 17.1 environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Upgrade your Red Hat Ceph Storage cluster to release 7. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_platform/17.1/html-single/framework_for_upgrades_16.2_to_17.1/index#assembly_ceph-6-to-7_upgrade_post-upgrade-external-ceph">Upgrading Red Hat Ceph Storage 6 to 7</a> in <em>Framework for upgrades (16.2 to 17.1)</em>.</p>
</li>
<li>
<p>Your Red Hat Ceph Storage 7 deployment is managed by <code>cephadm</code>.</p>
</li>
<li>
<p>The undercloud is still available, and the nodes and networks are managed by director.</p>
</li>
<li>
<p>If you use an externally deployed Red Hat Ceph Storage cluster, you must recreate a <code>ceph-nfs</code> cluster in the target nodes as well as propogate the <code>StorageNFS</code> network.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Complete the prerequisites for your specific Red Hat Ceph Storage environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#completing-prerequisites-for-migrating-ceph-monitoring-stack_ceph-prerequisites">Red Hat Ceph Storage with monitoring stack components</a></p>
</li>
<li>
<p><a href="#completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">Red Hat Ceph Storage RGW</a></p>
</li>
<li>
<p><a href="#completing-prerequisites-for-rbd-migration_ceph-prerequisites">Red Hat Ceph Storage RBD</a></p>
</li>
<li>
<p><a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">NFS Ganesha</a></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-migrating-ceph-monitoring-stack_ceph-prerequisites">1.11.1. Completing prerequisites for a Red Hat Ceph Storage cluster with monitoring stack components</h4>
<div class="paragraph _abstract">
<p>Before you migrate a Red Hat Ceph Storage cluster with monitoring stack components, you must gather monitoring stack information, review and update the container image registry, and remove the undercloud container images.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In addition to updating the container images related to the monitoring stack, you must update the configuration entry related to the <code>container_image_base</code>. This has an impact on all the Red Hat Ceph Storage daemons that rely on the undercloud images.
New daemons are deployed by using the new image registry location that is configured in the Red Hat Ceph Storage cluster.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Gather the current status of the monitoring stack. Verify that
the hosts have no <code>monitoring</code> label, or <code>grafana</code>, <code>prometheus</code>, or <code>alertmanager</code>, in cases of a per daemons placement evaluation:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The entire relocation process is driven by <code>cephadm</code> and relies on labels to be
assigned to the target nodes, where the daemons are scheduled.
For more information about assigning labels to nodes, review the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                    	ADDR       	LABELS                 	STATUS
cephstorage-0.redhat.local  192.168.24.11  osd mds
cephstorage-1.redhat.local  192.168.24.12  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.35  _admin mon mgr
controller-1.redhat.local   192.168.24.53  mon _admin mgr
controller-2.redhat.local   192.168.24.10  mon _admin mgr
6 hosts in cluster</pre>
</div>
</div>
<div class="paragraph">
<p>Confirm that the cluster is healthy and that both <code>ceph orch ls</code> and
<code>ceph orch ps</code> return the expected number of deployed daemons.</p>
</div>
</li>
<li>
<p>Review and update the container image registry:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you run the Red Hat Ceph Storage externalization procedure after you migrate the Red&#160;Hat OpenStack Platform control plane, update the container images in the Red Hat Ceph Storage cluster configuration. The current container images point to the undercloud registry, which might not be available anymore. Because the undercloud is not available after adoption is complete, replace the undercloud-provided images with an alternative registry.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ ceph config dump
...
...
mgr   advanced  mgr/cephadm/container_image_alertmanager    undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus-alertmanager:v4.10
mgr   advanced  mgr/cephadm/container_image_base            undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhceph
mgr   advanced  mgr/cephadm/container_image_grafana         undercloud-0.ctlplane.redhat.local:8787/rh-osbs/grafana:latest
mgr   advanced  mgr/cephadm/container_image_node_exporter   undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus-node-exporter:v4.10
mgr   advanced  mgr/cephadm/container_image_prometheus      undercloud-0.ctlplane.redhat.local:8787/rh-osbs/openshift-ose-prometheus:v4.10</pre>
</div>
</div>
</li>
<li>
<p>Remove the undercloud container images:</p>
<div class="listingblock">
<div class="content">
<pre>$ cephadm shell -- ceph config rm mgr mgr/cephadm/container_image_base \
for i in prometheus grafana alertmanager node_exporter; do \
    cephadm shell -- ceph config rm mgr mgr/cephadm/container_image_$i \
done</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">1.11.2. Completing prerequisites for Red Hat Ceph Storage RGW migration</h4>
<div class="paragraph _abstract">
<p>Complete the following prerequisites before you begin the Ceph Object Gateway (RGW) migration.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check the current status of the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud) [stack@undercloud-0 ~]$ metalsmith list


    +------------------------+    +----------------+
    | IP Addresses           |    |  Hostname      |
    +------------------------+    +----------------+
    | ctlplane=192.168.24.25 |    | cephstorage-0  |
    | ctlplane=192.168.24.10 |    | cephstorage-1  |
    | ctlplane=192.168.24.32 |    | cephstorage-2  |
    | ctlplane=192.168.24.28 |    | compute-0      |
    | ctlplane=192.168.24.26 |    | compute-1      |
    | ctlplane=192.168.24.43 |    | controller-0   |
    | ctlplane=192.168.24.7  |    | controller-1   |
    | ctlplane=192.168.24.41 |    | controller-2   |
    +------------------------+    +----------------+</pre>
</div>
</div>
</li>
<li>
<p>Log in to <code>controller-0</code> and check the Pacemaker status to identify important information for the RGW migration:</p>
<div class="listingblock">
<div class="content">
<pre>Full List of Resources:
  * ip-192.168.24.46	(ocf:heartbeat:IPaddr2):     	Started controller-0
  * ip-10.0.0.103   	(ocf:heartbeat:IPaddr2):     	Started controller-1
  * ip-172.17.1.129 	(ocf:heartbeat:IPaddr2):     	Started controller-2
  * ip-172.17.3.68  	(ocf:heartbeat:IPaddr2):     	Started controller-0
  * ip-172.17.4.37  	(ocf:heartbeat:IPaddr2):     	Started controller-1
  * Container bundle set: haproxy-bundle

[undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:
    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-2
    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-0
    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-1</pre>
</div>
</div>
</li>
<li>
<p>Identify the ranges of the storage networks. The following is an example and the values might differ in your environment:</p>
<div class="listingblock">
<div class="content">
<pre>[heat-admin@controller-0 ~]$ ip -o -4 a

1: lo	inet 127.0.0.1/8 scope host lo\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.45/24 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.46/32 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
7: <strong>br-ex</strong>	inet 10.0.0.122/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever
8: vlan70	inet 172.17.5.22/24 brd 172.17.5.255 scope global vlan70\   	valid_lft forever preferred_lft forever
8: vlan70	inet 172.17.5.94/32 brd 172.17.5.255 scope global vlan70\   	valid_lft forever preferred_lft forever
9: vlan50	inet 172.17.2.140/24 brd 172.17.2.255 scope global vlan50\   	valid_lft forever preferred_lft forever
10: <strong>vlan30</strong>	inet 172.17.3.73/24 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
10: vlan30	inet 172.17.3.68/32 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
11: vlan20	inet 172.17.1.88/24 brd 172.17.1.255 scope global vlan20\   	valid_lft forever preferred_lft forever
12: vlan40	inet 172.17.4.24/24 brd 172.17.4.255 scope global vlan40\   	valid_lft forever preferred_lft forever</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>br-ex</code> represents the External Network, where in the current environment, HAProxy has the front-end Virtual IP (VIP) assigned.</p>
</li>
<li>
<p><code>vlan30</code> represents the Storage Network, where the new RGW instances should be started on the Red Hat Ceph Storage nodes.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Identify the network that you previously had in HAProxy and propagate it through director to the Red Hat Ceph Storage nodes. Use this network to reserve a new VIP that is owned by Red Hat Ceph Storage as the entry point for the RGW service.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Log in to <code>controller-0</code> and find the <code>ceph_rgw</code> section in the current HAProxy configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ less /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg
...
...
listen ceph_rgw
  bind 10.0.0.103:8080 transparent
  bind 172.17.3.68:8080 transparent
  mode http
  balance leastconn
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Port %[dst_port]
  option httpchk GET /swift/healthcheck
  option httplog
  option forwardfor
  server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2
  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2
  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the network is used as an HAProxy front end. The following example shows that <code>controller-0</code> exposes the services by using the external network, which is absent from the Red Hat Ceph Storage nodes. You must propagate the external network through director:</p>
<div class="listingblock">
<div class="content">
<pre>[controller-0]$ ip -o -4 a

...
7: br-ex	inet 10.0.0.106/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever
...</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If the target nodes are not managed by director, you cannot use this procedure to configure the network. An administrator must manually configure all the required networks.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Propagate the HAProxy front-end network to Red Hat Ceph Storage nodes.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the NIC template that you use to define the <code>ceph-storage</code> network interfaces, add the new config section in the Red Hat Ceph Storage network configuration template file, for example, <code>/home/stack/composable_roles/network/nic-configs/ceph-storage.j2</code>:</p>
<div class="listingblock">
<div class="content">
<pre>---
network_config:
- type: interface
  name: nic1
  use_dhcp: false
  dns_servers: {{ ctlplane_dns_nameservers }}
  addresses:
  - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
  routes: {{ ctlplane_host_routes }}
- type: vlan
  vlan_id: {{ storage_mgmt_vlan_id }}
  device: nic1
  addresses:
  - ip_netmask: {{ storage_mgmt_ip }}/{{ storage_mgmt_cidr }}
  routes: {{ storage_mgmt_host_routes }}
- type: interface
  name: nic2
  use_dhcp: false
  defroute: false
- type: vlan
  vlan_id: {{ storage_vlan_id }}
  device: nic2
  addresses:
  - ip_netmask: {{ storage_ip }}/{{ storage_cidr }}
  routes: {{ storage_host_routes }}
- type: ovs_bridge
  name: {{ neutron_physical_bridge_name }}
  dns_servers: {{ ctlplane_dns_nameservers }}
  domain: {{ dns_search_domains }}
  use_dhcp: false
  addresses:
  - ip_netmask: {{ external_ip }}/{{ external_cidr }}
  routes: {{ external_host_routes }}
  members: []
  - type: interface
    name: nic3
    primary: true</pre>
</div>
</div>
</li>
<li>
<p>Add the External Network to the bare metal file, for example, <code>/home/stack/composable_roles/network/baremetal_deployment.yaml</code> that is used by <code>metalsmith</code>:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Ensure that <em>network_config_update</em> is enabled for network propagation to the target nodes when <code>os-net-config</code> is triggered.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
  name: ceph-0
  - hostname: cephstorage-1
  name: ceph-1
  - hostname: cephstorage-2
  name: ceph-2
  defaults:
  profile: ceph-storage
  network_config:
      template: /home/stack/composable_roles/network/nic-configs/ceph-storage.j2
      network_config_update: true
  networks:
  - network: ctlplane
      vif: true
  - network: storage
  - network: storage_mgmt
  - network: external</pre>
</div>
</div>
</li>
<li>
<p>Configure the new network on the bare metal nodes:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud) [stack@undercloud-0]$ openstack overcloud node provision \
   -o overcloud-baremetal-deployed-0.yaml \
   --stack overcloud \
   --network-config -y \
  $PWD/composable_roles/network/baremetal_deployment.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the new network is configured on the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>[root@cephstorage-0 ~]# ip -o -4 a

1: lo	inet 127.0.0.1/8 scope host lo\   	valid_lft forever preferred_lft forever
2: enp1s0	inet 192.168.24.54/24 brd 192.168.24.255 scope global enp1s0\   	valid_lft forever preferred_lft forever
11: vlan40	inet 172.17.4.43/24 brd 172.17.4.255 scope global vlan40\   	valid_lft forever preferred_lft forever
12: vlan30	inet 172.17.3.23/24 brd 172.17.3.255 scope global vlan30\   	valid_lft forever preferred_lft forever
14: br-ex	inet 10.0.0.133/24 brd 10.0.0.255 scope global br-ex\   	valid_lft forever preferred_lft forever</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="completing-prerequisites-for-rbd-migration_ceph-prerequisites">1.11.3. Completing prerequisites for a Red Hat Ceph Storage RBD migration</h4>
<div class="paragraph _abstract">
<p>Complete the following prerequisites before you begin the Red Hat Ceph Storage Rados Block Device (RBD) migration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The target CephStorage or ComputeHCI nodes are configured to have both <code>storage</code> and <code>storage_mgmt</code> networks. This ensures that you can use both Red Hat Ceph Storage public and cluster networks from the same node. From Red&#160;Hat OpenStack Platform 17.1 and later you do not have to run a stack update.</p>
</li>
<li>
<p>NFS Ganesha is migrated from a director deployment to <code>cephadm</code>. For more information, see <a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating an NFS Ganesha
cluster</a>.</p>
</li>
<li>
<p>Ceph Metadata Server, monitoring stack, Ceph Object Gateway, and any other daemon that is deployed on Controller nodes.</p>
</li>
<li>
<p>The daemons distribution follows the cardinality constraints that are
described in <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph
Storage: Supported configurations</a>.</p>
</li>
<li>
<p>The Red Hat Ceph Storage cluster is healthy, and the <code>ceph -s</code> command returns <code>HEALTH_OK</code>.</p>
</li>
<li>
<p>Run <code>os-net-config</code> on the bare metal node and configure additional networks:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If target nodes are <code>CephStorage</code>, ensure that the network is defined in the
bare metal file for the <code>CephStorage</code> nodes, for example, <code>/home/stack/composable_roles/network/baremetal_deployment.yaml</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
count: 2
instances:
- hostname: oc0-ceph-0
name: oc0-ceph-0
- hostname: oc0-ceph-1
name: oc0-ceph-1
defaults:
networks:
- network: ctlplane
vif: true
- network: storage_cloud_0
subnet: storage_cloud_0_subnet
- network: storage_mgmt_cloud_0
subnet: storage_mgmt_cloud_0_subnet
network_config:
template: templates/single_nic_vlans/single_nic_vlans_storage.j2</code></pre>
</div>
</div>
</li>
<li>
<p>Add the missing network:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack overcloud node provision \
-o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \
/--network-config -y --concurrency 2 /home/stack/metalsmith-0.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the storage network is configured on the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>(undercloud) [stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\       valid_lft forever preferred_lft forever
6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\       valid_lft forever preferred_lft forever
7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\       valid_lft forever preferred_lft forever
8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\       valid_lft forever preferred_lft forever</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="creating-a-ceph-nfs-cluster_ceph-prerequisites">1.11.4. Creating an NFS Ganesha cluster</h4>
<div class="paragraph _abstract">
<p>If you use CephFS through NFS with the Shared File Systems service (manila), you must create a new clustered NFS service on the Red Hat Ceph Storage cluster. This service replaces the standalone, Pacemaker-controlled <code>ceph-nfs</code> service that you use in Red&#160;Hat OpenStack Platform (RHOSP) 17.1.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Identify the Red Hat Ceph Storage nodes to deploy the new clustered NFS service, for example, <code>cephstorage-0</code>, <code>cephstorage-1</code>, <code>cephstorage-2</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must deploy this service on the <code>StorageNFS</code> isolated network so that you can mount your existing shares through the new NFS export locations.
You can deploy the new clustered NFS service on your existing CephStorage nodes or HCI nodes, or on new hardware that you enrolled in the Red Hat Ceph Storage cluster.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you deployed your Red Hat Ceph Storage nodes with director, propagate the <code>StorageNFS</code> network to the target nodes where the <code>ceph-nfs</code> service is deployed.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If the target nodes are not managed by director, you cannot use this procedure to configure the network. An administrator must manually configure all the required networks.
</td>
</tr>
</table>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Identify the node definition file, <code>overcloud-baremetal-deploy.yaml</code>, that is used in the RHOSP environment.
For more information about identifying the <code>overcloud-baremetal-deploy.yaml</code> file, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_the_red_hat_openstack_services_on_openshift_deployment/index#assembly_customizing-overcloud-networks">Customizing overcloud networks</a> in <em>Customizing the Red Hat OpenStack Services on OpenShift deployment</em>.</p>
</li>
<li>
<p>Edit the networks that are associated with the Red Hat Ceph Storage nodes to include the <code>StorageNFS</code> network:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
    name: ceph-0
  - hostname: cephstorage-1
    name: ceph-1
  - hostname: cephstorage-2
    name: ceph-2
  defaults:
    profile: ceph-storage
    network_config:
      template: /home/stack/network/nic-configs/ceph-storage.j2
      network_config_update: true
    networks:
    - network: ctlplane
      vif: true
    - network: storage
    - network: storage_mgmt
    - network: storage_nfs</code></pre>
</div>
</div>
</li>
<li>
<p>Edit the network configuration template file, for example, <code>/home/stack/network/nic-configs/ceph-storage.j2</code>, for the Red Hat Ceph Storage nodes
to include an interface that connects to the <code>StorageNFS</code> network:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">- type: vlan
  device: nic2
  vlan_id: {{ storage_nfs_vlan_id }}
  addresses:
  - ip_netmask: {{ storage_nfs_ip }}/{{ storage_nfs_cidr }}
  routes: {{ storage_nfs_host_routes }}</code></pre>
</div>
</div>
</li>
<li>
<p>Update the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack overcloud node provision \
    --stack overcloud   \
    --network-config -y  \
    -o overcloud-baremetal-deployed-storage_nfs.yaml \
    --concurrency 2 \
    /home/stack/network/baremetal_deployment.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>When the update is complete, ensure that a new interface is created in theRed Hat Ceph Storage nodes and that they are tagged with the VLAN that is associated with <code>StorageNFS</code>.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Identify the IP address from the <code>StorageNFS</code> network to use as the Virtual IP
address (VIP) for the Ceph NFS service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack port list -c "Fixed IP Addresses" --network storage_nfs</pre>
</div>
</div>
</li>
<li>
<p>In a running <code>cephadm</code> shell, identify the hosts for the NFS service:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch host ls</pre>
</div>
</div>
</li>
<li>
<p>Label each host that you identified. Repeat this command for each host that you want to label:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph orch host label add &lt;hostname&gt; nfs</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;hostname&gt;</code> with the name of the host that you identified.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create the NFS cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph nfs cluster create cephfs \
    "label:nfs" \
    --ingress \
    --virtual-ip=&lt;VIP&gt; \
    --ingress-mode=haproxy-protocol</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;VIP&gt;</code> with the VIP for the Ceph NFS service.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You must set the <code>ingress-mode</code> argument to <code>haproxy-protocol</code>. No other
ingress-mode is supported. This ingress mode allows you to enforce client
restrictions through the Shared File Systems service.
For more information about deploying the clustered Ceph NFS service, see <em>Management of NFS-Ganesha gateway using the Ceph Orchestrator</em> in the <em>Operations Guide</em> for your Red Hat Ceph Storage version:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.redhat.com/documentation/en-us/red_hat_ceph_storage/7/html/operations_guide/index#management-of-nfs-ganesha-gateway-using-the-ceph-orchestrator">Red Hat Ceph Storage 7 <em>Operations Guide</em></a></p>
</li>
<li>
<p><a href="https://docs.redhat.com/documentation/en-us/red_hat_ceph_storage/8/html/operations_guide/index#management-of-nfs-ganesha-gateway-using-the-ceph-orchestrator">Red Hat Ceph Storage 8 <em>Operations Guide</em></a></p>
</li>
<li>
<p><a href="https://docs.redhat.com/documentation/en-us/red_hat_ceph_storage/9/html/operations_guide/index#management-of-nfs-ganesha-gateway-using-the-ceph-orchestrator">Red Hat Ceph Storage 9 <em>Operations Guide</em></a></p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Check the status of the NFS cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph nfs cluster ls
$ ceph nfs cluster info cephfs</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="preparing-an-instance-HA-deployment-for-adoption_configuring-network">1.12. Preparing an Instance HA deployment for adoption</h3>
<div class="paragraph _abstract">
<p>To enable the high availability for Compute instances (Instance HA) service after you adopt the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 data plane, perform the following preparation tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Create a fencing configuration file to use after you adopt the RHOSO data plane.</p>
</li>
<li>
<p>Prevent Pacemaker from monitoring or recovering the Compute nodes.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="maintaining-instance-ha-functionality-after-adoption_preparing-instance-HA">1.12.1. Maintaining the Instance HA functionality after adoption</h4>
<div class="paragraph _abstract">
<p>To maintain the high availability for Compute instances (Instance HA) functionality after you adopt Red&#160;Hat OpenStack Services on OpenShift 18.0, create a fencing configuration file to use in your adopted environment.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Gather the fencing information from the <code>fencing.yaml</code> file in your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 cluster.</p>
</li>
<li>
<p>Retrieve the RHOSP 17.1 stonith configuration from any of your overcloud Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs config</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>Stonith Devices:
...
  Resource: stonith-fence_ipmilan-525400dde4f7 (class=stonith
      type=fence_ipmilan)
    Attributes: stonith-fence_ipmilan-525400dde4f7-instance_attributes
      delay=20
      ipaddr=172.16.0.1
      ipport=6231
      lanplus=true
      login=admin
      passwd=password
      pcmk_host_list=compute-1
    Operations:
      monitor: stonith-fence_ipmilan-525400dde4f7-monitor-interval-60s
        interval=60s
  Resource: stonith-fence_ipmilan-525400819ad3 (class=stonith
      type=fence_ipmilan)
    Attributes: stonith-fence_ipmilan-525400819ad3-instance_attributes
      delay=20
      ipaddr=172.16.0.1
      ipport=6230
      lanplus=true
      login=admin
      passwd=password
      pcmk_host_list=compute-0
    Operations:
      monitor: stonith-fence_ipmilan-525400819ad3-monitor-interval-60s
        interval=60s
...</pre>
</div>
</div>
</li>
<li>
<p>Generate the fencing configuration file:</p>
<div class="ulist">
<ul>
<li>
<p>To install the script that automatically generates this file, see <a href="https://access.redhat.com/solutions/7123932">How do I automatically generate fencing secret for RHOSO18 instanceha from a osp17.1 cluster that I want to adopt?</a>.</p>
</li>
<li>
<p>To create the fencing configuration file manually, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_high_availability_for_instances/assembly_deploying-and-configuring-the-high-availability-for-compute-instances-service_instance-ha#proc_configuring-the-fencing-of-compute-nodes_instance-ha">Configuring the fencing of Compute nodes</a> in <em>Configuring high availability for instances</em>.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="preventing-pacemaker-from-monitoring-compute-nodes_preparing-instance-HA">1.12.2. Preventing Pacemaker from monitoring Compute nodes</h4>
<div class="paragraph _abstract">
<p>You must disable Pacemaker so that it does not monitor your Compute nodes during the adoption. For example, if a network issue occurs during the adoption, Pacemaker attempts to reboot the Compute nodes to recover them, which breaks the adoption.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Retrieve the names of the Compute remote resources:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs stonith |grep -B1 stonith-fence_compute-fence-nova |grep Target |awk -F ': ' '{print $2}'</pre>
</div>
</div>
</li>
<li>
<p>Disable the stonith and <code>pacemaker_remote</code> resources on each Compute remote resource:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs property set stonith-enabled=false
$ sudo pcs resource disable &lt;compute_remote_resource&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;compute_remote_resource&gt;</code></dt>
<dd>
<p>Specifies the name of the Compute remote resource in your environment.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Retrieve the name of the Compute stonith resources:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs stonith |grep Level |grep fence_compute |awk '{print $4}' |awk -F ',' '{print $1}' |sort |uniq</pre>
</div>
</div>
</li>
<li>
<p>Remove the Compute node <code>pacemaker_remote</code> and fencing resources:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs stonith disable stonith-fence_compute-fence-nova
$ sudo pcs stonith disable &lt;compute_stonith_resource&gt;
$ sudo pcs stonith delete &lt;compute_stonith_resource&gt;
$ sudo pcs resource delete &lt;compute_remote_resource&gt;
$ sudo pcs resource disable compute-unfence-trigger-clone
$ sudo pcs resource delete compute-unfence-trigger-clone
$ sudo pcs resource disable nova-evacuate
$ sudo pcs resource delete nova-evacuate</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;compute_stonith_resource&gt;</code></dt>
<dd>
<p>Specifies the name of the Compute stonith resource in your environment.</p>
</dd>
</dl>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="comparing-configuration-files-between-deployments_configuring-network">1.13. Comparing configuration files between deployments</h3>
<div class="paragraph _abstract">
<p>To help you manage the configuration for your director and Red&#160;Hat OpenStack Platform (RHOSP) services, you can compare the configuration files between your director deployment and the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) cloud by using the os-diff tool.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Golang is installed and configured on your environment:</p>
<div class="listingblock">
<div class="content">
<pre>dnf install -y golang-github-openstack-k8s-operators-os-diff</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure the <code>/etc/os-diff/os-diff.cfg</code> file and the <code>/etc/os-diff/ssh.config</code> file according to your environment. To allow os-diff to connect to your clouds and pull files from the services that you describe in the <code>config.yaml</code> file, you must set the following options in the <code>os-diff.cfg</code> file:</p>
<div class="listingblock">
<div class="content">
<pre>[Default]

local_config_dir=/tmp/
service_config_file=config.yaml

[Tripleo]

<strong>ssh_cmd=ssh -F ssh.config</strong>
<strong>director_host=standalone</strong>
container_engine=podman
connection=ssh
remote_config_path=/tmp/tripleo
local_config_path=/tmp/

[Openshift]

ocp_local_config_path=/tmp/ocp
connection=local
ssh_cmd=""</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>ssh_cmd=ssh -F ssh.config</code> instructs os-diff to access your director host through SSH. The default value is <code>ssh -F ssh.config</code>. However, you can set the value without an ssh.config file, for example, <code>ssh -i /home/user/.ssh/id_rsa stack@my.undercloud.local</code>.</p>
</li>
<li>
<p><code>director_host=standalone</code> specifies the host to use to access your cloud, and the podman/docker binary is installed and allowed to interact with the running containers. You can leave this key blank.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If you use a host file to connect to your cloud, configure the <code>ssh.config</code> file to allow os-diff to access your RHOSP environment, for example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">Host *
    IdentitiesOnly yes

Host virthost
    Hostname virthost
    IdentityFile ~/.ssh/id_rsa
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null


Host standalone
    Hostname standalone
    IdentityFile &lt;path to SSH key&gt;
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null

Host crc
    Hostname crc
    IdentityFile ~/.ssh/id_rsa
    User stack
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;path to SSH key&gt;</code> with the path to your SSH key. You must provide a value for <code>IdentityFile</code> to get full working access to your RHOSP environment.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If you use an inventory file to connect to your cloud, generate the <code>ssh.config</code> file from your Ansible inventory, for example, <code>tripleo-ansible-inventory.yaml</code> file:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff configure -i tripleo-ansible-inventory.yaml -o ssh.config --yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Test your connection:</p>
<div class="listingblock">
<div class="content">
<pre>$ ssh -F ssh.config standalone</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="preventing-config-loss-when-using-oc-patch_configuring-network">1.14. Preventing configuration loss when using the <code>oc patch</code> command</h3>
<div class="paragraph _abstract">
<p>When you use the <code>oc patch</code> command to modify a resource, the changes are applied directly to the live object in your OpenShift cluster. If you later edit the custom resource (CR) file for the resource and apply the updates by using <code>oc apply -f &lt;filename&gt;</code>, your previous patched changes are overwritten and lost from the resource.</p>
</div>
<div class="paragraph">
<p>To prevent loss of configuration, you can use the <code>--patch-file</code> option to configure the patch and retain patch files. Alternatively, you can export your <code>openstackcontrolplane</code> CR after the patch is applied:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get &lt;resource_type&gt; &lt;resource_name&gt; -o yaml &gt; &lt;filename&gt;.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get OpenStackControlPlane openstack-control-plane -o yaml &gt; openstack_control_plane.yaml</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-tls-everywhere_configuring-network">2. Migrating TLS-e to the RHOSO deployment</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>If you enabled TLS everywhere (TLS-e) in your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment, you must migrate TLS-e to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment.</p>
</div>
<div class="paragraph">
<p>The RHOSO deployment uses the cert-manager operator to issue, track, and renew the certificates. In the following procedure, you extract the CA signing certificate from the FreeIPA instance that you use to provide the certificates in the RHOSP environment, and then import them into cert-manager in the RHOSO environment. As a result, you minimize the disruption on the Compute nodes because you do not need to install a new chain of trust.</p>
</div>
<div class="paragraph">
<p>You then decommission the previous FreeIPA node and no longer use it to issue certificates. This might not be possible if you use the IPA server to issue certificates for non-RHOSP systems.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>The following procedure was reproduced on a FreeIPA 4.10.1 server. The location of the files and directories might change depending on the version.</p>
</li>
<li>
<p>If the signing keys are stored in an hardware security module (HSM) instead of an NSS shared database (NSSDB), and the keys are retrievable, special HSM utilities might be required.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Your RHOSP deployment is using TLS-e.</p>
</li>
<li>
<p>Ensure that the back-end services on the new deployment are not started yet.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single-node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>IPA_SSH="ssh -i &lt;path_to_ssh_key&gt; &lt;admin user&gt;@&lt;freeipa-server-ip-address&gt; sudo"</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To locate the CA certificate and key, list all the certificates inside your NSSDB:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH certutil -L -d /etc/pki/pki-tomcat/alias</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>-L</code> option lists all certificates.</p>
</li>
<li>
<p>The <code>-d</code> option specifies where the certificates are stored.</p>
<div class="paragraph">
<p>The command produces an output similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Certificate Nickname                                         Trust Attributes
                                                             SSL,S/MIME,JAR/XPI

caSigningCert cert-pki-ca                                    CTu,Cu,Cu
ocspSigningCert cert-pki-ca                                  u,u,u
Server-Cert cert-pki-ca                                      u,u,u
subsystemCert cert-pki-ca                                    u,u,u
auditSigningCert cert-pki-ca                                 u,u,Pu</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Export the certificate and key from the <code>/etc/pki/pki-tomcat/alias</code> directory. The following example uses the <code>caSigningCert cert-pki-ca</code> certificate:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH pk12util -o /tmp/freeipa.p12 -n 'caSigningCert\ cert-pki-ca' -d /etc/pki/pki-tomcat/alias -k /etc/pki/pki-tomcat/alias/pwdfile.txt -w /etc/pki/pki-tomcat/alias/pwdfile.txt</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The command generates a P12 file with both the certificate and the key. The <code>/etc/pki/pki-tomcat/alias/pwdfile.txt</code> file contains the password that protects the key. You can use the password to both extract the key and generate the new file, <code>/tmp/freeipa.p12</code>. You can also choose another password. If you choose a different password for the new file, replace the parameter of the <code>-w</code> option, or use the <code>-W</code> option followed by the password, in clear text.</p>
</div>
<div class="paragraph">
<p>With that file, you can also get the certificate and the key by using the <code>openssl pkcs12</code> command.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create the secret that contains the root CA:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic rootca-internal</pre>
</div>
</div>
</li>
<li>
<p>Import the certificate and the key from FreeIPA:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch secret rootca-internal -p="{\"data\":{\"ca.crt\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nokeys | openssl x509 | base64 -w 0`\"}}"

$ oc patch secret rootca-internal -p="{\"data\":{\"tls.crt\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nokeys | openssl x509 | base64 -w 0`\"}}"

$ oc patch secret rootca-internal -p="{\"data\":{\"tls.key\": \"`$IPA_SSH openssl pkcs12 -in /tmp/freeipa.p12 -passin file:/etc/pki/pki-tomcat/alias/pwdfile.txt -nocerts -noenc | openssl rsa | base64 -w 0`\"}}"</pre>
</div>
</div>
</li>
<li>
<p>Create the cert-manager issuer and reference the secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: rootca-internal
  labels:
    osp-rootca-issuer-public: ""
    osp-rootca-issuer-internal: ""
    osp-rootca-issuer-libvirt: ""
    osp-rootca-issuer-ovn: ""
spec:
  ca:
    secretName: rootca-internal
EOF</pre>
</div>
</div>
</li>
<li>
<p>Delete the previously created p12 files:</p>
<div class="listingblock">
<div class="content">
<pre>$IPA_SSH rm /tmp/freeipa.p12</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that the necessary resources are created:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get issuers</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get secret rootca-internal -o yaml</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After the adoption is complete, the cert-manager operator issues new certificates and updates the secrets with the new certificates. As a result, the pods on the control plane automatically restart in order to obtain the new certificates. On the data plane, you must manually initiate a new deployment and restart certain processes to use the new certificates. The old certificates remain active until both the control plane and data plane obtain the new certificates.
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-databases-to-the-control-plane_configuring-network">3. Migrating databases to the control plane</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>To begin creating the control plane, enable back-end services and import the databases from your original Red&#160;Hat OpenStack Platform 17.1 deployment.</p>
</div>
<div class="sect2">
<h3 id="proc_retrieving-topology-specific-service-configuration_migrating-databases">3.1. Retrieving topology-specific service configuration</h3>
<div class="paragraph _abstract">
<p>Before you migrate your databases to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, retrieve the topology-specific service configuration from your Red&#160;Hat OpenStack Platform (RHOSP) environment. You need this configuration for the following reasons:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>To check your current database for inaccuracies</p>
</li>
<li>
<p>To ensure that you have the data you need before the migration</p>
</li>
<li>
<p>To compare your RHOSP database with the adopted RHOSO database</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Prerequisites</div>
<ol class="arabic">
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you use IPv6, define the <code>SOURCE_MARIADB_IP</code> value without brackets. For example, <code>SOURCE_MARIADB_IP=fd00:bbbb::2</code>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ PASSWORD_FILE="$HOME/overcloud-passwords.yaml"
$ MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0
$ declare -A TRIPLEO_PASSWORDS
$ CELLS="default cell1 cell2"
$ for CELL in $(echo $CELLS); do
&gt;    TRIPLEO_PASSWORDS[$CELL]="$PASSWORD_FILE"
&gt; done
$ declare -A SOURCE_DB_ROOT_PASSWORD
$ for CELL in $(echo $CELLS); do
&gt;     SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
&gt; done</pre>
</div>
</div>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=internalapi'
$ MARIADB_RUN_OVERRIDES="$MARIADB_CLIENT_ANNOTATIONS"</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For environments that are enabled with border gateway protocol (BGP), the network annotation must include a default route to enable proper routing. Use the following instead:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ MARIADB_CLIENT_ANNOTATIONS='--annotations=k8s.v1.cni.cncf.io/networks=[{"name":"internalapi","namespace":"openstack","default-route":["&lt;172.17.0.1&gt;"]}]'
$ MARIADB_RUN_OVERRIDES="$MARIADB_CLIENT_ANNOTATIONS"</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.1&gt;</dt>
<dd>
<p>Replace with the gateway IP address of your <code>internalapi</code> network.</p>
</dd>
</dl>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH="ssh -i *&lt;path to SSH key&gt;* root@*&lt;node IP&gt;*"

$ declare -A SOURCE_MARIADB_IP
$ SOURCE_MARIADB_IP[default]=*&lt;galera cluster VIP&gt;*
$ SOURCE_MARIADB_IP[cell1]=*&lt;galera cell1 cluster VIP&gt;*
$ SOURCE_MARIADB_IP[cell2]=*&lt;galera cell2 cluster VIP&gt;*
# ...</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Provide <code>CONTROLLER1_SSH</code> settings with SSH connection details for any non-cell Controller of the source director cloud.</p>
</li>
<li>
<p>For each cell that is defined in <code>CELLS</code>, replace <code>SOURCE_MARIADB_IP[*]= ...</code>, with the records lists for the cell names and VIP addresses of MariaDB Galera clusters, including the cells, of the source director cloud.</p>
</li>
<li>
<p>To get the values for <code>SOURCE_MARIADB_IP</code>, query the puppet-generated configurations in a Controller
and CellController
node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>If your source RHOSP environment uses border gateway protocol (BGP) for Layer 3 networking, create a <code>BGPConfiguration</code> custom resource to enable BGP routing:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; bgp.yaml
apiVersion: network.openstack.org/v1beta1
kind: BGPConfiguration
metadata:
  name: bgpconfiguration
  namespace: openstack
spec: {}
EOF

$ oc apply -f bgp.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>BGPConfiguration</code> resource enables BGP route advertisement between the Red Hat OpenShift Container Platform (RHOCP) cluster and the source cloud, which is necessary for the <code>mariadb-client</code> pod to reach the source MariaDB cluster.</p>
</div>
</li>
<li>
<p>Create a persistent <code>mariadb-client</code> pod for database operations:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod mariadb-client || true
$ oc run mariadb-client ${MARIADB_RUN_OVERRIDES} -q --image ${MARIADB_IMAGE} --restart=Never -- /usr/bin/sleep infinity</pre>
</div>
</div>
<div class="paragraph">
<p>This creates a long-running pod that is used for all subsequent database operations, avoiding the need to create temporary pods for each command.</p>
</div>
</li>
<li>
<p>Wait for the <code>mariadb-client</code> pod to be able to reach the source MariaDB:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh mariadb-client mysql -rsh "${SOURCE_MARIADB_IP[default]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[default]}" -e 'select 1;'</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For BGP-enabled environments, this command might take a few moments to succeed while BGP routes are advertised and propagated through the network. The <code>mariadb-client</code> pod needs to receive the route to the source MariaDB IP address through BGP before it can establish a connection. If the command fails, wait a few seconds and retry. The connection should succeed once the BGP route advertisement is complete.</p>
</div>
<div class="paragraph">
<p>For standard (non-BGP) deployments, this command should succeed immediately.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Export the shell variables for the following outputs and test the connection to the RHOSP database:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_DATABASES
$ declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
$ for CELL in $(echo $CELLS); do
&gt;     PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]=$(oc rsh mariadb-client \
&gt;         mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;')
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>If the connection is successful, the expected output is nothing.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>nova</code>, <code>nova_api</code>, and <code>nova_cell0</code> databases are included in the same database host for the main overcloud Orchestration service (heat) stack.
Additional cells use the <code>nova</code> database of their local Galera clusters.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run <code>mysqlcheck</code> on the RHOSP database to check for inaccuracies:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
$ declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
$ run_mysqlcheck() {
&gt;     PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK=$(oc rsh mariadb-client \
&gt;         mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" | grep -v OK)
&gt; }
$ for CELL in $(echo $CELLS); do
&gt;     run_mysqlcheck $CELL
&gt; done
$ if [ "$PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK" != "" ]; then
&gt;     for CELL in $(echo $CELLS); do
&gt;         MYSQL_UPGRADE=$(oc rsh mariadb-client \
&gt;             mysql_upgrade --skip-version-check -v -h ${SOURCE_MARIADB_IP[$CELL]} -u root -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}")
&gt;         # rerun mysqlcheck to check if problem is resolved
&gt;         run_mysqlcheck
&gt;     done
&gt; fi</pre>
</div>
</div>
</li>
<li>
<p>Get the Compute service (nova) cell mappings:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS=$(oc rsh mariadb-client \
    mysql -rsh "${SOURCE_MARIADB_IP[default]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[default]}" nova_api -e \
    'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')</pre>
</div>
</div>
</li>
<li>
<p>Get the hostnames of the registered Compute services:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
$ declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
$ for CELL in $(echo $CELLS); do
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]=$(oc rsh mariadb-client \
&gt;         mysql -rsh "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e \
&gt;             "select host from nova.services where services.binary='nova-compute' and deleted=0;")
&gt; done</pre>
</div>
</div>
</li>
<li>
<p>Get the list of the mapped Compute service cells:</p>
<div class="listingblock">
<div class="content">
<pre>export PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS=$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)</pre>
</div>
</div>
</li>
<li>
<p>Store the exported variables for future use:</p>
<div class="listingblock">
<div class="content">
<pre>$ unset SRIOV_AGENTS
$ declare -xA SRIOV_AGENTS
$ for CELL in $(echo $CELLS); do
&gt;     RCELL=$CELL
&gt;     [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
&gt;     cat &gt; ~/.source_cloud_exported_variables_$CELL &lt;&lt; EOF
&gt; unset PULL_OPENSTACK_CONFIGURATION_DATABASES
&gt; unset PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
&gt; unset PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_DATABASES
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK
&gt; declare -xA PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES
&gt; PULL_OPENSTACK_CONFIGURATION_DATABASES[$CELL]="$(oc rsh mariadb-client \
&gt;     mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e 'SHOW databases;')"
&gt; PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]="$(oc rsh mariadb-client \
&gt;     mysqlcheck --all-databases -h ${SOURCE_MARIADB_IP[$RCELL]} -u root -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} | grep -v OK)"
&gt; PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[$CELL]="$(oc rsh mariadb-client \
&gt;     mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} -e \
&gt;     "select host from nova.services where services.binary='nova-compute' and deleted=0;")"
&gt; if [ "$RCELL" = "default" ]; then
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS="$(oc rsh mariadb-client \
&gt;         mysql -rsh ${SOURCE_MARIADB_IP[$RCELL]} -uroot -p${SOURCE_DB_ROOT_PASSWORD[$RCELL]} nova_api -e \
&gt;             'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')"
&gt;     PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS="$($CONTROLLER1_SSH sudo podman exec -it nova_conductor nova-manage cell_v2 list_cells)"
&gt; fi
&gt; EOF
&gt; done
$ chmod 0600 ~/.source_cloud_exported_variables*</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>declare -xA SRIOV_AGENTS</code> gets the <code>neutron-sriov-nic-agent</code> configuration to use for the data plane adoption if <code>neutron-sriov-nic-agent</code> agents are running in your RHOSP deployment.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Clean up the <code>mariadb-client</code> pod:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod mariadb-client</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>mariadb-client</code> pod is no longer needed after all the data is exported and stored.</p>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>This configuration and the exported values are required later, during the data plane adoption post-checks. After the RHOSP control plane services are shut down, if any of the exported values are lost, re-running the <code>export</code> command fails because the control plane services are no longer running on the source cloud, and the data cannot be retrieved. To avoid data loss, preserve the exported values in an environment file before shutting down the control plane services.</p>
</div>
</div>
<div class="sect2">
<h3 id="deploying-backend-services_migrating-databases">3.2. Deploying back-end services</h3>
<div class="paragraph _abstract">
<p>Create the <code>OpenStackControlPlane</code> custom resource (CR) with the basic back-end services deployed, and disable all the Red&#160;Hat OpenStack Platform (RHOSP) services. This CR is the foundation of the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The cloud that you want to adopt is running, and it is on RHOSP 17.1.4 or later.</p>
</li>
<li>
<p>All control plane and data plane hosts of the source cloud are running, and continue to run throughout the adoption procedure.</p>
</li>
<li>
<p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is
not deployed.</p>
</li>
<li>
<p>Install the OpenStack Operators. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#assembly_installing-and-preparing-the-OpenStack-Operator">Installing and preparing the OpenStack Operator</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If you enabled TLS everywhere (TLS-e) on the RHOSP environment, you must copy the <code>tls</code> root CA from the RHOSP environment to the <code>rootca-internal</code> issuer.</p>
</li>
<li>
<p>There are free PVs available for Galera and RabbitMQ.</p>
</li>
<li>
<p>Set the desired admin password for the control plane deployment. This can
be the admin password from your original deployment or a different password:</p>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PASSWORD=SomePassword</pre>
</div>
</div>
<div class="paragraph">
<p>To use the existing RHOSP deployment password:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AdminPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
<li>
<p>Set the service password variables to match the original deployment.
Database passwords can differ in the control plane environment, but
you must synchronize the service account passwords.</p>
<div class="paragraph">
<p>For example, in developer environments with director Standalone, the passwords can be extracted:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>AODH_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' AodhPassword:' | awk -F ': ' '{ print $2; }')
BARBICAN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' BarbicanPassword:' | awk -F ': ' '{ print $2; }')
CEILOMETER_METERING_SECRET=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerMeteringSecret:' | awk -F ': ' '{ print $2; }')
CEILOMETER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CeilometerPassword:' | awk -F ': ' '{ print $2; }')
CINDER_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')
DESIGNATE_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' DesignatePassword:' | awk -F ': ' '{ print $2; }')
GLANCE_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')
HEAT_AUTH_ENCRYPTION_KEY=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatAuthEncryptionKey:' | awk -F ': ' '{ print $2; }')
HEAT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatPassword:' | awk -F ': ' '{ print $2; }')
HEAT_STACK_DOMAIN_ADMIN_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' HeatStackDomainAdminPassword:' | awk -F ': ' '{ print $2; }')
IRONIC_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')
MANILA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' ManilaPassword:' | awk -F ': ' '{ print $2; }')
NEUTRON_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')
NOVA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')
OCTAVIA_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')
PLACEMENT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')
SWIFT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' SwiftPassword:' | awk -F ': ' '{ print $2; }')</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ensure that you are using the Red Hat OpenShift Container Platform (RHOCP) namespace where you want the
control plane to be deployed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc project openstack</pre>
</div>
</div>
</li>
<li>
<p>Create the RHOSP secret. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html-single/deploying_red_hat_openstack_services_on_openshift/index#proc_providing-secure-access-to-the-RHOSO-services_preparing">Providing secure access to the Red Hat OpenStack Services on OpenShift services</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
<li>
<p>If the <code>$ADMIN_PASSWORD</code> is different than the password you set
in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AdminPassword=$ADMIN_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Set service account passwords in <code>osp-secret</code> to match the service
account passwords from the original deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "AodhPassword=$AODH_PASSWORD"
$ oc set data secret/osp-secret "BarbicanPassword=$BARBICAN_PASSWORD"
$ oc set data secret/osp-secret "CeilometerPassword=$CEILOMETER_PASSWORD"
$ oc set data secret/osp-secret "CinderPassword=$CINDER_PASSWORD"
$ oc set data secret/osp-secret "DesignatePassword=$DESIGNATE_PASSWORD"
$ oc set data secret/osp-secret "GlancePassword=$GLANCE_PASSWORD"
$ oc set data secret/osp-secret "HeatAuthEncryptionKey=$HEAT_AUTH_ENCRYPTION_KEY"
$ oc set data secret/osp-secret "HeatPassword=$HEAT_PASSWORD"
$ oc set data secret/osp-secret "HeatStackDomainAdminPassword=$HEAT_STACK_DOMAIN_ADMIN_PASSWORD"
$ oc set data secret/osp-secret "IronicPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "IronicInspectorPassword=$IRONIC_PASSWORD"
$ oc set data secret/osp-secret "ManilaPassword=$MANILA_PASSWORD"
$ oc set data secret/osp-secret "MetadataSecret=$METADATA_SECRET"
$ oc set data secret/osp-secret "NeutronPassword=$NEUTRON_PASSWORD"
$ oc set data secret/osp-secret "NovaPassword=$NOVA_PASSWORD"
$ oc set data secret/osp-secret "OctaviaPassword=$OCTAVIA_PASSWORD"
$ oc set data secret/osp-secret "PlacementPassword=$PLACEMENT_PASSWORD"
$ oc set data secret/osp-secret "SwiftPassword=$SWIFT_PASSWORD"</pre>
</div>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackControlPlane</code> CR. Ensure that you only enable the DNS, Galera, Memcached, and RabbitMQ services. All other services must
be disabled:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc apply -f - &lt;&lt;EOF
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: &lt;storage_class&gt;

  barbican:
    enabled: false
    template:
      barbicanAPI: {}
      barbicanWorker: {}
      barbicanKeystoneListener: {}

  cinder:
    enabled: false
    template:
      cinderAPI: {}
      cinderScheduler: {}
      cinderBackup: {}
      cinderVolumes: {}

  dns:
    template:
      override:
        service:
          metadata:
            annotations:
              metallb.universe.tf/address-pool: ctlplane
              metallb.universe.tf/allow-shared-ip: ctlplane
              metallb.universe.tf/loadBalancerIPs: &lt;loadBalancer_IP&gt;

          spec:
            type: LoadBalancer
      options:
      - key: server
        values:
        - 192.168.122.1
      replicas: 1

  glance:
    enabled: false
    template:
      glanceAPIs: {}

  heat:
    enabled: false
    template: {}

  horizon:
    enabled: false
    template: {}

  ironic:
    enabled: false
    template:
      ironicConductors: []

  keystone:
    enabled: false
    template: {}

  manila:
    enabled: false
    template:
      manilaAPI: {}
      manilaScheduler: {}
      manilaShares: {}

  galera:
    enabled: true
    templates:
      openstack:
        secret: osp-secret
        replicas: 3
        storageRequest: 5G
      openstack-cell1:
        secret: osp-secret
        replicas: 3
        storageRequest: 5G
      openstack-cell2:
        secret: osp-secret
        replicas: 1
        storageRequest: 5G
      openstack-cell3:
        secret: osp-secret
        replicas: 1
        storageRequest: 5G
  memcached:
    enabled: true
    templates:
      memcached:
        replicas: 3

  neutron:
    enabled: false
    template: {}

  nova:
    enabled: false
    template: {}

  ovn:
    enabled: false
    template:
      ovnController:
        networkAttachment: tenant
        nodeSelector:
          node: non-existing-node-name
      ovnNorthd:
        replicas: 0
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          networkAttachment: internalapi

  placement:
    enabled: false
    template: {}

  rabbitmq:
    templates:
      rabbitmq:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: &lt;loadBalancer_IP&gt;
            spec:
              type: LoadBalancer
      rabbitmq-cell1:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: &lt;loadBalancer_IP&gt;

            spec:
              type: LoadBalancer
      rabbitmq-cell2:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: &lt;loadBalancer_IP&gt;
            spec:
              type: LoadBalancer
      rabbitmq-cell3:
        persistence:
          storage: 1G
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: &lt;loadBalancer_IP&gt;
            spec:
              type: LoadBalancer
  telemetry:
    enabled: false
  tls:
    podLevel:
      enabled: false
    ingress:
      enabled: false
  swift:
    enabled: false
    template:
      swiftRing:
        ringReplicas: 3
      swiftStorage:
        replicas: 0
      swiftProxy:
        replicas: 2
EOF</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;storage_class&gt;</dt>
<dd>
<p>Specifies an existing storage class in your RHOCP cluster.</p>
</dd>
<dt class="hdlist1">&lt;loadBalancer_IP&gt;</dt>
<dd>
<p>Specifies the LoadBalancer IP address. If you use IPv6, change the load balancer IPs to the IPs in your environment, for example:</p>
<div class="listingblock">
<div class="content">
<pre>...
metallb.universe.tf/allow-shared-ip: ctlplane
metallb.universe.tf/loadBalancerIPs: fd00:aaaa::80
...
metallb.universe.tf/address-pool: internalapi
metallb.universe.tf/loadBalancerIPs: fd00:bbbb::85
...
metallb.universe.tf/address-pool: internalapi
metallb.universe.tf/loadBalancerIPs: fd00:bbbb::86</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>galera.openstack-cell1</code> provides the required infrastructure database and messaging services for the Compute cells, for example, <code>cell1</code>, <code>cell2</code>, and <code>cell3</code>. Adjust the values for fields such as <code>replicas</code>, <code>storage</code>, or <code>storageRequest</code>, for each Compute cell as needed.</p>
</li>
<li>
<p><code>spec.tls</code> specifies whether TLS-e is enabled. If you enabled TLS-e in your RHOSP environment, set <code>tls</code> to the following:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  ...
  tls:
    podLevel:
      enabled: true
      internal:
        ca:
          customIssuer: rootca-internal
      libvirt:
        ca:
          customIssuer: rootca-internal
      ovn:
        ca:
          customIssuer: rootca-internal
    ingress:
      ca:
        customIssuer: rootca-internal
      enabled: true</pre>
</div>
</div>
</li>
</ul>
</div>
</dd>
</dl>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that the Galera and RabbitMQ status is <code>Running</code> for all defined cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ RENAMED_CELLS="cell1 cell2 cell3"
$ oc get pod openstack-galera-0 -o jsonpath='{.status.phase}{"\n"}'
$ oc get pod rabbitmq-server-0 -o jsonpath='{.status.phase}{"\n"}'
$ for CELL in $(echo $RENAMED_CELLS); do
&gt;     oc get pod openstack-$CELL-galera-0 -o jsonpath='{.status.phase}{"\n"}'
&gt;     oc get pod rabbitmq-$CELL-server-0 -o jsonpath='{.status.phase}{"\n"}'
&gt; done</pre>
</div>
</div>
<div class="paragraph">
<p>The given cells names are later referred to by using the environment variable <code>RENAMED_CELLS</code>.
During the database migration procedure, the Nova cells are renamed. <code>RENAMED_CELLS</code> variable represents the new cell names used in the RHOSO deployment.</p>
</div>
</li>
<li>
<p>Ensure that the statuses of all the Rabbitmq and Galera CRs are <code>Setup complete</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get Rabbitmqs,Galera
NAME                                                                  STATUS   MESSAGE
rabbitmq.rabbitmq.openstack.org/rabbitmq                              True     Setup complete
rabbitmq.rabbitmq.openstack.org/rabbitmq-cell1                        True     Setup complete

NAME                                                               READY   MESSAGE
galera.mariadb.openstack.org/openstack                             True     Setup complete
galera.mariadb.openstack.org/openstack-cell1                       True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Verify that the <code>OpenStackControlPlane</code> CR is waiting for deployment
of the <code>openstackclient</code> pod:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get OpenStackControlPlane openstack
NAME        STATUS    MESSAGE
openstack   Unknown   OpenStackControlPlane Client not started</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="configuring-a-ceph-backend_migrating-databases">3.3. Configuring a Red Hat Ceph Storage back end</h3>
<div class="paragraph _abstract">
<p>If your Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment uses a Red Hat Ceph Storage back end for any service, such as Image Service (glance), Block Storage service (cinder), Compute service (nova), or Shared File Systems service (manila), you must configure the custom resources (CRs) to use the same back end in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
To run <code>ceph</code> commands, you must use SSH to connect to a Red Hat Ceph Storage node and run <code>sudo cephadm shell</code>. This generates a Ceph orchestrator container that enables you to run administrative commands against the Red Hat Ceph Storage cluster. If you deployed the Red Hat Ceph Storage cluster by using director, you can launch the <code>cephadm</code> shell from an RHOSP Controller node.
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> CR is created.</p>
</li>
<li>
<p>If your RHOSP 17.1 deployment uses the Shared File Systems service, the openstack keyring is updated. Modify the <code>openstack</code> user so that you can use it across all RHOSP services:</p>
<div class="listingblock">
<div class="content">
<pre>ceph auth caps client.openstack \
  mgr 'allow *' \
  mon 'allow r, profile rbd' \
  osd 'profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'</pre>
</div>
</div>
<div class="paragraph">
<p>Using the same user across all services makes it simpler to create a common Red Hat Ceph Storage secret that includes the keyring and <code>ceph.conf</code> file and propagate the secret to all the services that need it.</p>
</div>
</li>
<li>
<p>The following shell variables are defined. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CEPH_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;node IP&gt;</strong>"
CEPH_KEY=$($CEPH_SSH "cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0")
CEPH_CONF=$($CEPH_SSH "cat /etc/ceph/ceph.conf | base64 -w 0")</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>ceph-conf-files</code> secret that includes the Red Hat Ceph Storage configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  ceph.client.openstack.keyring: $CEPH_KEY
  ceph.conf: $CEPH_CONF
kind: Secret
metadata:
  name: ceph-conf-files
type: Opaque
EOF</pre>
</div>
</div>
<div class="paragraph">
<p>The content of the file should be similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>apiVersion: v1
kind: Secret
metadata:
  name: ceph-conf-files
stringData:
  ceph.client.openstack.keyring: |
    [client.openstack]
        key = &lt;secret key&gt;
        caps mgr = "allow *"
        caps mon = "allow r, profile rbd"
        caps osd = "pool=vms, profile rbd pool=volumes, profile rbd pool=images, allow rw pool manila_data'
  ceph.conf: |
    [global]
    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4
    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>mon_host</code> specifies the addresses of the cluster&#8217;s monitors. If you use IPv6, use brackets for the <code>mon_host</code>. For example:
<code>mon_host = [v2:[fd00:cccc::100]:3300/0,v1:[fd00:cccc::100]:6789/0]</code></p>
</li>
</ul>
</div>
</li>
<li>
<p>In your <code>OpenStackControlPlane</code> CR, inject <code>ceph.conf</code> and <code>ceph.client.openstack.keyring</code> to the RHOSP services that are defined in the propagation list. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
          - CinderVolume
          - CinderBackup
          - GlanceAPI
          - ManilaShare
          extraVolType: Ceph
          volumes:
          - name: ceph
            projected:
              sources:
              - secret:
                  name: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
'</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="stopping-openstack-services_migrating-databases">3.4. Stopping Red&#160;Hat OpenStack Platform services</h3>
<div class="paragraph _abstract">
<p>Before you start the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) adoption, you must stop the Red&#160;Hat OpenStack Platform (RHOSP) services to avoid inconsistencies in the data that you migrate for the data plane adoption. Inconsistencies are caused by resource changes after the database is copied to the new deployment.</p>
</div>
<div class="paragraph">
<p>You should not stop the infrastructure management services yet, such as:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Database</p>
</li>
<li>
<p>RabbitMQ</p>
</li>
<li>
<p>HAProxy Load Balancer</p>
</li>
<li>
<p>Ceph-nfs</p>
</li>
<li>
<p>Compute service</p>
</li>
<li>
<p>Containerized modular libvirt daemons</p>
</li>
<li>
<p>Object Storage service (swift) back-end services</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that there no long-running tasks that require the services that you plan to stop, such as instance live migrations, volume migrations, volume creation, backup and restore, attaching, detaching, and other similar operations:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack server list --all-projects -c ID -c Status |grep -E '\| .+ing \|'
$ openstack volume list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
$ openstack volume backup list --all-projects -c ID -c Status |grep -E '\| .+ing \|' | grep -vi error
$ openstack share list --all-projects -c ID -c Status |grep -E '\| .+ing \|'| grep -vi error
$ openstack image list -c ID -c Status |grep -E '\| .+ing \|'</pre>
</div>
</div>
</li>
<li>
<p>Collect the services topology-specific configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Define the following shell variables. The values are examples and refer to a single node standalone director deployment. Replace these example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-1 IP&gt;</strong>"
CONTROLLER2_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-2 IP&gt;</strong>"
CONTROLLER3_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-3 IP&gt;</strong>"</pre>
</div>
</div>
</li>
<li>
<p>Specify the IP addresses of all Controller nodes, for example:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-1 IP&gt;</strong>"
CONTROLLER2_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-2 IP&gt;</strong>"
CONTROLLER3_SSH="ssh -i <strong>&lt;path to SSH key&gt;</strong> root@<strong>&lt;controller-3 IP&gt;</strong>"
# ...</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>&lt;path_to_SSH_key&gt;</code> defines the path to your SSH key.</p>
</li>
<li>
<p><code>&lt;controller-&lt;X&gt; IP&gt;</code> defines the IP addresses of all Controller nodes.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>If your deployment enables CephFS through NFS as a back end for Shared File Systems service (manila), remove the following Pacemaker ordering and co-location constraints that govern the Virtual IP address of the <code>ceph-nfs</code> service and the <code>manila-share</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># check the co-location and ordering constraints concerning "manila-share"
sudo pcs constraint list --full

# remove these constraints
sudo pcs constraint remove colocation-openstack-manila-share-ceph-nfs-INFINITY
sudo pcs constraint remove order-ceph-nfs-openstack-manila-share-Optional</code></pre>
</div>
</div>
</li>
<li>
<p>Disable RHOSP control plane services:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># Update the services list to be stopped
ServicesToStop=("tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_designate_api.service"
                "tripleo_designate_backend_bind9.service"
                "tripleo_designate_central.service"
                "tripleo_designate_mdns.service"
                "tripleo_designate_producer.service"
                "tripleo_designate_worker.service"
                "tripleo_octavia_api.service"
                "tripleo_octavia_health_manager.service"
                "tripleo_octavia_rsyslog.service"
                "tripleo_octavia_driver_agent.service"
                "tripleo_octavia_housekeeping.service"
                "tripleo_octavia_worker.service"
                "tripleo_horizon.service"
                "tripleo_keystone.service"
                "tripleo_barbican_api.service"
                "tripleo_barbican_worker.service"
                "tripleo_barbican_keystone_listener.service"
                "tripleo_cinder_api.service"
                "tripleo_cinder_api_cron.service"
                "tripleo_cinder_scheduler.service"
                "tripleo_cinder_volume.service"
                "tripleo_cinder_backup.service"
                "tripleo_collectd.service"
                "tripleo_glance_api.service"
                "tripleo_gnocchi_api.service"
                "tripleo_gnocchi_metricd.service"
                "tripleo_gnocchi_statsd.service"
                "tripleo_manila_api.service"
                "tripleo_manila_api_cron.service"
                "tripleo_manila_scheduler.service"
                "tripleo_neutron_api.service"
                "tripleo_placement_api.service"
                "tripleo_nova_api_cron.service"
                "tripleo_nova_api.service"
                "tripleo_nova_conductor.service"
                "tripleo_nova_metadata.service"
                "tripleo_nova_scheduler.service"
                "tripleo_nova_vnc_proxy.service"
                "tripleo_aodh_api.service"
                "tripleo_aodh_api_cron.service"
                "tripleo_aodh_evaluator.service"
                "tripleo_aodh_listener.service"
                "tripleo_aodh_notifier.service"
                "tripleo_ceilometer_agent_central.service"
                "tripleo_ceilometer_agent_compute.service"
                "tripleo_ceilometer_agent_ipmi.service"
                "tripleo_ceilometer_agent_notification.service"
                "tripleo_ovn_cluster_northd.service"
                "tripleo_ironic_neutron_agent.service"
                "tripleo_ironic_api.service"
                "tripleo_ironic_inspector.service"
                "tripleo_ironic_conductor.service"
                "tripleo_ironic_inspector_dnsmasq.service"
                "tripleo_ironic_pxe_http.service"
                "tripleo_ironic_pxe_tftp.service"
                "tripleo_unbound.service")

PacemakerResourcesToStop=("openstack-cinder-volume"
                          "openstack-cinder-backup"
                          "openstack-manila-share")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done

echo "Stopping pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Stopping $resource"
                ${!SSH_CMD} sudo pcs resource disable $resource
            else
                echo "Service $resource not present"
            fi
    done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ! ${!SSH_CMD} sudo pcs resource status $resource | grep Started; then
                    echo "OK: Service $resource is stopped"
                else
                    echo "ERROR: Service $resource is started"
                fi
            fi
        done
        break
    fi
    done</code></pre>
</div>
</div>
<div class="paragraph">
<p>If the status of each service is <code>OK</code>, then the services stopped successfully.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-databases-to-mariadb-instances_migrating-databases">3.5. Migrating databases to MariaDB instances</h3>
<div class="paragraph _abstract">
<p>Migrate your databases from the original Red&#160;Hat OpenStack Platform (RHOSP) deployment to the MariaDB instances in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that the control plane MariaDB and RabbitMQ are running, and that no other control plane services are running.</p>
</li>
<li>
<p>Retrieve the topology-specific service configuration. For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
<li>
<p>Stop the RHOSP services. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>Ensure that there is network routability between the original MariaDB and the MariaDB for the control plane.</p>
</li>
<li>
<p>Define the following shell variables. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ STORAGE_CLASS=local-storage
$ MARIADB_IMAGE=registry.redhat.io/rhoso/openstack-mariadb-rhel9:18.0


$ CELLS="default cell1 cell2"
$ DEFAULT_CELL_NAME="cell3"
$ RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"

$ CHARACTER_SET=utf8 #
$ COLLATION=utf8_general_ci

$ declare -A PODIFIED_DB_ROOT_PASSWORD
$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
&gt; done

$ declare -A PODIFIED_MARIADB_IP
$ for CELL in $(echo "super $RENAMED_CELLS"); do
&gt;   if [ "$CELL" = "super" ]; then
&gt;     PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack" -ojsonpath='{.items[0].spec.clusterIP}')
&gt;   else
&gt;     PODIFIED_MARIADB_IP[$CELL]=$(oc get svc --selector "mariadb/name=openstack-$CELL" -ojsonpath='{.items[0].spec.clusterIP}')
&gt;   fi
&gt; done

$ declare -A TRIPLEO_PASSWORDS
$ for CELL in $(echo $CELLS); do
&gt;   if [ "$CELL" = "default" ]; then
&gt;     TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
&gt;   else
&gt;     # in a split-stack source cloud, it should take a stack-specific passwords file instead
&gt;     TRIPLEO_PASSWORDS[$CELL]="$HOME/overcloud-passwords.yaml"
&gt;   fi
&gt; done

$ declare -A SOURCE_DB_ROOT_PASSWORD
$ for CELL in $(echo $CELLS); do
&gt;   SOURCE_DB_ROOT_PASSWORD[$CELL]=$(cat ${TRIPLEO_PASSWORDS[$CELL]} | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')
&gt; done

$ declare -A SOURCE_MARIADB_IP
$ SOURCE_MARIADB_IP[default]=*&lt;galera cluster VIP&gt;*
$ SOURCE_MARIADB_IP[cell1]=*&lt;galera cell1 cluster VIP&gt;*
$ SOURCE_MARIADB_IP[cell2]=*&lt;galera cell2 cluster VIP&gt;*
# ...

$ declare -A SOURCE_GALERA_MEMBERS_DEFAULT
$ SOURCE_GALERA_MEMBERS_DEFAULT=(
&gt;   ["standalone.localdomain"]=172.17.0.100
&gt;   # [...]=...
&gt; )
$ declare -A SOURCE_GALERA_MEMBERS_CELL1
$ SOURCE_GALERA_MEMBERS_CELL1=(
&gt;   # ...
&gt; )
$ declare -A SOURCE_GALERA_MEMBERS_CELL2
$ SOURCE_GALERA_MEMBERS_CELL2=(
&gt;   # ...
&gt; )</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>CELLS</code> and <code>RENAMED_CELLS</code> represent changes that are going to be made after you import the databases. The <code>default</code> cell takes a new name from <code>DEFAULT_CELL_NAME</code>. In a multi-cell adoption scenario, <code>default</code> cell might retain its original <em>default</em> name as well.</p>
</li>
<li>
<p><code>CHARACTER_SET</code> and <code>COLLATION</code> should match the source database. If they do not match, then foreign key relationships break for any tables that are created in the future as part of the database sync.</p>
</li>
<li>
<p><code>SOURCE_MARIADB_IP[X]= ...</code> includes the data for each cell that is defined in <code>CELLS</code>. Provide records for the cell names and VIP addresses of MariaDB Galera clusters.</p>
</li>
<li>
<p><code>&lt;galera_cell1_cluster_VIP&gt;</code> defines the VIP of your galera cell1 cluster.</p>
</li>
<li>
<p><code>&lt;galera_cell2_cluster_VIP&gt;</code> defines the VIP of your galera cell2 cluster, and so on.</p>
</li>
<li>
<p><code>SOURCE_GALERA_MEMBERS_CELL&lt;X&gt;</code>, defines the names of the MariaDB Galera cluster members and their IP address for each cell defined in <code>CELLS</code>. Replace <code>["standalone.localdomain"]="172.17.0.100"</code> with the real hosts data.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
A standalone director environment only creates a <em>default</em> cell, which should be the only <code>CELLS</code> value in this case. The <code>DEFAULT_CELL_NAME</code> value should be <code>cell1</code>.
</td>
</tr>
</table>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The <code>super</code> is the top-scope Nova API upcall database instance. A super conductor connects to that database. In subsequent examples, the upcall and cells databases use the same password that is defined in <code>osp-secret</code>. Old passwords are only needed to prepare the data exports.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p>To get the values for <code>SOURCE_MARIADB_IP</code>, query the puppet-generated configurations in the Controller and CellController nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep bind</pre>
</div>
</div>
</li>
<li>
<p>To get the values for <code>SOURCE_GALERA_MEMBERS_*</code>, query the puppet-generated configurations in the Controller and CellController nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo grep -rI 'listen mysql' -A10 /var/lib/config-data/puppet-generated/ | grep server</pre>
</div>
</div>
<div class="paragraph">
<p>The source cloud always uses the same password for cells databases. For that reason, the same passwords file is used for all cells stacks. However, split-stack topology allows using different passwords files for each stack.</p>
</div>
</li>
<li>
<p>Prepare the MariaDB adoption helper pod:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a temporary volume claim and a pod for the database data copy. Edit the volume claim storage request if necessary, to give it enough space for the overcloud databases:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mariadb-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: mariadb-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $MARIADB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: mariadb-data
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: mariadb-data
    persistentVolumeClaim:
      claimName: mariadb-data
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready pod/mariadb-copy-data --timeout=30s</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check that the source Galera database clusters in each cell have its members online and synced:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $CELLS); do
  MEMBERS=SOURCE_GALERA_MEMBERS_$(echo ${CELL}|tr '[:lower:]' '[:upper:]')[@]
  for i in "${!MEMBERS}"; do
    echo "Checking for the database node $i WSREP status Synced"
    oc rsh mariadb-copy-data mysql \
      -h "$i" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
      -e "show global status like 'wsrep_local_state_comment'" | \
      grep -qE "\bSynced\b"
  done
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Each additional Compute service (nova) v2 cell runs a dedicated Galera database cluster, so the command checks each cell.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Get the count of source databases with the <code>NOK</code> (not-OK) status:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $CELLS); do
  oc rsh mariadb-copy-data mysql -h "${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" -e "SHOW databases;"
done</pre>
</div>
</div>
</li>
<li>
<p>Check that <code>mysqlcheck</code> had no errors:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $CELLS); do
  set +u
  . ~/.source_cloud_exported_variables_$CELL
  set -u
  test -z "${PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]}" || [ "${PULL_OPENSTACK_CONFIGURATION_MYSQLCHECK_NOK[$CELL]}" = " " ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
done</pre>
</div>
</div>
</li>
<li>
<p>Test the connection to the control plane upcall and cells databases:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo "super $RENAMED_CELLS"); do
  oc rsh mariadb-copy-data mysql -rsh "${PODIFIED_MARIADB_IP[$CELL]}" -uroot -p"${PODIFIED_DB_ROOT_PASSWORD[$CELL]}" -e 'SHOW databases;'
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must transition Compute services that you import later into a superconductor architecture by deleting the old service records in the cell databases, starting with <code>cell1</code>. New records are registered with different hostnames that are provided by the Compute service operator. All Compute services, except the Compute agent, have no internal state, and you can safely delete their service records. You also need to rename the former <code>default</code> cell to <code>DEFAULT_CELL_NAME</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a dump of the original databases:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $CELLS); do
  oc rsh mariadb-copy-data &lt;&lt; EOF
    mysql -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
    -N -e "show databases" | grep -E -v "schema|mysql|gnocchi|aodh" | \
    while read dbname; do
      echo "Dumping $CELL cell \${dbname}";
      mysqldump -h"${SOURCE_MARIADB_IP[$CELL]}" -uroot -p"${SOURCE_DB_ROOT_PASSWORD[$CELL]}" \
        --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \
        "\${dbname}" &gt; /backup/"${CELL}.\${dbname}".sql;
    done
EOF
done</pre>
</div>
</div>
</li>
<li>
<p>Restore the databases from <code>.sql</code> files into the control plane MariaDB:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $CELLS); do
  RCELL=$CELL
  [ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME
  oc rsh mariadb-copy-data &lt;&lt; EOF
    declare -A db_name_map
    db_name_map['nova']="nova_$RCELL"
    db_name_map['ovs_neutron']='neutron'
    db_name_map['ironic-inspector']='ironic_inspector'
    declare -A db_cell_map
    db_cell_map['nova']="nova_$DEFAULT_CELL_NAME"
    db_cell_map["nova_$RCELL"]="nova_$RCELL"
    declare -A db_server_map
    db_server_map['default']=${PODIFIED_MARIADB_IP['super']}
    db_server_map["nova"]=${PODIFIED_MARIADB_IP[$DEFAULT_CELL_NAME]}
    db_server_map["nova_$RCELL"]=${PODIFIED_MARIADB_IP[$RCELL]}
    declare -A db_server_password_map
    db_server_password_map['default']=${PODIFIED_DB_ROOT_PASSWORD['super']}
    db_server_password_map["nova"]=${PODIFIED_DB_ROOT_PASSWORD[$DEFAULT_CELL_NAME]}
    db_server_password_map["nova_$RCELL"]=${PODIFIED_DB_ROOT_PASSWORD[$RCELL]}
    cd /backup
    for db_file in \$(ls ${CELL}.*.sql); do
      db_name=\$(echo \${db_file} | awk -F'.' '{ print \$2; }')
      [[ "$CELL" != "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] &amp;&amp; continue
      if [[ "$CELL" == "default" &amp;&amp; -v "db_cell_map[\${db_name}]" ]] ; then
        target=$DEFAULT_CELL_NAME
      elif [[ "$CELL" == "default" &amp;&amp; ! -v "db_cell_map[\${db_name}]" ]] ; then
        target=super
      else
        target=$RCELL
      fi
      renamed_db_file="\${target}_new.\${db_name}.sql"
      mv -f \${db_file} \${renamed_db_file}
      if [[ -v "db_name_map[\${db_name}]" ]]; then
        echo "renaming $CELL cell \${db_name} to \$target \${db_name_map[\${db_name}]}"
        db_name=\${db_name_map[\${db_name}]}
      fi
      db_server=\${db_server_map["default"]}
      if [[ -v "db_server_map[\${db_name}]" ]]; then
        db_server=\${db_server_map[\${db_name}]}
      fi
      db_password=\${db_server_password_map['default']}
      if [[ -v "db_server_password_map[\${db_name}]" ]]; then
        db_password=\${db_server_password_map[\${db_name}]}
      fi
      echo "creating $CELL cell \${db_name} in \$target \${db_server}"
      mysql -h"\${db_server}" -uroot "-p\${db_password}" -e \
        "CREATE DATABASE IF NOT EXISTS \${db_name} DEFAULT \
        CHARACTER SET ${CHARACTER_SET} DEFAULT COLLATE ${COLLATION};"
      echo "importing $CELL cell \${db_name} into \$target \${db_server} from \${renamed_db_file}"
      mysql -h "\${db_server}" -uroot "-p\${db_password}" "\${db_name}" &lt; "\${renamed_db_file}"
    done
    if [ "$CELL" = "default" ] ; then
      mysql -h "\${db_server_map['default']}" -uroot -p"\${db_server_password_map['default']}" -e \
        "update nova_api.cell_mappings set name='$DEFAULT_CELL_NAME' where name='default';"
    fi
    mysql -h "\${db_server_map["nova_$RCELL"]}" -uroot -p"\${db_server_password_map["nova_$RCELL"]}" -e \
      "delete from nova_${RCELL}.services where host not like '%nova_${RCELL}-%' and services.binary != 'nova-compute';"
EOF
done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>db_name_map</code> defines which common databases to rename when importing them.</p>
</li>
<li>
<p><code>db_cell_map</code> defines which cells databases to import, and how to rename them, if needed.</p>
</li>
<li>
<p><code>db_cell_map["nova_$RCELL"]="nova_$RCELL"</code> omits importing special <code>cell0</code> databases of the cells, as its contents cannot be consolidated during adoption.</p>
</li>
<li>
<p><code>db_server_map</code> defines which databases to import into which servers, usually dedicated for cells.</p>
</li>
<li>
<p><code>db_server_password_map</code> defines the root passwords map for database servers. You can only use the same password for now.</p>
</li>
<li>
<p><code>renamed_db_file="\${target}_new.\${db_name}.sql"</code> assigns which databases to import into which hosts when extracting databases from the <code>default</code> cell.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Compare the following outputs with the topology-specific service configuration.
For more information, see <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check that the databases are imported correctly:</p>
<div class="listingblock">
<div class="content">
<pre>$ set +u
$ . ~/.source_cloud_exported_variables_default
$ set -u
$ dbs=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" -e 'SHOW databases;')
$ echo $dbs | grep -Eq '\bkeystone\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ echo $dbs | grep -Eq '\bneutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ echo "${PULL_OPENSTACK_CONFIGURATION_DATABASES[@]}" | grep -Eq '\bovs_neutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"
$ novadb_mapped_cells=$(oc exec openstack-galera-0 -c galera -- mysql -rs -uroot -p"${PODIFIED_DB_ROOT_PASSWORD['super']}" \
&gt;   nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')
&gt; uuidf='\S{8,}-\S{4,}-\S{4,}-\S{4,}-\S{12,}'
&gt; default=$(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS" | sed -rn "s/^($uuidf)\s+default\b.*$/\1/p")
&gt; difference=$(diff -ZNua \
&gt;   &lt;(printf "%s\n" "$PULL_OPENSTACK_CONFIGURATION_NOVADB_MAPPED_CELLS") \
&gt;   &lt;(printf "%s\n" "$novadb_mapped_cells")) || true
&gt; if [ "$DEFAULT_CELL_NAME" != "default" ]; then
&gt;   printf "%s\n" "$difference" | grep -qE "^\-$default\s+default\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   printf "%s\n" "$difference" | grep -qE "^\+$default\s+$DEFAULT_CELL_NAME\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   [ $(grep -E "^[-\+]$uuidf" &lt;&lt;&lt;"$difference" | wc -l) -eq 2 ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; else
&gt;   [ "x$difference" = "x" ] &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; fi
&gt; for CELL in $(echo $RENAMED_CELLS); do
&gt;   RCELL=$CELL
&gt;   [ "$CELL" = "$DEFAULT_CELL_NAME" ] &amp;&amp; RCELL=default
&gt;   set +u
&gt;   . ~/.source_cloud_exported_variables_$RCELL
&gt;   set -u
&gt;   c1dbs=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} -e 'SHOW databases;')
&gt;   echo $c1dbs | grep -Eq "\bnova_${CELL}\b" &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt;   novadb_svc_records=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} \
&gt;     nova_$CELL -e "select host from services where services.binary='nova-compute' and deleted=0 order by host asc;")
&gt;   diff -Z &lt;(echo "x$novadb_svc_records") &lt;(echo "x${PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[@]}") &amp;&amp; echo "OK" || echo "CHECK FAILED"
&gt; done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>echo "${PULL_OPENSTACK_CONFIGURATION_DATABASES[@]}" | grep -Eq '\bovs_neutron\b' &amp;&amp; echo "OK" || echo "CHECK FAILED"</code> ensures that the Networking service (neutron) database is renamed from <code>ovs_neutron</code>.</p>
</li>
<li>
<p><code>nova_api -e 'select uuid,name,transport_url,database_connection,disabled from cell_mappings;')</code> ensures that the <code>default</code> cell is renamed to <code>$DEFAULT_CELL_NAME</code>, and the cell UUIDs are retained.</p>
</li>
<li>
<p><code>for CELL in $(echo $RENAMED_CELLS); do</code> ensures that the registered Compute services names have not changed.</p>
</li>
<li>
<p><code>c1dbs=$(oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p${PODIFIED_DB_ROOT_PASSWORD[$CELL]} -e 'SHOW databases;')</code> ensures Compute service cells databases are extracted to separate database servers, and renamed from <code>nova</code> to <code>nova_cell&lt;X&gt;</code>.</p>
</li>
<li>
<p><code>diff -Z &lt;(echo "x$novadb_svc_records") &lt;(echo "x${PULL_OPENSTACK_CONFIGURATION_NOVA_COMPUTE_HOSTNAMES[@]}") &amp;&amp; echo "OK" || echo "CHECK FAILED"</code> ensures that the registered Compute service name has not changed.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Delete the <code>mariadb-data</code> pod and the <code>mariadb-copy-data</code> persistent volume claim that contains the database backup:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of them before deleting.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod mariadb-copy-data
$ oc delete pvc mariadb-data</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
During the pre-checks and post-checks, the <code>mariadb-client</code> pod might return a pod security warning related to the <code>restricted:latest</code> security context constraint. This warning is due to default security context constraints and does not prevent the admission controller from creating a pod. You see a warning for the short-lived pod, but it does not interfere with functionality.
For more information, see <a href="https://learn.redhat.com/t5/DO280-Red-Hat-OpenShift/About-pod-security-standards-and-warnings/m-p/32502">About pod security standards and warnings</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ovn-data_migrating-databases">3.6. Migrating OVN data</h3>
<div class="paragraph _abstract">
<p>Migrate the data in the OVN databases from the original Red&#160;Hat OpenStack Platform deployment to <code>ovsdb-server</code> instances that are running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The <code>OpenStackControlPlane</code> resource is created.</p>
</li>
<li>
<p><code>NetworkAttachmentDefinition</code> custom resources (CRs) for the original cluster are defined. Specifically, the <code>internalapi</code> network is defined.</p>
</li>
<li>
<p>The original Networking service (neutron) and OVN <code>northd</code> are not running.</p>
</li>
<li>
<p>There is network routability between the control plane services and the adopted cluster.</p>
</li>
<li>
<p>The cloud is migrated to the Modular Layer 2 plug-in with Open Virtual Networking (ML2/OVN) mechanism driver.</p>
</li>
<li>
<p>Define the following shell variables. Replace the example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>STORAGE_CLASS=local-storage
OVSDB_IMAGE=registry.redhat.io/rhoso/openstack-ovn-base-rhel9:18.0
SOURCE_OVSDB_IP=172.17.0.100 # For IPv4
SOURCE_OVSDB_IP=[fd00:bbbb::100] # For IPv6</pre>
</div>
</div>
<div class="paragraph">
<p>To get the value to set <code>SOURCE_OVSDB_IP</code>, query the puppet-generated configurations in a Controller node:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ grep -rI 'ovn_[ns]b_conn' /var/lib/config-data/puppet-generated/</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Prepare a temporary <code>PersistentVolume</code> claim and the helper pod for the OVN backup. Adjust the storage requests for a large database, if needed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: ovn-data-cert
spec:
  commonName: ovn-data-cert
  secretName: ovn-data-cert
  issuerRef:
    name: rootca-internal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ovn-data
spec:
  storageClassName: $STORAGE_CLASS
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: ovn-copy-data
  annotations:
    openshift.io/scc: anyuid
    k8s.v1.cni.cncf.io/networks: internalapi
  labels:
    app: adoption
spec:
  containers:
  - image: $OVSDB_IMAGE
    command: [ "sh", "-c", "sleep infinity"]
    name: adoption
    volumeMounts:
    - mountPath: /backup
      name: ovn-data
    - mountPath: /etc/pki/tls/misc
      name: ovn-data-cert
      readOnly: true
  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ALL
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - name: ovn-data
    persistentVolumeClaim:
      claimName: ovn-data
  - name: ovn-data-cert
    secret:
      secretName: ovn-data-cert
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the pod to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=condition=Ready pod/ovn-copy-data --timeout=30s</pre>
</div>
</div>
</li>
<li>
<p>If the podified internalapi cidr is different than the source internalapi cidr, add an iptables accept rule on the Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ $CONTROLLER1_SSH sudo iptables -I INPUT -s {PODIFIED_INTERNALAPI_NETWORK} -p tcp -m tcp --dport 6641 -m conntrack --ctstate NEW -j ACCEPT
$ $CONTROLLER1_SSH sudo iptables -I INPUT -s {PODIFIED_INTERNALAPI_NETWORK} -p tcp -m tcp --dport 6642 -m conntrack --ctstate NEW -j ACCEPT</pre>
</div>
</div>
</li>
<li>
<p>Back up your OVN databases:</p>
<div class="ulist">
<ul>
<li>
<p>If you did not enable TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup tcp:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, run the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6641 &gt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client backup --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$SOURCE_OVSDB_IP:6642 &gt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Start the control plane OVN database services prior to import, with <code>northd</code> disabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnDBCluster:
        ovndbcluster-nb:
          replicas: 3
          dbType: NB
          storageRequest: 10G
          networkAttachment: internalapi
        ovndbcluster-sb:
          replicas: 3
          dbType: SB
          storageRequest: 10G
          networkAttachment: internalapi
      ovnNorthd:
        replicas: 0
'</pre>
</div>
</div>
</li>
<li>
<p>Wait for the OVN database services to reach the <code>Running</code> phase:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-nb
$ oc wait --for=jsonpath='{.status.phase}'=Running pod --selector=service=ovsdbserver-sb</pre>
</div>
</div>
</li>
<li>
<p>Fetch the OVN database IP addresses on the <code>clusterIP</code> service network:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-nb-0" -ojsonpath='{.items[0].spec.clusterIP}')
PODIFIED_OVSDB_SB_IP=$(oc get svc --selector "statefulset.kubernetes.io/pod-name=ovsdbserver-sb-0" -ojsonpath='{.items[0].spec.clusterIP}')</pre>
</div>
</div>
</li>
<li>
<p>If you are  using IPv6, adjust the address to the format expected by <code>ovsdb-*</code> tools:</p>
<div class="listingblock">
<div class="content">
<pre>PODIFIED_OVSDB_NB_IP=[$PODIFIED_OVSDB_NB_IP]
PODIFIED_OVSDB_SB_IP=[$PODIFIED_OVSDB_SB_IP]</pre>
</div>
</div>
</li>
<li>
<p>Upgrade the database schema for the backup files:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema tcp:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &gt; /backup/ovs-nb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-nb.db /backup/ovs-nb.ovsschema"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client get-schema --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &gt; /backup/ovs-sb.ovsschema &amp;&amp; ovsdb-tool convert /backup/ovs-sb.db /backup/ovs-sb.ovsschema"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Restore the database backup to the new OVN database servers:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>If you did not enable TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
<li>
<p>If you enabled TLS everywhere, use the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_NB_IP:6641 &lt; /backup/ovs-nb.db"
$ oc exec ovn-copy-data -- bash -c "ovsdb-client restore --ca-cert=/etc/pki/tls/misc/ca.crt --private-key=/etc/pki/tls/misc/tls.key --certificate=/etc/pki/tls/misc/tls.crt ssl:$PODIFIED_OVSDB_SB_IP:6642 &lt; /backup/ovs-sb.db"</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Check that the data was successfully migrated by running the following commands against the new database servers, for example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it ovsdbserver-nb-0 -- ovn-nbctl show
$ oc exec -it ovsdbserver-sb-0 -- ovn-sbctl list Chassis</pre>
</div>
</div>
</li>
<li>
<p>Start the control plane <code>ovn-northd</code> service to keep both OVN databases in sync:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnNorthd:
        replicas: 1
'</pre>
</div>
</div>
</li>
<li>
<p>If you are running OVN gateway services on RHOCP nodes, enable the control plane <code>ovn-controller</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    enabled: true
    template:
      ovnController:
        nicMappings:
          physNet: NIC</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>physNet</code> defines the name of your physical network. <code>NIC</code> is the name of the physical interface that is connected to your physical network.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Running OVN gateways on RHOCP nodes might be prone to data plane downtime during Open vSwitch upgrades. Consider running OVN gateways on dedicated <code>Networker</code> data plane nodes for production deployments instead.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Delete the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> that is used to store OVN database backup files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete --ignore-not-found=true pod ovn-copy-data
$ oc delete --ignore-not-found=true pvc ovn-data</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Consider taking a snapshot of the <code>ovn-data</code> helper pod and the temporary <code>PersistentVolumeClaim</code> before deleting them. For more information, see <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.16/html/storage/index#lvms-about-volume-snapsot_logical-volume-manager-storage">About volume snapshots</a> in <em>OpenShift Container Platform storage overview</em>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Stop the adopted OVN database servers:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStop=("tripleo_ovn_cluster_north_db_server.service"
                "tripleo_ovn_cluster_south_db_server.service")

echo "Stopping systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            echo "Stopping the $service in controller $i"
            if ${!SSH_CMD} sudo systemctl is-active $service; then
                ${!SSH_CMD} sudo systemctl stop $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStop[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=inactive &gt;/dev/null; then
                echo "ERROR: Service $service still running on controller $i"
            else
                echo "OK: Service $service is not running on controller $i"
            fi
        fi
    done
done</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="adopting-openstack-control-plane-services_configuring-network">4. Adopting Red&#160;Hat OpenStack Platform control plane services</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Adopt your Red&#160;Hat OpenStack Platform 17.1 control plane services to deploy them in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 control plane.</p>
</div>
<div class="sect2">
<h3 id="adopting-the-identity-service_adopt-control-plane">4.1. Adopting the Identity service</h3>
<div class="paragraph _abstract">
<p>To adopt the Identity service (keystone), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Identity service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Create the keystone secret that includes the Fernet keys that were copied from the RHOSP environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
data:
  CredentialKeys0: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/credential-keys/0 | base64 -w 0)
  CredentialKeys1: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/credential-keys/1 | base64 -w 0)
  FernetKeys0: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/fernet-keys/0 | base64 -w 0)
  FernetKeys1: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/keystone/etc/keystone/fernet-keys/1 | base64 -w 0)
kind: Secret
metadata:
  name: keystone
type: Opaque
EOF</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  keystone:
    enabled: true
    apiOverride:
      route: {}
    template:
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
            spec:
              type: LoadBalancer
      databaseInstance: openstack
      secret: osp-secret
'</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Create an alias to use the <code>openstack</code> command in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
<li>
<p>Remove services and endpoints that still point to the RHOSP
control plane, excluding the Identity service and its endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep keystone | awk '/admin/{ print $2; }' | xargs ${BASH_ALIASES[openstack]} endpoint delete || true
&gt; for service in aodh heat heat-cfn barbican cinderv3 glance designate gnocchi manila manilav2 neutron nova placement swift ironic-inspector ironic octavia; do
&gt;  openstack service list | awk "/ $service /{ print \$2; }" | xargs -r ${BASH_ALIASES[openstack]} service delete || true
&gt; done</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that you can access the <code>OpenStackClient</code> pod. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/maintaining_the_red_hat_openstack_services_on_openshift_deployment/assembly_accessing-the-rhoso-cloud#proc_accessing-the-OpenStackClient-pod_cloud-access-admin">Accessing the OpenStackClient pod</a> in <em>Maintaining the Red&#160;Hat OpenStack Services on OpenShift deployment</em>.</p>
</li>
<li>
<p>Confirm that the Identity service endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep keystone</pre>
</div>
</div>
</li>
<li>
<p>Wait for the <code>OpenStackControlPlane</code> resource to become <code>Ready</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for=condition=Ready --timeout=1m OpenStackControlPlane openstack</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="configuring-ldap-with-domain-specific-drivers_adopt-control-plane">4.2. Configuring LDAP with domain-specific drivers</h3>
<div class="paragraph _abstract">
<p>If you need to integrate the Identity service (keystone) with one or more LDAP servers using domain-specific configurations, you can enable domain-specific drivers and provide the necessary LDAP settings.</p>
</div>
<div class="paragraph">
<p>This involves two main steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create the secret that holds the domain-specific LDAP configuration files that the Identity service uses. Each file within the secret corresponds to an LDAP domain.</p>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource (CR) to enable domain-specific drivers for the Identity service and mount a secret that contains the LDAP configurations.</p>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To create the <code>keystone-domains</code> secret that stores the actual LDAP configuration files that Identity service uses, create a local file that includes your LDAP configuration, for example, <code>keystone.myldapdomain.conf</code>:</p>
<div class="paragraph">
<p>The following example file includes the configuration for a single LDAP domain. If you have multiple LDAP domains, create a configuration file for each, for example, <code>keystone.DOMAIN_ONE.conf</code>, <code>keystone.DOMAIN_TWO.conf</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-ini hljs" data-lang="ini">[identity]
driver = ldap
[ldap]
url = ldap://&lt;ldap_server_host&gt;:&lt;ldap_server_port&gt;
user = &lt;bind_dn_user&gt;
password = &lt;bind_dn_password&gt;
suffix = &lt;user_tree_dn&gt;
query_scope = sub
# User configuration
user_tree_dn = &lt;user_tree_dn&gt;
user_objectclass = &lt;user_object_class&gt;
user_id_attribute = &lt;user_id_attribute&gt;
user_name_attribute = &lt;user_name_attribute&gt;
user_mail_attribute = &lt;user_mail_attribute&gt;
user_enabled_attribute = &lt;user_enabled_attribute&gt;
user_enabled_default = true
# Group configuration
group_tree_dn = &lt;group_tree_dn&gt;
group_objectclass = &lt;group_object_class&gt;
group_id_attribute = &lt;group_id_attribute&gt;
group_name_attribute = &lt;group_name_attribute&gt;
group_member_attribute = &lt;group_member_attribute&gt;
group_members_are_ids = true</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace the values, such as <code>&lt;ldap_server_host&gt;</code>, <code>&lt;bind_dn_user&gt;</code>, <code>&lt;user_tree_dn&gt;</code>, and so on, with your LDAP server details.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create the secret from this file:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic keystone-domains --from-file=&lt;keystone.DOMAIN_NAME.conf&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;keystone.DOMAIN_NAME.conf&gt;</code> with the name of your local configuration file. If applicable, include additional configuration files by using the <code>--from-file</code> option. After creating the secret, you can remove the local configuration file if it is no longer needed, or store it securely.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
The name of the file that you provide to <code>--from-file</code>, for example <code>keystone.DOMAIN_NAME.conf</code>, is critical. The Identity service uses this filename to map incoming authentication requests for a domain to the correct LDAP configuration. Ensure that <code>DOMAIN_NAME</code> matches the name of the domain you are configuring in the Identity service.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane &lt;cr_name&gt; --type=merge -p '
spec:
  keystone:
    template:
      customServiceConfig: |
          [identity]
          domain_specific_drivers_enabled = true
      extraMounts:
        - name: v1
          region: r1
          extraVol:
            - propagation:
              - Keystone
              extraVolType: Conf
              volumes:
              - name: keystone-domains
                secret:
                  secretName: keystone-domains
              mounts:
              - name: keystone-domains
                mountPath: "/etc/keystone/domains"
                readOnly: true</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;cr_name&gt;</code> with the name of your <code>OpenStackControlPlane</code> CR (for example, <code>openstack</code>).</p>
</li>
<li>
<p>This patch does the following:</p>
<div class="ulist">
<ul>
<li>
<p>Sets <code>spec.keystone.template.customServiceConfig</code>. Ensure that you do not overwrite any previously defined value.</p>
</li>
<li>
<p>Defines <code>spec.keystone.template.extraMounts</code> to mount a secret named <code>keystone-domains</code> into the Identity service pods at <code>/etc/keystone/domains</code>. This secret contains your LDAP configuration files.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You might need to wait a few minutes for the changes to propagate and for the Identity service pods to be updated.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that users from the LDAP domain are accessible:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t openstackclient -- openstack user list --domain &lt;domain_name&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;domain_name&gt;</code> with your LDAP domain name.</p>
<div class="paragraph">
<p>This command returns a list of users from your LDAP server.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Verify that groups from the LDAP domain are accessible:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t openstackclient -- openstack group list --domain &lt;domain_name&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>This command returns a list of groups from your LDAP server.</p>
</div>
</li>
<li>
<p>Test authentication with an LDAP user:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t openstackclient -- openstack --os-auth-url &lt;keystone_auth_url&gt; --os-identity-api-version 3 --os-user-domain-name &lt;domain_name&gt; --os-username &lt;ldap_username&gt; --os-password &lt;ldap_password&gt; token issue</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;keystone_auth_url&gt;</code> with the Identity service authentication URL.</p>
</li>
<li>
<p>Replace <code>&lt;ldap_username&gt;</code> and <code>&lt;ldap_password&gt;</code> with valid LDAP user credentials.</p>
<div class="paragraph">
<p>If successful, this command returns a token, confirming that LDAP authentication is working correctly.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Verify group membership for an LDAP user:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t openstackclient -- openstack group contains user --group-domain &lt;domain_name&gt; --user-domain &lt;domain_name&gt; &lt;group_name&gt; &lt;username&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;domain_name&gt;</code>, <code>&lt;group_name&gt;</code>, and <code>&lt;username&gt;</code> with the appropriate values from your LDAP server.</p>
<div class="paragraph">
<p>This command verifies that the user is properly associated with the group through LDAP.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-key-manager-service_adopt-control-plane">4.3. Adopting the Key Manager service</h3>
<div class="paragraph _abstract">
<p>To adopt the Key Manager service (barbican), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where Key Manager service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment. You configure the Key Manager service to use the <code>simple_crypto</code> back end.</p>
</div>
<div class="paragraph">
<p>The Key Manager service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>BarbicanAPI</code>, <code>BarbicanWorker</code>, and <code>BarbicanKeystoneListener</code> services are up and running.</p>
</li>
<li>
<p>Keystone endpoints are updated, and the same crypto plugin of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
To configure hardware security module (HSM) integration with Proteccio HSM, see <a href="#adopting-the-key-manager-service-with-proteccio-hsm_adopt-control-plane">Adopting the Key Manager service with Proteccio HSM integration</a>.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the kek secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc set data secret/osp-secret "BarbicanSimpleCryptoKEK=$($CONTROLLER1_SSH "python3 -c \"import configparser; c = configparser.ConfigParser(); c.read('/var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf'); print(c['simple_crypto_plugin']['kek'])\"")"</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Key Manager service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  barbican:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: barbican
      messagingBus:
        cluster: rabbitmq
      secret: osp-secret
      simpleCryptoBackendSecret: osp-secret
      serviceAccount: barbican
      serviceUser: barbican
      passwordSelectors:
        service: BarbicanPassword
        simplecryptokek: BarbicanSimpleCryptoKEK
      barbicanAPI:
        replicas: 1
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
              spec:
                type: LoadBalancer
      barbicanWorker:
        replicas: 1
      barbicanKeystoneListener:
        replicas: 1
'</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Ensure that the Identity service (keystone) endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>Ensure that Barbican API service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep key-manager</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>List the secrets:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret list</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-key-manager-service-with-hsm_adopt-control-plane">4.4. Adopting the Key Manager service with HSM integration</h3>
<div class="paragraph _abstract">
<p>Adopt the Key Manager service (barbican) from director to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) when your source environment includes hardware security module (HSM) integration to preserve HSM functionality and maintain access to HSM-backed secrets. HSM provides enhanced security for cryptographic operations by storing encryption keys in dedicated hardware devices.</p>
</div>
<div class="paragraph">
<p>For additional information about the Key Manager service before you start the adoption, see the following resources:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Key Manager service service configuration documentation</p>
</li>
<li>
<p>Hardware security module vendor-specific documentation</p>
</li>
<li>
<p>OpenStack Barbican PKCS#11 plugin documentation</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="key-manager-service-hsm-adoption-approaches_hsm-integration">4.4.1. Key Manager service HSM adoption approaches</h4>
<div class="paragraph _abstract">
<p>The Key Manager service (barbican) adoption approach depends on your source director environment configuration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Use the standard adoption approach if your environment includes only the <code>simple_crypto</code> plugin for secret storage and has no HSM integration.</p>
</li>
<li>
<p>Use the HSM-enabled adoption approach if your source environment has HSM integration that uses Public Key Cryptography Standard (PKCS) #11, Key Management Interoperability Protocol (KMIP), or other HSM back ends alongside <code>simple_crypto</code>.</p>
<div class="dlist">
<dl>
<dt class="hdlist1">Standard adoption approach</dt>
</dl>
</div>
</li>
<li>
<p>Uses the existing Key Manager service adoption procedure</p>
</li>
<li>
<p>Migrates a simple crypto back-end configuration</p>
</li>
<li>
<p>Provides a single-step adoption process</p>
</li>
<li>
<p>Is suitable for development, testing, and standard production environments</p>
<div class="dlist">
<dl>
<dt class="hdlist1">HSM-enabled adoption approach</dt>
</dl>
</div>
</li>
<li>
<p>Uses the enhanced <code>barbican_adoption</code> role with HSM awareness</p>
</li>
<li>
<p>Configures HSM integration through a simple boolean flag (<code>barbican_hsm_enabled: true</code>)</p>
</li>
<li>
<p>Automatically creates required Kubernetes secrets (<code>hsm-login</code> and <code>proteccio-data</code>)</p>
</li>
<li>
<p>Preserves HSM metadata during database migration</p>
</li>
<li>
<p>Supports both simple crypto and HSM back ends in the target environment</p>
</li>
<li>
<p>Requires HSM-specific configuration variables and custom container images with HSM client libraries (built using the <code>rhoso_proteccio_hsm</code> Ansible role)</p>
</li>
<li>
<p>Uses HSM client certificates and configuration files accessible via URLs</p>
</li>
<li>
<p>Requires proper HSM partition and key configuration that matches your source environment</p>
</li>
<li>
<p>The HSM-enabled adoption approach currently supports:</p>
<div class="ulist">
<ul>
<li>
<p>Proteccio (Eviden Trustway): Fully supported with PKCS#11 integration</p>
</li>
<li>
<p>Luna (Thales): PKCS#11 support available</p>
</li>
<li>
<p>nCipher (Entrust): PKCS#11 support available</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>HSM adoption requires additional configuration steps, including:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Custom Barbican container images with HSM client libraries that are built using the <code>rhoso_proteccio_hsm</code> Ansible role</p>
</li>
<li>
<p>HSM client certificates and configuration files that are accessible by using URLs</p>
</li>
<li>
<p>Proper HSM partition and key configuration that matches your source environment</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>These approaches are mutually exclusive. Choose an approach based on your source environment configuration.</p>
</div>
</td>
</tr>
</table>
</div>
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 33.3333%;"/>
<col style="width: 33.3333%;"/>
<col style="width: 33.3334%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Source environment characteristic</th>
<th class="tableblock halign-left valign-top">Approach</th>
<th class="tableblock halign-left valign-top">Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Only <code>simple_crypto</code> back-end configured</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard adoption</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">No HSM complexity needed</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">HSM integration present (PKCS#11, KMIP, and so on)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HSM-enabled adoption</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Preserves HSM functionality and secrets</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Development or testing environment</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Standard adoption</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Simpler setup and maintenance</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Production with compliance requirements</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">HSM-enabled adoption</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Maintains security compliance</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Unknown back-end configuration</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Check source environment first</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Determine appropriate approach</p></td>
</tr>
</tbody>
</table>
</div>
<div class="sect3">
<h4 id="adopting-the-key-manager-service-with-proteccio-hsm_hsm-integration">4.4.2. Adopting the Key Manager service with Proteccio HSM integration</h4>
<div class="paragraph _abstract">
<p>To adopt the Key Manager service (barbican) with Proteccio hardware security module (HSM) integration, you use the enhanced Barbican adoption role with HSM support enabled through a configuration flag. This approach preserves HSM integration while adopting all existing secrets from your source Red&#160;Hat OpenStack Platform (RHOSP) environment.
When you run the data plane adoption tests with HSM support enabled, the adoption process performs the following actions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Extracts the simple crypto KEK from the source configuration.</p>
</li>
<li>
<p>Creates the required HSM secrets (hsm-login and proteccio-data) in the target namespace.</p>
</li>
<li>
<p>Deploys Barbican with HSM-enabled configuration by using the PKCS#11 plugin.</p>
</li>
<li>
<p>Verifies the HSM functionality and secret migration.
When you run the data plane adoption tests with HSM support enabled, the adoption process performs the following actions:</p>
</li>
<li>
<p>Extracts the simple crypto KEK from the source configuration.</p>
</li>
<li>
<p>Creates the required HSM secrets (hsm-login and proteccio-data) in the target namespace.</p>
</li>
<li>
<p>Deploys Barbican with HSM-enabled configuration by using the PKCS#11 plugin.</p>
</li>
<li>
<p>Verifies the HSM functionality and secret migration.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Key Manager service Proteccio HSM adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>BarbicanAPI</code> and <code>BarbicanWorker</code> services are up and running with HSM-enabled configuration.</p>
</li>
<li>
<p>All secrets from the source RHOSP 17.1 environment are available in Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0.</p>
</li>
<li>
<p>The PKCS11 crypto plugin is available alongside <code>simple_crypto</code> for new secret storage.</p>
</li>
<li>
<p>HSM functionality is verified and operational.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If your environment does not include Proteccio HSM, to adopt the Key Manager service by using <code>simple_crypto</code>, see <a href="#adopting-the-key-manager-service_hsm-integration">Adopting the Key Manager service</a>.</p>
</div>
<div class="paragraph">
<p>The enhanced Key Manager service adoption role supports HSM configuration through a simple boolean flag. This approach integrates seamlessly with the standard data plane adoption framework while providing HSM support.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have a running director environment with Proteccio HSM integration (the source cloud).</p>
</li>
<li>
<p>You have a Single Node OpenShift or OpenShift Local running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>You have SSH access to the source director undercloud and Controller nodes.</p>
</li>
<li>
<p>You have configured HSM variables in your adoption configuration files.</p>
</li>
<li>
<p>Custom Key Manager service container images with the Proteccio client libraries are available in your registry.</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The HSM adoption process requires proper configuration of HSM-related variables. The adoption role automatically creates the required Kubernetes secrets (<code>hsm-login</code> and <code>proteccio-data</code>) when <code>barbican_hsm_enabled</code> is set to <code>true</code>. Ensure that your environment includes the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>All HSM-related variables are properly set in your configuration files</p>
</li>
<li>
<p>The Proteccio client ISO, certificates, and configuration files are accessible from the configured URLs</p>
</li>
<li>
<p>Custom Key Manager service images with Proteccio client are built and available in your container registry</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Without proper HSM configuration, your HSM-protected secrets become inaccessible after adoption.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Configure HSM integration variables in your adoption configuration (Zuul job vars or CI framework configuration):</p>
<div class="listingblock">
<div class="content">
<pre># Enable HSM integration for the Barbican adoption role
barbican_hsm_enabled: true

# HSM login credentials
proteccio_login_password: "your_hsm_password"

# Kubernetes secret names (defaults shown)
proteccio_login_secret_name: "hsm-login"
proteccio_client_data_secret_name: "proteccio-data"

# HSM partition and key configuration
cifmw_hsm_proteccio_partition: "VHSM1"
cifmw_hsm_mkek_label: "adoption_mkek_1"
cifmw_hsm_hmac_label: "adoption_hmac_1"
cifmw_hsm_proteccio_library_path: "/usr/lib64/libnethsm.so"
cifmw_hsm_key_wrap_mechanism: "CKM_AES_CBC_PAD"

# HSM client sources (URLs to download Proteccio client files)
cifmw_hsm_proteccio_client_src: "&lt;URL_of_Proteccio_ISO_file&gt;"
cifmw_hsm_proteccio_conf_src: "&lt;URL_of_proteccio.rc_config_file&gt;"
cifmw_hsm_proteccio_client_crt_src: "&lt;URL_of_client_certificate_file&gt;"
cifmw_hsm_proteccio_client_key_src: "&lt;URL_of_client_certificate_key&gt;"
cifmw_hsm_proteccio_server_crt_src:
  - "&lt;URL_of_HSM_certificate_file&gt;"</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;URL_of_Proteccio_ISO_file&gt;</dt>
<dd>
<p>Specifies the full URL (including "http://" or "https://") of the Proteccio client ISO image file.</p>
</dd>
<dt class="hdlist1">&lt;URL_of_proteccio.rc_config_file&gt;</dt>
<dd>
<p>Specifies the full URL (including "http://" or "https://") of the <code>proteccio.rc</code> configuration in your RHOSO environment.</p>
</dd>
<dt class="hdlist1">&lt;URL_of_client_certificate_file&gt;</dt>
<dd>
<p>Specifies the full URL (including "http://" or "https://") of the HSM client certificate file.</p>
</dd>
<dt class="hdlist1">&lt;URL_of_client_certificate_key&gt;</dt>
<dd>
<p>Specifies the full URL (including "http://" or "https://") of the client key file.</p>
</dd>
<dt class="hdlist1">&lt;URL_of_HSM_certificate_file&gt;</dt>
<dd>
<p>Specifies the full URL (including "http://" or "https://") of the HSM certificate file.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Run the data plane adoption tests with HSM support enabled:</p>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Ensure that the Identity service (keystone) endpoints are defined and are pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the Barbican API service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep key-manager</pre>
</div>
</div>
</li>
<li>
<p>Verify that all secrets from the source environment are available:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret list</pre>
</div>
</div>
</li>
<li>
<p>Confirm that Barbican services are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -n openstack -l service=barbican -o wide</pre>
</div>
</div>
</li>
<li>
<p>Test secret creation to verify HSM functionality:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret store --name adoption-verification --payload 'HSM adoption successful'</pre>
</div>
</div>
</li>
<li>
<p>Verify that the HSM back end is operational:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret get &lt;secret_id&gt; --payload</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;secret_id&gt;</dt>
<dd>
<p>Specifies the ID of the HSM secret.</p>
</dd>
</dl>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-key-manager-service-with-hsm-integration_hsm-integration">4.4.3. Adopting the Key Manager service with HSM integration</h4>
<div class="paragraph _abstract">
<p>When your source director environment includes hardware security module (HSM) integration, you must use the HSM-enabled adoption approach to preserve HSM functionality and maintain access to HSM-backed secrets.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The source director environment with HSM integration is configured.</p>
</li>
<li>
<p>HSM client software and certificates are available from accessible URLs.</p>
</li>
<li>
<p>The target Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment with HSM infrastructure is accessible.</p>
</li>
<li>
<p>HSM-enabled Key Manager service (barbican) container images are built and available in your registry.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you use the automated adoption process by setting <code>barbican_hsm_enabled: true</code>, the required HSM secrets (<code>hsm-login</code> and <code>proteccio-data</code>) are created automatically. You only need to manually create the secret when you perform the manual adoption steps.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Confirm that your source environment configuration includes HSM integration:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ssh tripleo-admin@controller-0.ctlplane \
  "sudo cat /var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf | grep -A5 '\[.*plugin\]'"</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you see <code>[p11_crypto_plugin]</code> or other HSM-specific sections, continue with the HSM adoption.</p>
</div>
</li>
<li>
<p>Extract the simple crypto key encryption keys (KEK) from your source environment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ SIMPLE_CRYPTO_KEK=$(ssh tripleo-admin@controller-0.ctlplane \
  "sudo python3 -c \"import configparser; c = configparser.ConfigParser(); c.read('/var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf'); print(c['simple_crypto_plugin']['kek'])\"")</code></pre>
</div>
</div>
</li>
<li>
<p>Add the KEK to the target environment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc set data secret/osp-secret "BarbicanSimpleCryptoKEK=${SIMPLE_CRYPTO_KEK}"</code></pre>
</div>
</div>
</li>
<li>
<p>If you are not using the automated adoption, create HSM-specific secrets in the target environment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create HSM login credentials secret
$ oc create secret generic hsm-login \
  --from-literal=PKCS11Pin=&lt;your_hsm_password&gt; \
  -n openstack

# Create HSM client configuration and certificates secret
$ oc create secret generic proteccio-data \
  --from-file=client.crt=&lt;path_to_client_cert&gt; \
  --from-file=client.key=&lt;path_to_client_key&gt; \
  --from-file=10_8_60_93.CRT=&lt;path_to_server_cert&gt; \
  --from-file=proteccio.rc=&lt;path_to_hsm_config&gt; \
  -n openstack</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;your_hsm_password&gt;</dt>
<dd>
<p>Specifies the HSM password for your RHOSO environment.</p>
</dd>
<dt class="hdlist1">&lt;path_to_client_cert&gt;</dt>
<dd>
<p>Specifies the path to the HSM client certificate.</p>
</dd>
<dt class="hdlist1">&lt;path_to_client_key&gt;</dt>
<dd>
<p>Specifies the path to the client key.</p>
</dd>
<dt class="hdlist1">&lt;path_to_server_cert&gt;</dt>
<dd>
<p>Specifies the path to the server certificate.</p>
</dd>
<dt class="hdlist1">&lt;path_to_hsm_config&gt;</dt>
<dd>
<p>Specifies the path to your HSM configuration in your RHOSO environment.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When you use the automated adoption by setting <code>barbican_hsm_enabled: true</code>, the <code>barbican_adoption</code> role creates these secrets automatically. The secret names default to <code>hsm-login</code> and <code>proteccio-data</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource to deploy Key Manager service with HSM support:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  barbican:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: barbican
      rabbitMqClusterName: rabbitmq
      secret: osp-secret
      simpleCryptoBackendSecret: osp-secret
      serviceAccount: barbican
      serviceUser: barbican
      passwordSelectors:
        database: BarbicanDatabasePassword
        service: BarbicanPassword
        simplecryptokek: BarbicanSimpleCryptoKEK
      customServiceConfig: |
        [p11_crypto_plugin]
        plugin_name = PKCS11
        library_path = /usr/lib64/libnethsm.so
        token_labels = VHSM1
        mkek_label = adoption_mkek_1
        hmac_label = adoption_hmac_1
        encryption_mechanism = CKM_AES_CBC
        hmac_key_type = CKK_GENERIC_SECRET
        hmac_keygen_mechanism = CKM_GENERIC_SECRET_KEY_GEN
        hmac_mechanism = CKM_SHA256_HMAC
        key_wrap_mechanism = CKM_AES_CBC_PAD
        key_wrap_generate_iv = true
        always_set_cka_sensitive = true
        os_locking_ok = false
      globalDefaultSecretStore: pkcs11
      enabledSecretStores: ["simple_crypto", "pkcs11"]
      pkcs11:
        loginSecret: hsm-login
        clientDataSecret: proteccio-data
        clientDataPath: /etc/proteccio
      barbicanAPI:
        replicas: 1
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
      barbicanWorker:
        replicas: 1
      barbicanKeystoneListener:
        replicas: 1
'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>library_path</code> specifies the path to the PKCS#11 library, for example, <code>/usr/lib64/libnethsm.so</code> for Proteccio).</p>
</li>
<li>
<p><code>token_labels</code> specifies the HSM partition name, for example, <code>VHSM1</code>.</p>
</li>
<li>
<p><code>mkek_label</code> and <code>hmac_label</code> specify key labels that are configured in the HSM.</p>
</li>
<li>
<p><code>loginSecret</code> specifies the name of the Kubernetes secret that contains the HSM PIN.</p>
</li>
<li>
<p><code>clientDataSecret</code> specifies the name of the Kubernetes secret that contains the HSM certificates and configuration.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that both secret stores are available:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ openstack secret store list</code></pre>
</div>
</div>
</li>
<li>
<p>Test the HSM back-end functionality:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ openstack secret store --name "hsm-test-$(date +%s)" \
  --payload "test-payload" \
  --algorithm aes --mode cbc --bit-length 256</code></pre>
</div>
</div>
</li>
<li>
<p>Verify that the migrated secrets are accessible:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ openstack secret list</code></pre>
</div>
</div>
</li>
<li>
<p>Check that the Key Manager service services are operational:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc get pods -l service=barbican
NAME                                          READY   STATUS    RESTARTS      AGE
barbican-api-5d65949b4-xhkd7                  2/2     Running   7 (10m ago)   29d
barbican-keystone-listener-687cbdc77d-4kjnk   2/2     Running   3 (11m ago)   29d
barbican-worker-5c4b947d5c-l9jdh              2/2     Running   3 (11m ago)   29d</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>HSM adoption preserves both simple crypto and HSM-backed secrets. The migration process maintains HSM metadata and secret references, ensuring continued access to existing secrets while enabling new secrets to use either back-end.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="troubleshooting-key-manager-hsm-adoption_hsm-integration">4.4.4. Troubleshooting Key Manager HSM adoption</h4>
<div class="paragraph _abstract">
<p>Review troubleshooting guidance for common issues that you might encounter while you perform the HSM-enabled Key Manager (Barbican) service adoption.</p>
</div>
<div class="paragraph">
<p>If issues persist after following the troubleshooting guide:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Collect adoption logs and configuration for analysis.</p>
</li>
<li>
<p>Check the HSM vendor documentation for vendor-specific troubleshooting.</p>
</li>
<li>
<p>Verify HSM server status and connectivity independently.</p>
</li>
<li>
<p>Review the adoption summary report for additional diagnostic information.</p>
</li>
</ul>
</div>
<div class="sect4">
<h5 id="resolving-configuration-validation-failures_troubleshooting-hsm">Resolving configuration validation failures</h5>
<div class="paragraph _abstract">
<p>If the adoption fails with validation errors about placeholder values, replace the placeholder values with your environment&#8217;s configuration values.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [Validate all required variables are set] ****
fatal: [localhost]: FAILED! =&gt; {
    "msg": "Required variable proteccio_certs_path contains placeholder value."
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Edit your hardware security module configuration in the Zuul job vars or CI framework configuration file.</p>
</li>
<li>
<p>Check the following key variables and replace all placeholder values with actual configuration values for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>cifmw_hsm_password: &lt;your_actual_hsm_password&gt;
cifmw_barbican_proteccio_partition: &lt;VHSM1&gt;
cifmw_barbican_proteccio_mkek_label: &lt;your_mkek_label&gt;
cifmw_barbican_proteccio_hmac_label: &lt;your_hmac_label&gt;
cifmw_hsm_proteccio_client_src: &lt;https://your-server/path/to/Proteccio.iso&gt;
cifmw_hsm_proteccio_conf_src: &lt;https://your-server/path/to/proteccio.rc&gt;</pre>
</div>
</div>
</li>
<li>
<p>Verify that no placeholder values remain in your configuration.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-missing-HSM-file-prerequisites_troubleshooting-hsm">Resolving missing HSM file prerequisites</h5>
<div class="paragraph _abstract">
<p>If the adoption fails because hardware security module (HSM) certificates or client software cannot be found, update your configuration to point to the files in their specific locations.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [Validate Proteccio prerequisites exist] ****
fatal: [localhost]: FAILED! =&gt; {
    "msg": "Proteccio client ISO not found: /opt/proteccio/Proteccio3.06.05.iso"
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that all required HSM files are accessible from the configured URLs. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ curl -I https://your-server/path/to/Proteccio3.06.05.iso
$ curl -I https://your-server/path/to/proteccio.rc
$ curl -I https://your-server/path/to/client.crt
$ curl -I https://your-server/path/to/client.key</code></pre>
</div>
</div>
</li>
<li>
<p>If the files are in different locations, update the URL variables in your configuration. For example:</p>
<div class="listingblock">
<div class="content">
<pre>cifmw_hsm_proteccio_client_src: "https://correct-server/path/to/Proteccio3.06.05.iso"
cifmw_hsm_proteccio_conf_src: "https://correct-server/path/to/proteccio.rc"
cifmw_hsm_proteccio_client_crt_src: "https://correct-server/path/to/client.crt"
cifmw_hsm_proteccio_client_key_src: "https://correct-server/path/to/client.key"</pre>
</div>
</div>
</li>
<li>
<p>Check the network connectivity and authentication to ensure that the URLs are accessible from the CI environment.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-source-environment-connectivity-issues_troubleshooting-hsm">Resolving source environment connectivity issues</h5>
<div class="paragraph _abstract">
<p>If the adoption cannot connect to the source Red&#160;Hat OpenStack Platform environment to extract the configuration, check your SSH connectivity to the source Controller node and update the configuration if needed.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [detect source environment HSM configuration] ****
fatal: [localhost]: FAILED! =&gt; {
    "msg": "SSH connection to source environment failed"
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify SSH connectivity to the source Controller node:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ssh -o StrictHostKeyChecking=no tripleo-admin@controller-0.ctlplane</code></pre>
</div>
</div>
</li>
<li>
<p>Update the <code>controller1_ssh</code> variable if needed:</p>
<div class="listingblock">
<div class="content">
<pre>$ controller1_ssh: "ssh -o StrictHostKeyChecking=no tripleo-admin@&lt;controller_ip&gt;"</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;controller_ip&gt;</code></dt>
<dd>
<p>Specifies the IP address of your Controller node.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Ensure that the SSH keys are properly configured for passwordless access.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-HSM-secret-creation-failures_troubleshooting-hsm">Resolving HSM secret creation failures</h5>
<div class="paragraph _abstract">
<p>If hardware security module (HSM) secrets cannot be created in the target environment, check whether you need to update the names of your secrets in your source configuration file.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [Create HSM secrets in target environment] ****
fatal: [localhost]: FAILED! =&gt; {
    "msg": "Failed to create secret proteccio-data"
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify target environment access:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ export KUBECONFIG=/path/to/.kube/config
$ oc get secrets -n openstack</code></pre>
</div>
</div>
</li>
<li>
<p>Check if secrets already exist:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc get secret proteccio-data hsm-login -n openstack</code></pre>
</div>
</div>
</li>
<li>
<p>If secrets exist with different names, update the configuration variables:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">proteccio_login_secret_name: "your-hsm-login-secret"
proteccio_client_data_secret_name: "your-proteccio-data-secret"</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-custom-image-registry-issues_troubleshooting-hsm">Resolving custom image registry issues</h5>
<div class="paragraph _abstract">
<p>If custom Barbican images cannot be pushed to or pulled from the configured registry, you can verify the authentication, test image push permissions, and then update the configuration as needed.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [Create Proteccio-enabled Barbican images] ****
fatal: [localhost]: FAILED! =&gt; {
    "msg": "Failed to push image to registry"
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify registry authentication:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ podman login &lt;registry_url&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;registry_url&gt;</code></dt>
<dd>
<p>Specifies the URL of your configured registry.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Test image push permissions:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ podman tag hello-world &lt;registry&gt;/&lt;namespace&gt;/test:latest
$ podman push &lt;registry&gt;/&lt;namespace&gt;/test:latest</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;registry&gt;</code></dt>
<dd>
<p>Specifies the name of your registry server.</p>
</dd>
<dt class="hdlist1"><code>&lt;namespace&gt;</code></dt>
<dd>
<p>Specifies the namespace of your container image.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Update registry configuration variables if needed:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">cifmw_update_containers_registry: "your-registry:5001"
cifmw_update_containers_org: "your-namespace"
cifmw_image_registry_verify_tls: false</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="sect5">
<h6 id="resolving-HSM-back-end-detection-failures_troubleshooting-hsm">Resolving HSM back-end detection failures</h6>
<div class="paragraph _abstract">
<p>If the adoption role cannot detect hardware security module (HSM) configuration in the source environment, you must force the HSM adoption.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [detect source environment HSM configuration] ****
ok: [localhost] =&gt; {
    "msg": "No HSM configuration found - using standard adoption"
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Manually verify that the HSM configuration exists in the source environment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ssh tripleo-admin@controller-0.ctlplane \
  "sudo grep -A 10 '\[p11_crypto_plugin\]' \
  /var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf"</code></pre>
</div>
</div>
</li>
<li>
<p>If HSM is configured but not detected, force HSM adoption by setting the <code>barbican_hsm_enabled</code> variable:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># In your Zuul job vars or CI framework configuration
barbican_hsm_enabled: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>This configuration ensures that the <code>barbican_adoption</code> role uses the HSM-enabled patch for Key Manager service (barbican) deployment.</p>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect4">
<h5 id="resolving-database-migration-issues_troubleshooting-hsm">Resolving database migration issues</h5>
<div class="paragraph _abstract">
<p>If hardware security module (HSM) metadata is not preserved during database migration, check the database logs for any errors and verify that the source database includes the HSM secrets.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>TASK [Verify database migration preserves HSM references] ****
ok: [localhost] =&gt; {
    "msg": "HSM secrets found in migrated database: 0"
}</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the source database contains the HSM secrets:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ ssh tripleo-admin@controller-0.ctlplane \
  "sudo mysql barbican -e 'SELECT COUNT(*) FROM secret_store_metadata WHERE key=\"plugin_name\" AND value=\"PKCS11\";'"</code></pre>
</div>
</div>
</li>
<li>
<p>Check the database migration logs for errors:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc logs deployment/barbican-api | grep -i migration</code></pre>
</div>
</div>
</li>
<li>
<p>If the migration failed, restore the database from backup and retry.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-service-startup-failures_troubleshooting-hsm">Resolving service startup failures</h5>
<div class="paragraph _abstract">
<p>If the Key Manager service (barbican) services fail to start after the hardware security module (HSM) configuration is applied, check the configuration in the pod.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=barbican
NAME                           READY   STATUS    RESTARTS   AGE
barbican-api-xyz               0/1     Error     0          2m</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check pod logs for HSM connectivity issues:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc logs barbican-api-xyz</code></pre>
</div>
</div>
</li>
<li>
<p>Verify HSM library is accessible:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc exec barbican-api-xyz -- ls -la /usr/lib64/libnethsm.so</code></pre>
</div>
</div>
</li>
<li>
<p>Check HSM configuration in the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc exec barbican-api-xyz -- cat /etc/proteccio/proteccio.rc</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-performance-and-connectivity-issues_troubleshooting-hsm">Resolving performance and connectivity issues</h5>
<div class="paragraph _abstract">
<p>If the hardware security module (HSM) operations are slow or fail intermittently, check the HSM connectivity and monitor the HSM server logs.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Test HSM connectivity from Key Manager service (barbican) pods:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc exec barbican-api-xyz -- pkcs11-tool --module /usr/lib64/libnethsm.so --list-slots</code></pre>
</div>
</div>
</li>
<li>
<p>Check HSM server connectivity:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc exec barbican-api-xyz -- nc -zv &lt;hsm_server_ip&gt; &lt;hsm_port&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;hsm_server_ip&gt;</code></dt>
<dd>
<p>Specifies the IP address of the HSM server.</p>
</dd>
<dt class="hdlist1"><code>&lt;hsm_port&gt;</code></dt>
<dd>
<p>Specifies the port of your HSM server.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Monitor HSM server logs for authentication or capacity issues.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="troubleshooting-key-manager-proteccio-adoption_hsm-integration">4.4.5. Troubleshooting Key Manager service Proteccio HSM adoption</h4>
<div class="paragraph _abstract">
<p>Use this reference to troubleshoot common issues that might occur during Key Manager service (barbican) adoption with Proteccio HSM integration. If Proteccio HSM issues persist, consult the Eviden Trustway documentation and ensure that HSM server configuration matches the client settings.</p>
</div>
<div class="sect4">
<h5 id="resolving-prerequisite-validation-failures_troubleshooting-proteccio">Resolving prerequisite validation failures</h5>
<div class="paragraph _abstract">
<p>If the adoption script fails during the prerequisites check, verify that your configuration includes all the required Proteccio files and that the HSM Ansible role is available.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ERROR: Required file proteccio_files/YOUR_CERT_FILE not found
ERROR: Cannot connect to OpenShift cluster
ERROR: Proteccio HSM Ansible role not found</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that all required Proteccio files are present:</p>
<div class="listingblock">
<div class="content">
<pre>$ ls -la /path/to/your/proteccio_files/</pre>
</div>
</div>
<div class="paragraph">
<p>Ensure that your configured certificate files, private key, HSM certificate file, and configuration file exist as specified in your <code>proteccio_required_files</code> configuration.</p>
</div>
</li>
<li>
<p>Test OpenShift cluster connectivity:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc cluster-info
$ oc get pods -n openstack</pre>
</div>
</div>
</li>
<li>
<p>Verify that the HSM Ansible role is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ ls -la /path/to/your/roles/ansible-role-rhoso-proteccio-hsm/</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-SSH-connection-failures_troubleshooting-proteccio">Resolving SSH connection failures to the source environment</h5>
<div class="paragraph _abstract">
<p>If you cannot connect to the source director environment, verify your SSH key access and test the SSH commands that the adoption uses.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Warning: Permanently added 'YOUR_UNDERCLOUD_HOST' (ED25519) to the list of known hosts.
Permission denied (publickey).</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify SSH key access to the undercloud:</p>
<div class="listingblock">
<div class="content">
<pre>$ ssh YOUR_UNDERCLOUD_HOST echo "Connection test"</pre>
</div>
</div>
</li>
<li>
<p>Test the specific SSH commands used by the adoption:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo ssh -t YOUR_UNDERCLOUD_HOST 'sudo -u stack bash -lc "echo test"'
$ sudo ssh -t YOUR_UNDERCLOUD_HOST 'sudo -u stack ssh -t tripleo-admin@YOUR_CONTROLLER_HOST.ctlplane "echo test"'</pre>
</div>
</div>
</li>
<li>
<p>If the connection fails, verify the SSH configuration and ensure that the undercloud hostname resolves correctly.</p>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-database-import-failures_troubleshooting-proteccio">Resolving database import failures</h5>
<div class="paragraph _abstract">
<p>If the source database export or import fails, check the the source Galera container, database connectivity, and the source Key Manager service (barbican) configuration.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Error: no container with name or ID "galera-bundle-podman-0" found
mysqldump: Got error: 1045: "Access denied for user 'barbican'@'localhost'"</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the source Galera container is running:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo ssh -t YOUR_UNDERCLOUD_HOST 'sudo -u stack ssh -t tripleo-admin@YOUR_CONTROLLER_HOST.ctlplane "sudo podman ps | grep galera"'</pre>
</div>
</div>
</li>
<li>
<p>Test database connectivity with the extracted credentials:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo ssh -t YOUR_UNDERCLOUD_HOST 'sudo -u stack ssh -t tripleo-admin@YOUR_CONTROLLER_HOST.ctlplane "sudo podman exec galera-bundle-podman-0 mysql -u barbican -p&lt;password&gt; -e \"SELECT 1;\""'</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;password&gt;</code></dt>
<dd>
<p>Specifies your database password.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Check the source Key Manager service configuration for the correct database password:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo ssh -t YOUR_UNDERCLOUD_HOST 'sudo -u stack ssh -t tripleo-admin@YOUR_CONTROLLER_HOST.ctlplane "sudo grep connection /var/lib/config-data/puppet-generated/barbican/etc/barbican/barbican.conf"'</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-custom-image-pull-failures_troubleshooting-proteccio">Resolving custom image pull failures</h5>
<div class="paragraph _abstract">
<p>If Proteccio custom images fail to pull or start, verify image registry access, image pull secrets, and registry authentication.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Failed to pull image "&lt;Your Custom Pod Image and Tag&gt;": rpc error
Pod has unbound immediate PersistentVolumeClaims</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify image registry access:</p>
<div class="listingblock">
<div class="content">
<pre>$ podman pull &lt;custom_pod_image_and_tag&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;custom_pod_image_and_tag&gt;</code></dt>
<dd>
<p>Specifies your custom pod image and the image tag.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Check image pull secrets and registry authentication:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get secrets -n openstack | grep pull
$ oc describe pod &lt;barbican_pod_name&gt; -n openstack</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;barbican_pod_name&gt;</code></dt>
<dd>
<p>Specifies your Barbican pod name.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Verify that the <code>OpenStackVersion</code> resource was applied correctly:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get openstackversion openstack -n openstack -o yaml</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-HSM-certificate-mounting-issues_troubleshooting-proteccio">Resolving HSM certificate mounting issues</h5>
<div class="paragraph _abstract">
<p>If Proteccio client certificates are not properly mounted in pods, check the secret creation and ensure that the Key Manager service (barbican) configuration includes the correct volume mounts.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc exec &lt;barbican-pod&gt; -c barbican-api -- ls -la /etc/proteccio/
ls: cannot access '/etc/proteccio/': No such file or directory</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the <code>proteccio-data</code> secret was created correctly:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc describe secret proteccio-data -n openstack</pre>
</div>
</div>
</li>
<li>
<p>Check that the secret contains the expected files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get secret proteccio-data -n openstack -o yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Key Manager service configuration includes the correct volume mounts:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get barbican barbican -n openstack -o yaml | grep -A10 pkcs11</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-service-startup-failures_troubleshooting-proteccio">Resolving service startup failures</h5>
<div class="paragraph _abstract">
<p>If the Key Manager service (barbican) services fail to start after the hardware security module (HSM) configuration is applied, check the configuration in the pod.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=barbican
NAME                           READY   STATUS    RESTARTS   AGE
barbican-api-xyz               0/1     Error     0          2m</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Check pod logs for HSM connectivity issues:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc logs barbican-api-xyz</code></pre>
</div>
</div>
</li>
<li>
<p>Verify HSM library is accessible:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc exec barbican-api-xyz -- ls -la /usr/lib64/libnethsm.so</code></pre>
</div>
</div>
</li>
<li>
<p>Check HSM configuration in the pod:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">$ oc exec barbican-api-xyz -- cat /etc/proteccio/proteccio.rc</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="resolving-adoption-verification-failures_troubleshooting-proteccio">Resolving adoption verification failures</h5>
<div class="paragraph _abstract">
<p>If the secrets from the source environment are not accessible after adoption, verify that the database import completed successfully, test API connectivity, and check for schema adoption issues.</p>
</div>
<div class="paragraph">
<p>Example error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack secret list
# Returns empty list or HTTP 500 errors</pre>
</div>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the database import completed successfully:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstack-galera-0 -n openstack -- mysql -u root -p&lt;password&gt; barbican -e "SELECT COUNT(*) FROM secrets;"</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;password&gt;</code></dt>
<dd>
<p>Specifies your database password.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Check for schema adoption issues:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc logs job.batch/barbican-db-sync -n openstack</pre>
</div>
</div>
</li>
<li>
<p>Test API connectivity:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -n openstack -- curl -s -k -H "X-Auth-Token: $(openstack token issue -f value -c id)" https://barbican-internal.openstack.svc:9311/v1/secrets</pre>
</div>
</div>
</li>
<li>
<p>Verify that projects and users were adopted correctly, as secrets are project-scoped.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="rolling-back-the-HSM-adoption_hsm-integration">4.4.6. Rolling back the HSM adoption</h4>
<div class="paragraph _abstract">
<p>If the hardware security module (HSM) adoption fails, you can restore your environment to its original state and attempt the adoption again.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Restore the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 database backup:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -i openstack-galera-0 -n openstack -- mysql -u root -p&lt;password&gt; barbican &lt; /path/to/your/backups/rhoso18_barbican_backup.sql</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;password&gt;</dt>
<dd>
<p>Specifies your database password.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Reset to standard images:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete openstackversion openstack -n openstack</pre>
</div>
</div>
</li>
<li>
<p>Restore the base control plane configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f /path/to/your/base_controlplane.yaml</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>To avoid additional issues when attempting your adoption again, consider the following suggestions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Check the adoption logs that are stored in your configured working directory with timestamped summary reports.</p>
</li>
<li>
<p>For HSM-specific issues, consult the Proteccio documentation and verify HSM connectivity from the target environment.</p>
</li>
<li>
<p>Run the adoption in dry-run mode (<code>./run_proteccio_adoption.sh</code> option 3) to validate the environment before making changes.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-networking-service_hsm-integration">4.5. Adopting the Networking service</h3>
<div class="paragraph _abstract">
<p>To adopt the Networking service (neutron), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Networking service disabled. The patch starts the service with the
configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Networking service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>NeutronAPI</code> service is running.</p>
</li>
<li>
<p>The Identity service (keystone) endpoints are updated, and the same back end of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Ensure that Single Node OpenShift or OpenShift Local is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>Adopt the Identity service. For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
<li>
<p>Migrate your OVN databases to <code>ovsdb-server</code> instances that run in the Red Hat OpenShift Container Platform (RHOCP) cluster. For more information, see <a href="#migrating-ovn-data_migrating-databases">Migrating OVN data</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Networking service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  neutron:
    enabled: true
    apiOverride:
      route: {}
    template:
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
            spec:
              type: LoadBalancer
      databaseInstance: openstack
      databaseAccount: neutron
      secret: osp-secret
      networkAttachments:
      - internalapi
'</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you used the <code>neutron-dhcp-agent</code> in your RHOSP 17.1 deployment and you still need to use it after adoption, you must enable the <code>dhcp_agent_notification</code> for the <code>neutron-api</code> service:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ oc patch openstackcontrolplane openstack --type=merge --patch '
 spec:
  neutron:
    template:
      customServiceConfig: |
        [DEFAULT]
        dhcp_agent_notification = True
'</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Inspect the resulting Networking service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=neutron</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the <code>Neutron API</code> service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep network</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-shell hljs" data-lang="shell">$ openstack endpoint list | grep network

| 6a805bd6c9f54658ad2f24e5a0ae0ab6 | regionOne | neutron      | network      | True    | public    | http://neutron-public-openstack.apps-crc.testing  |
| b943243e596847a9a317c8ce1800fa98 | regionOne | neutron      | network      | True    | internal  | http://neutron-internal.openstack.svc:9696        |</code></pre>
</div>
</div>
</li>
<li>
<p>Create sample resources so that you can test whether the user can create networks, subnets, ports, or routers:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack network create net
$ openstack subnet create --network net --subnet-range 10.0.0.0/24 subnet
$ openstack router create router</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="configuring-control-plane-networking-for-spine-leaf_hsm-integration">4.6. Configuring control plane networking for spine-leaf topologies</h3>
<div class="paragraph _abstract">
<p>If you are adopting a spine-leaf or Distributed Compute Node (DCN) deployment, update the control plane networking for communication across sites. Add subnets for remote sites to your existing <code>NetConfig</code> custom resource (CR) and update <code>NetworkAttachmentDefinition</code> CRs with routes to enable connectivity between the central control plane and remote sites.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have deployed the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane.</p>
</li>
<li>
<p>You have configured a <code>NetConfig</code> CR for the central site. For more information, see <a href="#configuring-isolated-networks_configuring-network">Configuring isolated networks</a>.</p>
</li>
<li>
<p>You have the network topology information for all remote sites, including:</p>
<div class="ulist">
<ul>
<li>
<p>IP address ranges for each service network at each site</p>
</li>
<li>
<p>VLAN IDs for each service network at each site</p>
</li>
<li>
<p>Gateway addresses for inter-site routing</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Update your existing <code>NetConfig</code> CR to add subnets for each remote site. Each service network must include a subnet for the central site and each remote site. Use unique VLAN IDs for each site. For example:</p>
<div class="ulist">
<ul>
<li>
<p>Central site: VLANs 20-23</p>
</li>
<li>
<p>Edge site 1: VLANs 30-33</p>
</li>
<li>
<p>Edge site 2: VLANs 40-43</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: &lt;subnet1&gt;
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
    - name: &lt;ctlplanesite1&gt;
      allocationRanges:
      - end: 192.168.133.120
        start: 192.168.133.100
      cidr: 192.168.133.0/24
      gateway: 192.168.133.1
    - name: &lt;ctlplanesite2&gt;
      allocationRanges:
      - end: 192.168.144.120
        start: 192.168.144.100
      cidr: 192.168.144.0/24
      gateway: 192.168.144.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
    - name: internalapisite1
      allocationRanges:
      - end: 172.17.10.250
        start: 172.17.10.100
      cidr: 172.17.10.0/24
      vlan: 30
    - name: internalapisite2
      allocationRanges:
      - end: 172.17.20.250
        start: 172.17.20.100
      cidr: 172.17.20.0/24
      vlan: 40
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
    - name: storagesite1
      allocationRanges:
      - end: 172.18.10.250
        start: 172.18.10.100
      cidr: 172.18.10.0/24
      vlan: 31
    - name: storagesite2
      allocationRanges:
      - end: 172.18.20.250
        start: 172.18.20.100
      cidr: 172.18.20.0/24
      vlan: 41
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
    - name: tenantsite1
      allocationRanges:
      - end: 172.19.10.250
        start: 172.19.10.100
      cidr: 172.19.10.0/24
      vlan: 32
    - name: tenantsite2
      allocationRanges:
      - end: 172.19.20.250
        start: 172.19.20.100
      cidr: 172.19.20.0/24
      vlan: 42</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;subnet1&gt;</code></dt>
<dd>
<p>Specifies a user-defined subnet name for the central site subnet.</p>
</dd>
<dt class="hdlist1"><code>&lt;ctlplanesite1&gt;</code></dt>
<dd>
<p>Specifies a user-defined subnet for the first DCN edge site.</p>
</dd>
<dt class="hdlist1"><code>&lt;ctlplanesite2&gt;</code></dt>
<dd>
<p>Specifies a user-defined subnet for the second DCN edge site.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You must have the <code>storagemgmt</code> network on OpenShift nodes when using DCN with Swift storage. It is not necessary when using Red Hat Ceph Storage.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Update the <code>NetworkAttachmentDefinition</code> CR for the <code>internalapi</code> network to include routes to remote site subnets. These <code>routes</code> fields enable control plane pods attached to the <code>internalapi</code> network, such as OVN Southbound database, to communicate with Compute nodes at remote sites through the central site gateway, and are required for DCN:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: internalapi
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "internalapi",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.30",
        "range_end": "172.17.0.70",
        "routes": [
          { "dst": "172.17.10.0/24", "gw": "172.17.0.1" },
          { "dst": "172.17.20.0/24", "gw": "172.17.0.1" }
        ]
      }
    }</code></pre>
</div>
</div>
</li>
<li>
<p>Update the <code>NetworkAttachmentDefinition</code> CR for the <code>ctlplane</code> network to include routes to remote site subnets:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: ctlplane
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "ctlplane",
      "type": "macvlan",
      "master": "ospbr",
      "ipam": {
        "type": "whereabouts",
        "range": "192.168.122.0/24",
        "range_start": "192.168.122.30",
        "range_end": "192.168.122.70",
        "routes": [
          { "dst": "192.168.133.0/24", "gw": "192.168.122.1" },
          { "dst": "192.168.144.0/24", "gw": "192.168.122.1" }
        ]
      }
    }</code></pre>
</div>
</div>
</li>
<li>
<p>Update the <code>NetworkAttachmentDefinition</code> CR for the <code>storage</code> network to include routes to remote site subnets:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: storage
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "storage",
      "type": "macvlan",
      "master": "storage",
      "ipam": {
        "type": "whereabouts",
        "range": "172.18.0.0/24",
        "range_start": "172.18.0.30",
        "range_end": "172.18.0.70",
        "routes": [
          { "dst": "172.18.10.0/24", "gw": "172.18.0.1" },
          { "dst": "172.18.20.0/24", "gw": "172.18.0.1" }
        ]
      }
    }</code></pre>
</div>
</div>
</li>
<li>
<p>Update the <code>NetworkAttachmentDefinition</code> CR for the <code>tenant</code> network to include routes to remote site subnets:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: tenant
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "tenant",
      "type": "macvlan",
      "master": "tenant",
      "ipam": {
        "type": "whereabouts",
        "range": "172.19.0.0/24",
        "range_start": "172.19.0.30",
        "range_end": "172.19.0.70",
        "routes": [
          { "dst": "172.19.10.0/24", "gw": "172.19.0.1" },
          { "dst": "172.19.20.0/24", "gw": "172.19.0.1" }
        ]
      }
    }</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Adjust the IP ranges, subnets, and gateway addresses in all NAD configurations to match your network topology. The <code>master</code> interface name must match the interface on the OpenShift nodes where the VLAN is configured.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you have already deployed OVN services, restart the OVN Southbound database pods to pick up the new routes:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete pod -l service=ovsdbserver-sb</pre>
</div>
</div>
<div class="paragraph">
<p>The pods are automatically recreated with the updated network configuration.</p>
</div>
</li>
<li>
<p>Configure the Networking service (neutron) to recognize all site physnets. In the <code>OpenStackControlPlane</code> CR, ensure the Networking service configuration includes all physnets:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  neutron:
    template:
      customServiceConfig: |
        [ml2_type_vlan]
        network_vlan_ranges = leaf0:1:1000,leaf1:1:1000,leaf2:1:1000
        [ovn]
        ovn_emit_need_to_frag = false</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">leaf0</dt>
<dd>
<p>Represents the physnet for the central site.</p>
</dd>
<dt class="hdlist1">leaf1</dt>
<dd>
<p>Represents the physnet for the first remote site.</p>
</dd>
<dt class="hdlist1">leaf2</dt>
<dd>
<p>Represents the physnet for the second remote site.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Adjust the physnet names to match your Red&#160;Hat OpenStack Platform deployment. Common conventions include <code>leaf0/leaf1/leaf2</code> or <code>datacentre/dcn1/dcn2</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the <code>NetConfig</code> CR is created with all subnets:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get netconfig netconfig -o yaml | grep -A2 "name: subnet1\|name: .*site"</pre>
</div>
</div>
</li>
<li>
<p>Verify that each <code>NetworkAttachmentDefinition</code> includes routes to remote site subnets:</p>
<div class="listingblock">
<div class="content">
<pre>for nad in ctlplane internalapi storage tenant; do
  echo "=== $nad ==="
  oc get net-attach-def $nad -o jsonpath='{.spec.config}' | jq '.ipam.routes
done</pre>
</div>
</div>
</li>
<li>
<p>After restarting OVN SB pods, verify they have routes to remote site subnets:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec $(oc get pod -l service=ovsdbserver-sb -o name | head -1) -- ip route show | grep 172.17</pre>
</div>
</div>
<div class="paragraph">
<p>Sample output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.17.10.0/24 via 172.17.0.1 dev internalapi
172.17.20.0/24 via 172.17.0.1 dev internalapi</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-object-storage-service_hsm-integration">4.7. Adopting the Object Storage service</h3>
<div class="paragraph _abstract">
<p>If you are using Object Storage as a service, adopt the Object Storage service (swift) to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment. If you are using the Object Storage API of the Ceph Object Gateway (RGW), skip the following procedure.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The Object Storage service storage back-end services are running in the Red&#160;Hat OpenStack Platform (RHOSP) deployment.</p>
</li>
<li>
<p>The storage network is properly configured on the Red Hat OpenShift Container Platform (RHOCP) cluster. For more information, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/deploying_red_hat_openstack_services_on_openshift/assembly_preparing-rhocp-for-rhoso#proc_configuring-the-data-plane-network_preparing">Preparing Red Hat OpenShift Container Platform for Red Hat OpenStack Services on OpenShift</a> in <em>Deploying Red Hat OpenStack Services on OpenShift</em>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the <code>swift-conf</code> secret that includes the Object Storage service hash path suffix and prefix:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: swift-conf
type: Opaque
data:
  swift.conf: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/swift/etc/swift/swift.conf | base64 -w0)
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>swift-ring-files</code> <code>ConfigMap</code> that includes the Object Storage service ring files:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: swift-ring-files
binaryData:
  swiftrings.tar.gz: $($CONTROLLER1_SSH "cd /var/lib/config-data/puppet-generated/swift/etc/swift &amp;&amp; tar cz *.builder *.ring.gz backups/ | base64 -w0")
  account.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/account.ring.gz")
  container.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/container.ring.gz")
  object.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/object.ring.gz")
EOF</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource to deploy the Object Storage service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  swift:
    enabled: true
    template:
      memcachedInstance: memcached
      swiftRing:
        ringReplicas: 3
      swiftStorage:
        replicas: 0
        networkAttachments:
        - storage
        <strong>storageClass: local-storage</strong>
        storageRequest: 10Gi
      swiftProxy:
        secret: osp-secret
        replicas: 2
        passwordSelectors:
          service: SwiftPassword
        serviceUser: swift
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
              spec:
                type: LoadBalancer
        <strong>networkAttachments:</strong>
        - storage
'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.swift.swiftStorage.storageClass</code> must match the RHOSO deployment storage class.</p>
</li>
<li>
<p><code>metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;</code> specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</li>
<li>
<p><code>spec.swift.swiftProxy.networkAttachments</code> must match the network attachment for the previous Object Storage service configuration from the RHOSP deployment.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Inspect the resulting Object Storage service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l component=swift-proxy</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Object Storage proxy service is registered in the Identity service (keystone):</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep swift
| b5b9b1d3c79241aa867fa2d05f2bbd52 | swift    | object-store |</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep swift
| 32ee4bd555414ab48f2dc90a19e1bcd5 | regionOne | swift        | object-store | True    | public    | https://swift-public-openstack.apps-crc.testing/v1/AUTH_%(tenant_id)s |
| db4b8547d3ae4e7999154b203c6a5bed | regionOne | swift        | object-store | True    | internal  | http://swift-internal.openstack.svc:8080/v1/AUTH_%(tenant_id)s        |</pre>
</div>
</div>
</li>
<li>
<p>Verify that you are able to upload and download objects:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack container create test
+---------------------------------------+-----------+------------------------------------+
| account                               | container | x-trans-id                         |
+---------------------------------------+-----------+------------------------------------+
| AUTH_4d9be0a9193e4577820d187acdd2714a | test      | txe5f9a10ce21e4cddad473-0065ce41b9 |
+---------------------------------------+-----------+------------------------------------+

$ openstack object create test --name obj &lt;(echo "Hello World!")
+--------+-----------+----------------------------------+
| object | container | etag                             |
+--------+-----------+----------------------------------+
| obj    | test      | d41d8cd98f00b204e9800998ecf8427e |
+--------+-----------+----------------------------------+

$ openstack object save test obj --file -
Hello World!</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Object Storage data is still stored on the existing RHOSP nodes. For more information about migrating the actual data from the RHOSP deployment to the RHOSO deployment, see <a href="#migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">Migrating the Object Storage service (swift) data from RHOSP to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) nodes</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-image-service_hsm-integration">4.8. Adopting the Image service</h3>
<div class="paragraph _abstract">
<p>To adopt the Image Service (glance) you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Image service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>The Image service adoption is complete if you see the following results:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>GlanceAPI</code> service up and running.</p>
</li>
<li>
<p>The Identity service endpoints are updated, and the same back end of the source cloud is available.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To complete the Image service adoption, ensure that your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You have a running director environment (the source cloud).</p>
</li>
<li>
<p>You have a Single Node OpenShift or OpenShift Local that is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>Optional: You can reach an internal/external <code>Ceph</code> cluster by both <code>crc</code> and director.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you have image quotas in RHOSP 17.1, these quotas are transferred to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 because the image quota system in 18.0 is disabled by default. For more information about enabling image quotas in 18.0, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_persistent_storage/assembly_glance-customizing-the-image-service_customizing-cinder#assembly_glance-configuring-quotas_configuring-glance">Configuring image quotas</a> in <em>Customizing persistent storage</em>. If you enable image quotas in RHOSO 18.0, the new quotas replace the legacy quotas from RHOSP 17.1.</p>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-object-storage-backend_image-service">4.8.1. Adopting the Image service that is deployed with a Object Storage service back end</h4>
<div class="paragraph _abstract">
<p>Adopt the Image Service (glance) that you deployed with an Object Storage service (swift) back end in the Red&#160;Hat OpenStack Platform (RHOSP) environment. The control plane <code>glanceAPI</code> instance is deployed with the following configuration. You use this configuration in the patch manifest that deploys the Image service with the object storage back end:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:swift
          [glance_store]
          default_backend = default_backend
          [default_backend]
          swift_store_create_container_on_put = True
          swift_store_auth_version = 3
          swift_store_auth_address = {{ .KeystoneInternalURL }}
          swift_store_endpoint_type = internalURL
          swift_store_user = service:glance
          swift_store_key = {{ .ServicePassword }}</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example, <code>glance_swift.patch</code>, and include the following content:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      databaseInstance: openstack
      storage:
        storageRequest: 10G
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:swift
        [glance_store]
        default_backend = default_backend
        [default_backend]
        swift_store_create_container_on_put = True
        swift_store_auth_version = 3
        swift_store_auth_address = {{ .KeystoneInternalURL }}
        swift_store_endpoint_type = internalURL
        swift_store_user = service:glance
        swift_store_key = {{ .ServicePassword }}
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
                spec:
                  type: LoadBalancer
          networkAttachments:
            - storage</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Object Storage service as a back end establishes a dependency with the Image service. Any deployed <code>GlanceAPI</code> instances do not work if the Image service is configured with the Object Storage service that is not available in the <code>OpenStackControlPlane</code> custom resource.
After the Object Storage service, and in particular <code>SwiftProxy</code>, is adopted, you can proceed with the <code>GlanceAPI</code> adoption. For more information, see <a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a>.
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
<li>
<p>Verify that <code>SwiftProxy</code> is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l component=swift-proxy | grep Running
swift-proxy-75cb47f65-92rxq   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>GlanceAPI</code> service that is deployed in the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_swift.patch</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-block-storage-backend_image-service">4.8.2. Adopting the Image service that is deployed with a Block Storage service back end</h4>
<div class="paragraph _abstract">
<p>Adopt the Image Service (glance) that you deployed with a Block Storage service (cinder) back end in the Red&#160;Hat OpenStack Platform (RHOSP) environment. The control plane <code>glanceAPI</code> instance is deployed with the following configuration. You use this configuration in the patch manifest that deploys the Image service with the block storage back end:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:cinder
          [glance_store]
          default_backend = default_backend
          [default_backend]
          description = Default cinder backend
          cinder_store_auth_address = {{ .KeystoneInternalURL }}
          cinder_store_user_name = {{ .ServiceUser }}
          cinder_store_password = {{ .ServicePassword }}
          cinder_store_project_name = service
          cinder_catalog_info = volumev3::internalURL
          cinder_use_multipath = true
          [oslo_concurrency]
          lock_path = /var/lib/glance/tmp</pre>
</div>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example <code>glance_cinder.patch</code>, and include the following content:</p>
<div class="listingblock">
<div class="content">
<pre>spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      secret: osp-secret
      databaseInstance: openstack
      storage:
        storageRequest: 10G
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:cinder
        [glance_store]
        default_backend = default_backend
        [default_backend]
        description = Default cinder backend
        cinder_store_auth_address = {{ .KeystoneInternalURL }}
        cinder_store_user_name = {{ .ServiceUser }}
        cinder_store_password = {{ .ServicePassword }}
        cinder_store_project_name = service
        cinder_catalog_info = volumev3::internalURL
        cinder_use_multipath = true
        [oslo_concurrency]
        lock_path = /var/lib/glance/tmp
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    <strong>metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;</strong>
                spec:
                  type: LoadBalancer
          networkAttachments:
            - storage</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Block Storage service as a back end establishes a dependency with the Image service. Any deployed <code>GlanceAPI</code> instances do not work if the Image service is configured with the Block Storage service that is not available in the <code>OpenStackControlPlane</code> custom resource.
After the Block Storage service, and in particular <code>CinderVolume</code>, is adopted, you can proceed with the <code>GlanceAPI</code> adoption. For more information, see <a href="#adopting-the-block-storage-service_adopt-control-plane">Adopting the Block Storage service</a>.
</td>
</tr>
</table>
</div>
</dd>
</dl>
</div>
</li>
<li>
<p>Verify that <code>CinderVolume</code> is available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l component=cinder-volume | grep Running
cinder-volume-75cb47f65-92rxq   3/3     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>GlanceAPI</code> service that is deployed in the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_cinder.patch</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-nfs-backend_image-service">4.8.3. Adopting the Image service that is deployed with an NFS back end</h4>
<div class="paragraph _abstract">
<p>Adopt the Image Service (glance) that you deployed with an NFS back end. To complete the following procedure, ensure that your environment meets the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Storage network is propagated to the Red&#160;Hat OpenStack Platform (RHOSP) control plane.</p>
</li>
<li>
<p>The Image service can reach the Storage network and connect to the nfs-server through the port <code>2049</code>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>In the source cloud, verify the NFS parameters that the overcloud uses to configure the Image service back end. Specifically, in yourdirector heat templates, find the following variables that override the default content that is provided by the <code>glance-nfs.yaml</code> file in the
<code>/usr/share/openstack-tripleo-heat-templates/environments/storage</code> directory:</p>
<div class="listingblock">
<div class="content">
<pre>GlanceBackend: file
GlanceNfsEnabled: true
GlanceNfsShare: 192.168.24.1:/var/nfs</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In this example, the <code>GlanceBackend</code> variable shows that the Image service has no notion of an NFS back end. The variable is using the <code>File</code> driver and, in the background, the <code>filesystem_store_datadir</code>. The <code>filesystem_store_datadir</code> is mapped to the export value provided by the <code>GlanceNfsShare</code> variable instead of <code>/var/lib/glance/images/</code>.
If you do not export the <code>GlanceNfsShare</code> through a network that is propagated to the adopted Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane, you must stop the <code>nfs-server</code> and remap the export to the <code>storage</code> network. Before doing so, ensure that the Image service is stopped in the source Controller nodes.</p>
</div>
<div class="paragraph">
<p>In the control plane, the Image service is attached to the Storage network, then propagated through the associated <code>NetworkAttachmentsDefinition</code> custom resource (CR), and the resulting pods already have the right permissions to handle the Image service traffic through this network.
In a deployed RHOSP control plane, you can verify that the network mapping matches with what has been deployed in the director-based environment by checking both the <code>NodeNetworkConfigPolicy</code> (<code>nncp</code>) and the <code>NetworkAttachmentDefinition</code> (<code>net-attach-def</code>). The following is an example of the output that you should check in the Red Hat OpenShift Container Platform (RHOCP) environment to make sure that there are no issues with the propagated networks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc get nncp
NAME                        STATUS      REASON
enp6s0-crc-8cf2w-master-0   Available   SuccessfullyConfigured

$ oc get net-attach-def
NAME
ctlplane
internalapi
storage
tenant

$ oc get ipaddresspool -n metallb-system
NAME          AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
ctlplane      true          false             ["192.168.122.80-192.168.122.90"]
internalapi   true          false             ["172.17.0.80-172.17.0.90"]
storage       true          false             ["172.18.0.80-172.18.0.90"]
tenant        true          false             ["172.19.0.80-172.19.0.90"]</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Adopt the Image service and create a new <code>default</code> <code>GlanceAPI</code> instance that is connected with the existing NFS share:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; glance_nfs_patch.yaml

spec:
  extraMounts:
  - extraVol:
    - extraVolType: Nfs
      mounts:
      - mountPath: /var/lib/glance/images
        name: nfs
      propagation:
      - Glance
      volumes:
      - name: nfs
        nfs:
          <strong>path: &lt;exported_path&gt;</strong>
          <strong>server: &lt;ip_address&gt;</strong>
    name: r1
    region: r1
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = default_backend:file
        [glance_store]
        default_backend = default_backend
        [default_backend]
        filesystem_store_datadir = /var/lib/glance/images/
      storage:
        storageRequest: 10G
      keystoneEndpoint: nfs
      glanceAPIs:
        nfs:
          replicas: 3
          type: single
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
                spec:
                  type: LoadBalancer
          networkAttachments:
          - storage
EOF</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;exported_path&gt;</dt>
<dd>
<p>Specifies the exported path in the <code>nfs-server</code>.</p>
</dd>
<dt class="hdlist1">&lt;ip_address&gt;</dt>
<dd>
<p>Specifies the IP address that you use to communicate with the <code>nfs-server</code>.</p>
</dd>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Image service with an NFS back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file glance_nfs_patch.yaml</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to remove the default Image service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=json -p="[{'op': 'remove', 'path': '/spec/glance/template/glanceAPIs/default'}]"</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>When <code>GlanceAPI</code> is active, confirm that you can see a single API instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=glance
NAME                      READY   STATUS    RESTARTS
glance-nfs-single-0   2/2     Running   0
glance-nfs-single-1   2/2     Running   0
glance-nfs-single-2   2/2     Running   0</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the description of the pod reports the following output:</p>
<div class="listingblock">
<div class="content">
<pre>Mounts:
...
  nfs:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    {{ server ip address }}
    Path:      {{ nfs export path }}
    ReadOnly:  false
...</pre>
</div>
</div>
</li>
<li>
<p>Check that the mountpoint that points to <code>/var/lib/glance/images</code> is mapped to the expected <code>nfs server ip</code> and <code>nfs path</code> that you defined in the new default <code>GlanceAPI</code> instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh -c glance-api glance-default-single-0

sh-5.1# mount
...
...
{{ ip address }}:/var/nfs on /var/lib/glance/images type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.5,local_lock=none,addr=172.18.0.5)
...
...</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the UUID is created in the exported directory on the NFS node. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient
$ openstack image list

sh-5.1$  curl -L -o /tmp/cirros-0.6.3-x86_64-disk.img http://download.cirros-cloud.net/0.6.3/cirros-0.6.3-x86_64-disk.img
...
...

sh-5.1$ openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.6.3-x86_64-disk.img cirros
...
...

sh-5.1$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 634482ca-4002-4a6d-b1d5-64502ad02630 | cirros | active |
+--------------------------------------+--------+--------+</pre>
</div>
</div>
</li>
<li>
<p>On the <code>nfs-server</code> node, the same <code>uuid</code> is in the exported <code>/var/nfs</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ ls /var/nfs/
634482ca-4002-4a6d-b1d5-64502ad02630</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="adopting-image-service-with-ceph-backend_image-service">4.8.4. Adopting the Image service that is deployed with a Red Hat Ceph Storage back end</h4>
<div class="paragraph _abstract">
<p>Adopt the Image Service (glance) that you deployed with a Red Hat Ceph Storage back end. Use the <code>customServiceConfig</code> parameter to inject the right configuration to the <code>GlanceAPI</code> instance.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>Ensure that the Ceph-related secret (<code>ceph-conf-files</code>) is created in
the <code>openstack</code> namespace and that the <code>extraMounts</code> property of the
<code>OpenStackControlPlane</code> custom resource (CR) is configured properly. For more information, see <a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph back end</a>.</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; glance_patch.yaml
spec:
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends=default_backend:rbd
        [glance_store]
        default_backend=default_backend
        [default_backend]
        rbd_store_ceph_conf=/etc/ceph/ceph.conf
        rbd_store_user=openstack
        rbd_store_pool=images
        store_description=Ceph glance store backend.
      storage:
        storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 3
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    <strong>metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;</strong>
                spec:
                  type: LoadBalancer
          networkAttachments:
          - storage
EOF</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you backed up your Red&#160;Hat OpenStack Platform (RHOSP) services configuration file from the original environment, you can compare it with the confgiuration file that you adopted and ensure that the configuration is correct.
For more information, see <a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>os-diff diff /tmp/collect_tripleo_configs/glance/etc/glance/glance-api.conf glance_patch.yaml --crd</pre>
</div>
</div>
<div class="paragraph">
<p>This command produces the difference between both ini configuration files.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Image service with a Red Hat Ceph Storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file glance_patch.yaml</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="verifying-the-image-service-adoption_image-service">4.8.5. Verifying the Image service adoption</h4>
<div class="paragraph _abstract">
<p>Verify that you adopted the Image Service (glance) to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 deployment.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Test the Image service from the Red&#160;Hat OpenStack Platform CLI. You can compare and ensure that the configuration is applied to the Image service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff diff /etc/glance/glance.conf.d/02-config.conf glance_patch.yaml --frompod -p glance-api</pre>
</div>
</div>
<div class="paragraph">
<p>If no line appears, then the configuration is correct.</p>
</div>
</li>
<li>
<p>Inspect the resulting Image service pods:</p>
<div class="listingblock">
<div class="content">
<pre>GLANCE_POD=`oc get pod |grep glance-default | cut -f 1 -d' ' | head -n 1`
oc exec -t $GLANCE_POD -c glance-api -- cat /etc/glance/glance.conf.d/02-config.conf

[DEFAULT]
enabled_backends=default_backend:rbd
[glance_store]
default_backend=default_backend
[default_backend]
rbd_store_ceph_conf=/etc/ceph/ceph.conf
rbd_store_user=openstack
rbd_store_pool=images
store_description=Ceph glance store backend.</pre>
</div>
</div>
</li>
<li>
<p>If you use a Red Hat Ceph Storage back end, ensure that the Red Hat Ceph Storage secrets are mounted:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t $GLANCE_POD -c glance-api -- ls /etc/ceph
ceph.client.openstack.keyring
ceph.conf</pre>
</div>
</div>
</li>
<li>
<p>Check that the service is active, and that the endpoints are updated in the RHOSP CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient
$ openstack service list | grep image

| fc52dbffef36434d906eeb99adfc6186 | glance    | image        |

$ openstack endpoint list | grep image

| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |
| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |</pre>
</div>
</div>
</li>
<li>
<p>Check that the images that you previously listed in the source cloud are available in the adopted service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |
+--------------------------------------+--------+--------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-placement-service_hsm-integration">4.9. Adopting the Placement service</h3>
<div class="paragraph _abstract">
<p>To adopt the Placement service, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Placement service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You import your databases to MariaDB instances on the control plane. For more information, see <a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a>.</p>
</li>
<li>
<p>You adopt the Identity service (keystone). For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Placement service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  placement:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: placement
      secret: osp-secret
      override:
        service:
          internal:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/allow-shared-ip: internalapi
                metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
            spec:
              type: LoadBalancer
'</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Check that the Placement service endpoints are defined and pointing to the
control plane FQDNs, and that the Placement API responds:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"

$ openstack endpoint list | grep placement


# Without OpenStack CLI placement plugin installed:
$ PLACEMENT_PUBLIC_URL=$(openstack endpoint list -c 'Service Name' -c 'Service Type' -c URL | grep placement | grep public | awk '{ print $6; }')
$ oc exec -t openstackclient -- curl "$PLACEMENT_PUBLIC_URL"

# With OpenStack CLI placement plugin installed:
$ openstack resource class list</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-bare-metal-provisioning-service_hsm-integration">4.10. Adopting the Bare Metal Provisioning service</h3>
<div class="paragraph _abstract">
<p>Review information about your Bare Metal Provisioning service (ironic) configuration and then adopt the Bare Metal Provisioning service to the Red&#160;Hat OpenStack Services on OpenShift control plane.</p>
</div>
<div class="sect3">
<h4 id="con_bare-metal-provisioning-service-configurations_adopting-bare-metal-provisioning">4.10.1. Bare Metal Provisioning service configurations</h4>
<div class="paragraph _abstract">
<p>You configure the Bare Metal Provisioning service (ironic) by using configuration snippets. For more information about configuring the control plane with the Bare Metal Provisioning service, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/customizing_the_red_hat_openstack_services_on_openshift_deployment/index">Customizing the Red Hat OpenStack Services on OpenShift deployment</a>.</p>
</div>
<div class="paragraph">
<p>Some Bare Metal Provisioning service configuration is overridden in director, for example, PXE Loader file names are often overridden at intermediate layers. You must pay attention to the settings you apply in your Red&#160;Hat OpenStack Services on OpenShift (RHOSO) deployment. The <code>ironic-operator</code> applies a reasonable working default configuration, but if you override them with your prior configuration, your experience might not be ideal or your new Bare Metal Provisioning service fails to operate. Similarly, additional configuration might be necessary, for example, if you enable and use additional hardware types in your <code>ironic.conf</code> file.</p>
</div>
<div class="paragraph">
<p>The model of reasonable defaults includes commonly used hardware-types and driver interfaces. For example, the <code>redfish-virtual-media</code> boot interface and the <code>ramdisk</code> deploy interface are enabled by default. If you add new bare metal nodes after the adoption is complete, the driver interface selection occurs based on the order of precedence in the configuration if you do not explicitly set it on the node creation request or as an established default in the <code>ironic.conf</code> file.</p>
</div>
<div class="paragraph">
<p>Some configuration parameters do not need to be set on an individual node level, for example, network UUID values, or they are centrally configured in the <code>ironic.conf</code> file, as the setting controls security behavior.</p>
</div>
<div class="paragraph">
<p>It is critical that you maintain the following parameters that you configured and formatted as <code>[section]</code> and parameter name from the prior deployment to the new deployment. These parameters that govern the underlying behavior and values in the previous configuration would have used specific values if set.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>[neutron]cleaning_network</p>
</li>
<li>
<p>[neutron]provisioning_network</p>
</li>
<li>
<p>[neutron]rescuing_network</p>
</li>
<li>
<p>[neutron]inspection_network</p>
</li>
<li>
<p>[conductor]automated_clean</p>
</li>
<li>
<p>[deploy]erase_devices_priority</p>
</li>
<li>
<p>[deploy]erase_devices_metadata_priority</p>
</li>
<li>
<p>[conductor]force_power_state_during_sync</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can set the following parameters individually on a node. However, you might choose to use embedded configuration options to avoid the need to set the parameters individually when creating or managing bare metal nodes. Check your prior <code>ironic.conf</code> file for these parameters, and if set, apply a specific override configuration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>[conductor]bootloader</p>
</li>
<li>
<p>[conductor]rescue_ramdisk</p>
</li>
<li>
<p>[conductor]rescue_kernel</p>
</li>
<li>
<p>[conductor]deploy_kernel</p>
</li>
<li>
<p>[conductor]deploy_ramdisk</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The instances of <code>kernel_append_params</code>, formerly <code>pxe_append_params</code> in the <code>[pxe]</code> and <code>[redfish]</code> configuration sections, are used to apply boot time options like "console" for the deployment ramdisk and as such often must be changed.</p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
You cannot migrate hardware types that are set with the <code>ironic.conf</code> file <code>enabled_hardware_types</code> parameter, and hardware type driver interfaces starting with <code>staging-</code> into the adopted configuration.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="deploying-the-bare-metal-provisioning-service_adopting-bare-metal-provisioning">4.10.2. Deploying the Bare Metal Provisioning service</h4>
<div class="paragraph _abstract">
<p>To deploy the Bare Metal Provisioning service (ironic), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Bare Metal Provisioning service disabled. The <code>ironic-operator</code> applies the configuration and starts the Bare Metal Provisioning services. After the services are running, the Bare Metal Provisioning service automatically begins polling the power state of the bare-metal nodes that it manages.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
By default, RHOSO versions 18.0 and later of the Bare Metal Provisioning service include a new multi-tenant aware role-based access control (RBAC) model. As a result, bare-metal nodes might be missing when you run the <code>openstack baremetal node list</code> command after you adopt the Bare Metal Provisioning service. Your nodes are not deleted. Due to the increased access restrictions in the RBAC model, you must identify which project owns the missing bare-metal nodes and set the <code>owner</code> field on each missing bare-metal node.
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have imported the service databases into the control plane database.</p>
</li>
<li>
<p>The Bare Metal Provisioning service is disabled in the RHOSO 18.0. The following command should return a string of <code>false</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get openstackcontrolplanes.core.openstack.org &lt;name&gt; -o jsonpath='{.spec.ironic.enabled}'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;name&gt;</code> with the name of your existing <code>OpenStackControlPlane</code> CR, for example, <code>openstack-control-plane</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>The Identity service (keystone), Networking service (neutron), and Image Service (glance) are operational.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you use the Bare Metal Provisioning service in a Bare Metal as a Service configuration, do not adopt the Compute service (nova) before you adopt the Bare Metal Provisioning service.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>For the Bare Metal Provisioning service conductor services, the services must be able to reach Baseboard Management Controllers of hardware that is configured to be managed by the Bare Metal Provisioning service. If this hardware is unreachable, the nodes might enter "maintenance" state and be unavailable until connectivity is restored later.</p>
</li>
<li>
<p>You have downloaded the <code>ironic.conf</code> file locally:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/ironic/etc/ironic/ironic.conf &gt; ironic.conf</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This configuration file must come from one of the Controller nodes and not a director undercloud node. The director undercloud node operates with different configuration that does not apply when you adopt the Overcloud Ironic deployment.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If you are adopting the Ironic Inspector service, you need the value of the <code>IronicInspectorSubnets</code> director parameter. Use the same values to populate the <code>dhcpRanges</code> parameter in the RHOSO environment.</p>
</li>
<li>
<p>You have defined the following shell variables. Replace the following example values with values that apply to your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource (CR) to deploy the Bare Metal Provisioning service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ironic:
    enabled: true
    template:
      rpcTransport: oslo
      databaseInstance: openstack
      ironicAPI:
        replicas: 1
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: &lt;loadBalancer_IP&gt;
              spec:
                type: LoadBalancer
      ironicConductors:
      - replicas: 1
        networkAttachments:
          - baremetal
        provisionNetwork: baremetal
        storageRequest: 10G
        customServiceConfig: |
          [neutron]
          cleaning_network=&lt;cleaning network uuid&gt;
          provisioning_network=&lt;provisioning network uuid&gt;
          rescuing_network=&lt;rescuing network uuid&gt;
          inspection_network=&lt;introspection network uuid&gt;
          [conductor]
          automated_clean=true
      ironicInspector:
        replicas: 1
        inspectionNetwork: baremetal
        networkAttachments:
          - baremetal
        dhcpRanges:
          - name: inspector-0
            cidr: 172.20.1.0/24
            start: 172.20.1.190
            end: 172.20.1.199
            gateway: 172.20.1.1
        serviceUser: ironic-inspector
        databaseAccount: ironic-inspector
        passwordSelectors:
          database: IronicInspectorDatabasePassword
          service: IronicInspectorPassword
      ironicNeutronAgent:
        replicas: 1
        messagingBus:
          cluster: rabbitmq
      secret: osp-secret
'</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;loadBalancer_IP&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>Wait for the Bare Metal Provisioning service control plane services CRs to become ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s ironics.ironic.openstack.org ironic</pre>
</div>
</div>
</li>
<li>
<p>Verify that the individual services are ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s ironicapis.ironic.openstack.org ironic-api
$ oc wait --for condition=Ready --timeout=300s ironicconductors.ironic.openstack.org ironic-conductor
$ oc wait --for condition=Ready --timeout=300s ironicinspectors.ironic.openstack.org ironic-inspector
$ oc wait --for condition=Ready --timeout=300s ironicneutronagents.ironic.openstack.org ironic-ironic-neutron-agent</pre>
</div>
</div>
</li>
<li>
<p>Update the DNS Nameservers on the provisioning, cleaning, and rescue networks:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For name resolution to work for Bare Metal Provisioning service operations, you must set the DNS nameserver to use the internal DNS servers in the RHOSO control plane:
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack subnet set --dns-nameserver 192.168.122.80 provisioning-subnet</pre>
</div>
</div>
</li>
<li>
<p>Verify that no Bare Metal Provisioning service nodes are missing from the node list:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack baremetal node list</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
If the <code>openstack baremetal node list</code> command output reports an incorrect power status, wait a few minutes and re-run the command to see if the output syncs with the actual state of the hardware being managed. The time required for the Bare Metal Provisioning service to review and reconcile the power state of bare-metal nodes depends on the number of operating conductors through the <code>replicas</code> parameter and which are present in the Bare Metal Provisioning service deployment being adopted.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If any Bare Metal Provisioning service nodes are missing from the <code>openstack baremetal node list</code> command, temporarily disable the new RBAC policy to see the nodes again:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ironic:
    enabled: true
    template:
      databaseInstance: openstack
      ironicAPI:
        replicas: 1
        customServiceConfig: |
          [oslo_policy]
          enforce_scope=false
          enforce_new_defaults=false
'</pre>
</div>
</div>
<div class="paragraph">
<p>After this configuration is applied, the operator restarts the Ironic API service and disables the new RBAC policy that is enabled by default.</p>
</div>
</li>
<li>
<p>View the bare-metal nodes that do not have an owner assigned:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack baremetal node list --long -c UUID -c Owner -c 'Provisioning State'</pre>
</div>
</div>
</li>
<li>
<p>Assign all bare-metal nodes with no owner to a new project, for example, the admin project:</p>
<div class="listingblock">
<div class="content">
<pre>ADMIN_PROJECT_ID=$(openstack project show -c id -f value --domain default admin)
for node in $(openstack baremetal node list -f json -c UUID -c Owner | jq -r '.[] | select(.Owner == null) | .UUID'); do openstack baremetal node set --owner $ADMIN_PROJECT_ID $node; done</pre>
</div>
</div>
</li>
<li>
<p>Re-apply the default RBAC by removing the <code>customServiceConfig</code> section or by setting the following values in the <code>customServiceConfig</code> section to <code>true</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ironic:
    enabled: true
    template:
      databaseInstance: openstack
      ironicAPI:
        replicas: 1
        customServiceConfig: |
          [oslo_policy]
          enforce_scope=true
          enforce_new_defaults=true
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify the list of endpoints:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list |grep ironic</pre>
</div>
</div>
</li>
<li>
<p>Verify the list of bare-metal nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack baremetal node list</pre>
</div>
</div>
</li>
<li>
<p>Reset the deploy images on all bare-metal nodes to use the new centrally configured images:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After adoption, bare-metal nodes might still reference the old deployment&#8217;s kernel and ramdisk
images in their <code>driver_info</code> fields. Resetting these values causes the Bare Metal Provisioning service to use the
new centrally configured  <code>deploy_kernel</code> and <code>deploy_ramdisk</code> values from the <code>ironic.conf</code> file.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>for node in $(openstack baremetal node list -c UUID -f value); do
  openstack baremetal node set $node \
    --driver-info deploy_ramdisk= \
    --driver-info deploy_kernel=
done</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-compute-service_hsm-integration">4.11. Adopting the Compute service</h3>
<div class="paragraph _abstract">
<p>To adopt the Compute service (nova), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Compute service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment. The following procedure describes a single-cell setup.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have completed the previous adoption steps.</p>
</li>
<li>
<p>You have defined the following shell variables. Replace the following example values with the values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>alias openstack="oc exec -t openstackclient -- openstack"

DEFAULT_CELL_NAME="cell3"
RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The source cloud <code>default</code> cell takes a new <code>$DEFAULT_CELL_NAME</code>. In a multi-cell adoption scenario, the <em>default</em> cell might retain its original name,<code>DEFAULT_CELL_NAME=default</code>, or become renamed as a cell that is free for use. Do not use other existing cell names for <code>DEFAULT_CELL_NAME</code>, except for <code>default</code>.</p>
</li>
<li>
<p>If you deployed the source cloud with a <code>default</code> cell, and want to rename it during adoption, define the new name that you want to use, as shown in the following example:</p>
<div class="listingblock">
<div class="content">
<pre>DEFAULT_CELL_NAME="cell1"
RENAMED_CELLS="cell1"</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Compute service:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This procedure assumes that Compute service metadata is deployed on the top level and not on each cell level. If the RHOSP deployment has a per-cell metadata deployment, adjust the following patch as needed. You cannot run the metadata service in <code>cell0</code>.
To enable the metadata services of a local cell, set the <code>enabled</code> property in the <code>metadataServiceTemplate</code> field of the local cell to <code>true</code> in the <code>OpenStackControlPlane</code> CR.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ rm -f celltemplates
$ for CELL in $(echo $RENAMED_CELLS); do
&gt;  cat &gt;&gt; celltemplates &lt;&lt; EOF
&gt;       ${CELL}:
&gt;         <strong>hasAPIAccess: true</strong>
&gt;         cellDatabaseAccount: nova-$CELL
&gt;         <strong>cellDatabaseInstance: openstack-$CELL</strong>
&gt;         <strong>cellMessageBusInstance: rabbitmq-$CELL</strong>
&gt;         metadataServiceTemplate:
&gt;           enabled: false
&gt;           override:
&gt;               service:
&gt;                 metadata:
&gt;                   annotations:
&gt;                     metallb.universe.tf/address-pool: internalapi
&gt;                     metallb.universe.tf/allow-shared-ip: internalapi
&gt;                     metallb.universe.tf/loadBalancerIPs: 172.17.0.$(( 79 + ${CELL##*cell} ))
&gt;                 spec:
&gt;                   type: LoadBalancer
&gt;           customServiceConfig: |
&gt;             [workarounds]
&gt;             disable_compute_service_check_for_ffu=true
&gt;         conductorServiceTemplate:
&gt;           customServiceConfig: |
&gt;             [workarounds]
&gt;             disable_compute_service_check_for_ffu=true
&gt;EOF
&gt;done

$ cat &gt; oscp-patch.yaml &lt;&lt; EOF
&gt;spec:
&gt;  nova:
&gt;   enabled: true
&gt;   apiOverride:
&gt;     route: {}
&gt;   template:
&gt;     secret: osp-secret
&gt;     apiDatabaseAccount: nova-api
&gt;     apiServiceTemplate:
&gt;       override:
&gt;         service:
&gt;           internal:
&gt;             metadata:
&gt;               annotations:
&gt;                 metallb.universe.tf/address-pool: internalapi
&gt;                 metallb.universe.tf/allow-shared-ip: internalapi
&gt;                 metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;
&gt;             spec:
&gt;               type: LoadBalancer
&gt;       customServiceConfig: |
&gt;         [workarounds]
&gt;         disable_compute_service_check_for_ffu=true
&gt;     metadataServiceTemplate:
&gt;       enabled: true
&gt;       override:
&gt;         service:
&gt;           metadata:
&gt;             annotations:
&gt;               metallb.universe.tf/address-pool: internalapi
&gt;               metallb.universe.tf/allow-shared-ip: internalapi
&gt;               metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;
&gt;           spec:
&gt;             type: LoadBalancer
&gt;       customServiceConfig: |
&gt;         [workarounds]
&gt;         disable_compute_service_check_for_ffu=true
&gt;     schedulerServiceTemplate:
&gt;       customServiceConfig: |
&gt;         [workarounds]
&gt;         disable_compute_service_check_for_ffu=true
&gt;     cellTemplates:
&gt;       cell0:
&gt;         hasAPIAccess: true
&gt;         cellDatabaseAccount: nova-cell0
&gt;         cellDatabaseInstance: openstack
&gt;         cellMessageBusInstance: rabbitmq
&gt;         conductorServiceTemplate:
&gt;           customServiceConfig: |
&gt;             [workarounds]
&gt;             disable_compute_service_check_for_ffu=true
&gt;EOF
$ cat celltemplates &gt;&gt; oscp-patch.yaml
$ oc patch openstackcontrolplane openstack  --type=merge --patch-file=oscp-patch.yaml</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>${CELL}.hasAPIAccess</code> specifies upcall access to the API. In the source cloud, cells are always configured with the main Nova API database upcall access. You can disable upcall access to the API by setting <code>hasAPIAccess</code> to <code>false</code>. However, do not make changes to the API during adoption.</p>
</li>
<li>
<p><code>${CELL}.cellDatabaseInstance</code> specifies the database instance that is used by the cell. The database instance names must match the names that are defined in the <code>OpenStackControlPlane</code> CR that you created in when you deployed the back-end services as described in <a href="#deploying-backend-services__migrating-databases">Deploying back-end services</a>.</p>
</li>
<li>
<p><code>${CELL}.cellMessageBusInstance</code> specifies the message bus instance that is used by the cell. The message bus instance names must match the names that are defined in the <code>OpenStackControlPlane</code> CR.</p>
</li>
<li>
<p><code>metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;</code> specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If you are adopting the Compute service with the Bare Metal Provisioning service (ironic), append the <code>novaComputeTemplates</code> field with the following content in each cell in the Compute service CR patch. For example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        cell1:
          novaComputeTemplates:
            standalone:
              customServiceConfig: |
                [DEFAULT]
                host = &lt;hostname&gt;
                [workarounds]
                disable_compute_service_check_for_ffu=true
              computeDriver: ironic.IronicDriver
        ...</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;hostname&gt;</code> with the hostname of the node that is running the <code>ironic</code> Compute driver in the source cloud.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Wait for the CRs for the Compute control plane services to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s Nova/nova</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The local Conductor services are started for each cell, while the superconductor runs in <code>cell0</code>.
Note that <code>disable_compute_service_check_for_ffu</code> is mandatory for all imported Compute services until the external data plane is imported, and until the Compute services are fast-forward upgraded. For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a> and <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Upgrading Compute services</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Check that Compute service endpoints are defined and pointing to the
control plane FQDNs, and that the Nova API responds:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep nova
$ openstack server list</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Compare the outputs with the topology-specific configuration in <a href="#proc_retrieving-topology-specific-service-configuration_migrating-databases">Retrieving topology-specific service configuration</a>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Query the superconductor to check that the expected cells exist, and compare it to its pre-adoption values:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $CELLS); do
set +u
. ~/.source_cloud_exported_variables_$CELL
set -u
RCELL=$CELL
[ "$CELL" = "default" ] &amp;&amp; RCELL=$DEFAULT_CELL_NAME

echo "comparing $CELL to $RCELL"
echo $PULL_OPENSTACK_CONFIGURATION_NOVAMANAGE_CELL_MAPPINGS | grep -F "| $CELL |"
oc rsh nova-cell0-conductor-0 nova-manage cell_v2 list_cells | grep -F "| $RCELL |"
done</pre>
</div>
</div>
<div class="paragraph">
<p>The following changes are expected for each cell:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>cellX</code> <code>nova</code> database and username become <code>nova_cellX</code>.</p>
</li>
<li>
<p>The <code>default</code> cell is renamed to <code>DEFAULT_CELL_NAME</code>. The <code>default</code> cell might retain the original name if there are multiple cells.</p>
</li>
<li>
<p>The RabbitMQ transport URL no longer uses <code>guest</code>.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>At this point, the Compute service control plane services do not control the existing Compute service workloads. The control plane manages the data plane only after the data adoption process is completed. For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
To import external Compute services to the RHOSO data plane, you must upgrade them first.
For more information, see <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>, and <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-block-storage-service_hsm-integration">4.12. Adopting the Block Storage service</h3>
<div class="paragraph _abstract">
<p>To adopt a director-deployed Block Storage service (cinder), create the manifest based on the existing <code>cinder.conf</code> file, deploy the Block Storage service, and validate the new deployment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have reviewed the Block Storage service limitations. For more information, see <a href="#block-storage-limitations_storage-requirements">Limitations for adopting the Block Storage service</a>.</p>
</li>
<li>
<p>You have planned the placement of the Block Storage services.</p>
</li>
<li>
<p>You have prepared the Red Hat OpenShift Container Platform (RHOCP) nodes where the volume and backup services run. For more information, see <a href="#openshift-preparation-for-block-storage-adoption_storage-requirements">RHOCP preparation for Block Storage service adoption</a>.</p>
</li>
<li>
<p>The Block Storage service (cinder) is stopped.</p>
</li>
<li>
<p>The service databases are imported into the control plane MariaDB.</p>
</li>
<li>
<p>The Identity service (keystone) is adopted.</p>
</li>
<li>
<p>If your Red&#160;Hat OpenStack Platform 17.1 deployment included the Key Manager service (barbican), the Key Manager service is adopted.</p>
</li>
<li>
<p>The Storage network is correctly configured on the RHOCP cluster.</p>
</li>
<li>
<p>The contents of <code>cinder.conf</code> file. Download the file so that you can access it locally:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/cinder/etc/cinder/cinder.conf &gt; cinder.conf</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new file, for example, <code>cinder_api.patch</code>, and apply the configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;patch_name&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;patch_name&gt;</code> with the name of your patch file.</p>
<div class="paragraph">
<p>The following example shows a <code>cinder_api.patch</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
  cinder:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: cinder
      secret: osp-secret
      cinderAPI:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: <strong>&lt;172.17.0.80&gt;</strong>
              spec:
                type: LoadBalancer
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          default_volume_type=tripleo
      cinderScheduler:
        replicas: 0
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 0
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 0</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;172.17.0.80&gt;</dt>
<dd>
<p>Specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</dd>
</dl>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Retrieve the list of the previous scheduler and backup services:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-scheduler | standalone.localdomain | nova | enabled | down  | 2024-11-04T17:47:14.000000 |
| cinder-backup    | standalone.localdomain | nova | enabled | down  | 2024-11-04T17:47:14.000000 |
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | down  | 2024-11-04T17:47:14.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>Remove services for hosts that are in the <code>down</code> state:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -t cinder-api-0 -c cinder-api -- cinder-manage service remove &lt;service_binary&gt; &lt;service_host&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;service_binary&gt;</code> with the name of the binary, for example, <code>cinder-backup</code>.</p>
</li>
<li>
<p>Replace <code>&lt;service_host&gt;</code> with the host name, for example, <code>cinder-backup-0</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Deploy the scheduler, backup, and volume services:</p>
<div class="ulist">
<ul>
<li>
<p>Create another file, for example, <code>cinder_services.patch</code>, and apply the configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;patch_name&gt;</pre>
</div>
</div>
</li>
<li>
<p>Replace <code>&lt;patch_name&gt;</code> with the name of your patch file.</p>
</li>
<li>
<p>The following example shows a <code>cinder_services.patch</code> file for a Ceph RBD deployment:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderScheduler:
        replicas: 1
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          backup_driver=cinder.backup.drivers.ceph.CephBackupDriver
          backup_ceph_conf=/etc/ceph/ceph.conf
          backup_ceph_user=openstack
          backup_ceph_pool=backups
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 1
          customServiceConfig: |
            [tripleo_ceph]
            backend_host=hostgroup
            volume_backend_name=tripleo_ceph
            volume_driver=cinder.volume.drivers.rbd.RBDDriver
            rbd_ceph_conf=/etc/ceph/ceph.conf
            rbd_user=openstack
            rbd_pool=volumes
            rbd_flatten_volume_from_snapshot=False
            report_discard_supported=True</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Ensure that you use the same configuration group name for the driver that you used in the source cluster. In this example, the driver configuration group in <code>customServiceConfig</code> is called <code>tripleo_ceph</code> because it reflects the value of the configuration group name in the <code>cinder.conf</code> file of the source OpenStack cluster.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Configure the NetApp NFS Block Storage volume service:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a secret that includes sensitive information such as hostnames, passwords, and usernames to access the third-party NetApp NFS storage. You can find the credentials in the <code>cinder.conf</code> file that was generated from the director deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  labels:
    service: cinder
    component: cinder-volume
  name: cinder-volume-ontap-secrets
type: Opaque
stringData:
  ontap-cinder-secrets: |
    [tripleo_netapp]
    netapp_login= netapp_username
    netapp_password= netapp_password
    netapp_vserver= netapp_vserver
    nas_host= netapp_nfsip
    nas_share_path=/netapp_nfspath
    netapp_pool_name_search_pattern=(netapp_poolpattern)
EOF</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy NetApp NFS Block Storage volume back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;cinder_netappNFS.patch&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;cinder_netappNFS.patch&gt;</code> with the name of the patch file for your NetApp NFS Block Storage volume back end.</p>
<div class="paragraph">
<p>The following example shows a <code>cinder_netappNFS.patch</code> file that configures a NetApp NFS Block Storage volume service:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">spec:
  cinder:
    enabled: true
    template:
      cinderVolumes:
        ontap-nfs:
          networkAttachments:
            - storage
          customServiceConfig: |
            [tripleo_netapp]
            volume_backend_name=ontap-nfs
            volume_driver=cinder.volume.drivers.netapp.common.NetAppDriver
            nfs_snapshot_support=true
            nas_secure_file_operations=false
            nas_secure_file_permissions=false
            netapp_server_hostname= netapp_backendip
            netapp_server_port=80
            netapp_storage_protocol=nfs
            netapp_storage_family=ontap_cluster
          customServiceConfigSecrets:
          - cinder-volume-ontap-secrets</code></pre>
</div>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Configure the NetApp iSCSI Block Storage volume service:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a secret that includes sensitive information such as hostnames, passwords, and usernames to access the third-party NetApp iSCSI storage. You can find the credentials in the <code>cinder.conf</code> file that was generated from the director deployment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  labels:
    service: cinder
    component: cinder-volume
  name: cinder-volume-ontap-secrets
type: Opaque
stringData:
  ontap-cinder-secrets: |
    [tripleo_netapp]
    netapp_server_hostname = netapp_host
    netapp_login = netapp_username
    netapp_password = netapp_password
    netapp_vserver = netapp_vserver
    netapp_pool_name_search_pattern=(netapp_poolpattern)
EOF</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource (CR) to deploy the NetApp iSCSI Block Storage volume back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=&lt;cinder_netappISCSI.patch&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;cinder_netappISCSI.patch&gt;</code> with the name of the patch file for your NetApp iSCSI Block Storage volume back end.</p>
<div class="paragraph">
<p>The following example shows a <code>cinder_netappISCSI.patch</code> file that configures a NetApp iSCSI Block Storage volume service:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>spec:
  cinder:
    enabled: true
    template:
      cinderVolumes:
        ontap-iscsi:
          networkAttachments:
            - storage
          customServiceConfig: |
            [tripleo_netapp]
            volume_backend_name=ontap-iscsi
            volume_driver=cinder.volume.drivers.netapp.common.NetAppDriver
            netapp_storage_protocol=iscsi
            netapp_storage_family=ontap_cluster
            consistencygroup_support=True
          customServiceConfigSecrets:
            - cinder-volume-ontap-secrets</pre>
</div>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Check if all the services are up and running:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume service list

+------------------+--------------------------+------+---------+-------+----------------------------+
| Binary           | Host                     | Zone | Status  | State | Updated At                 |
+------------------+--------------------------+------+---------+-------+----------------------------+
| cinder-volume    | hostgroup@tripleo_netapp | nova | enabled | up    | 2023-06-28T17:00:03.000000 |
| cinder-scheduler | cinder-scheduler-0       | nova | enabled | up    | 2023-06-28T17:00:02.000000 |
| cinder-backup    | cinder-backup-0          | nova | enabled | up    | 2023-06-28T17:00:01.000000 |
+------------------+--------------------------+------+---------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>Apply the DB data migrations:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You are not required to run the data migrations at this step, but you must run them before the next upgrade. However, for adoption, you can run the migrations now to ensure that there are no issues before you run production workloads on the deployment.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it cinder-scheduler-0 -- cinder-manage db online_data_migrations</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Ensure that the <code>openstack</code> alias is defined:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"</pre>
</div>
</div>
</li>
<li>
<p>Confirm that Block Storage service endpoints are defined and pointing to the control plane FQDNs:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list --service &lt;endpoint&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;endpoint&gt;</code> with the name of the endpoint that you want to confirm.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Confirm that the Block Storage services are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume service list</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Cinder API services do not appear in the list. However, if you get a response from the <code>openstack volume service list</code> command, that means at least one of the cinder API services is running.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Confirm that you have your previous volume types, volumes, snapshots, and backups:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume type list
$ openstack volume list
$ openstack volume snapshot list
$ openstack volume backup list</pre>
</div>
</div>
</li>
<li>
<p>To confirm that the configuration is working, perform the following steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a volume from an image to check that the connection to Image Service (glance) is working:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack volume create --image cirros --bootable --size 1 disk_new</pre>
</div>
</div>
</li>
<li>
<p>Back up the previous attached volume:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack --os-volume-api-version 3.47 volume create --backup &lt;backup_name&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;backup_name&gt;</code> with the name of your new backup location.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You do not boot a Compute service (nova) instance by using the new <code>volume from</code> image or try to detach the previous volume because the Compute service and the Block Storage service are still not connected.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-openstack-dashboard_hsm-integration">4.13. Adopting the Dashboard service</h3>
<div class="paragraph _abstract">
<p>To adopt the Dashboard service (horizon), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has the Dashboard service disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You adopted Memcached. For more information, see <a href="#deploying-backend-services_migrating-databases">Deploying back-end services</a>.</p>
</li>
<li>
<p>You adopted the Identity service (keystone). For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Dashboard service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  horizon:
    enabled: true
    apiOverride:
      route: {}
    template:
      memcachedInstance: memcached
      secret: osp-secret
'</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the Dashboard service instance is successfully deployed and ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get horizon</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the Dashboard service is reachable and returns a <code>200</code> status code:</p>
<div class="listingblock">
<div class="content">
<pre>PUBLIC_URL=$(oc get horizon horizon -o jsonpath='{.status.endpoint}')
curl --silent --output /dev/stderr --head --write-out "%{http_code}" "$PUBLIC_URL/dashboard/auth/login/?next=/dashboard/" -k | grep 200</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-shared-file-systems-service_hsm-integration">4.14. Adopting the Shared File Systems service</h3>
<div class="paragraph _abstract">
<p>The Shared File Systems service (manila) in Red&#160;Hat OpenStack Services on OpenShift (RHOSO) provides a self-service API to create and manage file shares. File shares (or "shares"), are built for concurrent read/write access from multiple clients. This makes the Shared File Systems service essential in cloud environments that require a ReadWriteMany persistent storage.</p>
</div>
<div class="paragraph">
<p>File shares in RHOSO require network access. Ensure that the networking in the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 environment matches the network plans for your new cloud after adoption. This ensures that tenant workloads remain connected to storage during the adoption process. The Shared File Systems service control plane services are not in the data path. Shutting down the API, scheduler, and share manager services do not impact access to existing shared file systems.</p>
</div>
<div class="paragraph">
<p>Typically, storage and storage device management are separate networks. Shared File Systems services only need access to the storage device management network.
For example, if you used a Red Hat Ceph Storage cluster in the deployment, the "storage"
network refers to the Red Hat Ceph Storage cluster&#8217;s public network, and the Shared File Systems service&#8217;s share manager service needs to be able to reach it.</p>
</div>
<div class="paragraph">
<p>The Shared File Systems service supports the following storage networking scenarios:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You can directly control the networking for your respective file shares.</p>
</li>
<li>
<p>The RHOSO administrator configures the storage networking.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="preparing-the-shared-file-systems-service-configuration_adopting-shared-file-systems">4.14.1. Guidelines for preparing the Shared File Systems service configuration</h4>
<div class="paragraph _abstract">
<p>To deploy Shared File Systems service (manila) on the control plane, you must copy the original configuration file from the Red&#160;Hat OpenStack Platform 17.1 deployment. You must review the content in the file to make sure you are adopting the correct configuration for Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0. Not all of the content needs to be brought into the new cloud environment.</p>
</div>
<div class="paragraph">
<p>Review the following guidelines for preparing your Shared File Systems service configuration file for adoption:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Shared File Systems service operator sets up the following configurations and can be ignored:</p>
<div class="ulist">
<ul>
<li>
<p>Database-related configuration (<code>[database]</code>)</p>
</li>
<li>
<p>Service authentication (<code>auth_strategy</code>, <code>[keystone_authtoken]</code>)</p>
</li>
<li>
<p>Message bus configuration (<code>transport_url</code>, <code>control_exchange</code>)</p>
</li>
<li>
<p>The default paste config (<code>api_paste_config</code>)</p>
</li>
<li>
<p>Inter-service communication configuration (<code>[neutron]</code>, <code>[nova]</code>, <code>[cinder]</code>, <code>[glance]</code> <code>[oslo_messaging_*]</code>)</p>
</li>
</ul>
</div>
</li>
<li>
<p>Ignore the <code>osapi_share_listen</code> configuration. In Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0, you rely on Red Hat OpenShift Container Platform (RHOCP) routes and ingress.</p>
</li>
<li>
<p>Check for policy overrides. In RHOSO 18.0, the Shared File Systems service ships with a secure default Role-based access control (RBAC), and overrides might not be necessary.</p>
</li>
<li>
<p>If a custom policy is necessary, you must provide it as a <code>ConfigMap</code>. The following example spec illustrates how you can set up a <code>ConfigMap</code> called <code>manila-policy</code> with the contents of a file called <code>policy.yaml</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        manilaAPI:
          customServiceConfig: |
             [oslo_policy]
             policy_file=/etc/manila/policy.yaml
        extraMounts:
        - extraVol:
          - extraVolType: Undefined
            mounts:
            - mountPath: /etc/manila/
              name: policy
              readOnly: true
            propagation:
            - ManilaAPI
            volumes:
            - name: policy
              projected:
                sources:
                - configMap:
                    name: manila-policy
                    items:
                      - key: policy
                        path: policy.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>The value of the <code>host</code> option under the <code>[DEFAULT]</code> section must be <code>hostgroup</code>.</p>
</li>
<li>
<p>To run the Shared File Systems service API service, you must add the <code>enabled_share_protocols</code> option to the <code>customServiceConfig</code> section in <code>manila: template: manilaAPI</code>.</p>
</li>
<li>
<p>If you have scheduler overrides, add them to the <code>customServiceConfig</code>
section in <code>manila: template: manilaScheduler</code>.</p>
</li>
<li>
<p>If you have multiple storage back-end drivers configured with RHOSP 17.1, you need to split them up when deploying RHOSO 18.0. Each storage back-end driver needs to use its own instance of the <code>manila-share</code> service.</p>
</li>
<li>
<p>If a storage back-end driver needs a custom container image, find it in the
<a href="https://catalog.redhat.com/software/containers/search?gs&amp;q=manila">Red Hat Ecosystem Catalog</a>, and create or modify an <code>OpenStackVersion</code> custom resource (CR) to specify the custom image using the same <code>custom name</code>.</p>
<div class="paragraph">
<p>The following example shows a manila spec from the <code>OpenStackControlPlane</code> CR that includes multiple storage back-end drivers, where only one is using a custom container image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        manilaAPI:
          customServiceConfig: |
            [DEFAULT]
            enabled_share_protocols = nfs
          replicas: 3
        manilaScheduler:
          replicas: 3
        manilaShares:
         netapp:
           customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends = netapp
             host = hostgroup
             [netapp]
             driver_handles_share_servers = False
             share_backend_name = netapp
             share_driver = manila.share.drivers.netapp.common.NetAppDriver
             netapp_storage_family = ontap_cluster
             netapp_transport_type = http
           replicas: 1
         pure:
            customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends=pure-1
             host = hostgroup
             [pure-1]
             driver_handles_share_servers = False
             share_backend_name = pure-1
             share_driver = manila.share.drivers.purestorage.flashblade.FlashBladeShareDriver
             flashblade_mgmt_vip = 203.0.113.15
             flashblade_data_vip = 203.0.10.14
            replicas: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>The following example shows the <code>OpenStackVersion</code> CR that defines the custom container image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: core.openstack.org/v1beta1
kind: OpenStackVersion
metadata:
  name: openstack
spec:
  customContainerImages:
    cinderVolumeImages:
      pure: registry.connect.redhat.com/purestorage/openstack-manila-share-pure-rhosp-18-0</code></pre>
</div>
</div>
<div class="paragraph">
<p>The name of the <code>OpenStackVersion</code> CR must match the name of your <code>OpenStackControlPlane</code> CR.</p>
</div>
</li>
<li>
<p>If you are providing sensitive information, such as passwords, hostnames, and usernames, use RHOCP secrets, and the <code>customServiceConfigSecrets</code> key. You can use <code>customConfigSecrets</code> in any service. If you use third party storage that requires credentials, create a secret that is referenced in the manila CR/patch file by using the <code>customServiceConfigSecrets</code> key. For example:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Create a file that includes the secrets, for example, <code>netapp_secrets.conf</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/netapp_secrets.conf

[netapp]
netapp_server_hostname = 203.0.113.10
netapp_login = fancy_netapp_user
netapp_password = secret_netapp_password
netapp_vserver = mydatavserver
__EOF__</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic osp-secret-manila-netapp --from-file=~/&lt;secret&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;secret&gt;</code> with the name of the file that includes your secrets, for example, <code>netapp_secrets.conf</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Add the secret to any Shared File Systems service file in the <code>customServiceConfigSecrets</code> section. The following example adds the <code>osp-secret-manila-netapp</code> secret to the <code>manilaShares</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">  spec:
    manila:
      enabled: true
      template:
        &lt; . . . &gt;
        manilaShares:
         netapp:
           customServiceConfig: |
             [DEFAULT]
             debug = true
             enabled_share_backends = netapp
             host = hostgroup
             [netapp]
             driver_handles_share_servers = False
             share_backend_name = netapp
             share_driver = manila.share.drivers.netapp.common.NetAppDriver
             netapp_storage_family = ontap_cluster
             netapp_transport_type = http
           customServiceConfigSecrets:
             - osp-secret-manila-netapp
           replicas: 1
    &lt; . . . &gt;</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="deploying-file-systems-service-control-plane_adopting-shared-file-systems">4.14.2. Deploying the Shared File Systems service on the control plane</h4>
<div class="paragraph _abstract">
<p>Copy the Shared File Systems service (manila) configuration from the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 deployment, and then deploy the Shared File Systems service on the control plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The Shared File Systems service systemd services such as <code>api</code>, <code>cron</code>, and <code>scheduler</code> are stopped. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>If the deployment uses CephFS through NFS as a storage back end, the Pacemaker ordering and collocation constraints are adjusted. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>The Shared File Systems service Pacemaker service (<code>openstack-manila-share</code>) is stopped. For more information, see <a href="#stopping-openstack-services_migrating-databases">Stopping Red&#160;Hat OpenStack Platform services</a>.</p>
</li>
<li>
<p>The database migration is complete. For more information, see <a href="#migrating-databases-to-mariadb-instances_migrating-databases">Migrating databases to MariaDB instances</a>.</p>
</li>
<li>
<p>The Red Hat OpenShift Container Platform (RHOCP) nodes where the <code>manila-share</code> service is to be deployed can reach the management network that the storage system is in.</p>
</li>
<li>
<p>If the deployment uses CephFS through NFS as a storage back end, a new clustered Ceph NFS service is deployed on the Red Hat Ceph Storage cluster with the help
of Ceph orchestrator. For more information, see <a href="#creating-a-ceph-nfs-cluster_ceph-prerequisites">Creating a Ceph NFS cluster</a>.</p>
</li>
<li>
<p>Services such as the Identity service (keystone) and memcached are available prior to adopting the Shared File Systems services.</p>
</li>
<li>
<p>If you enabled tenant-driven networking by setting <code>driver_handles_share_servers=True</code>, the Networking service (neutron) is deployed.</p>
</li>
<li>
<p>The <code>CONTROLLER1_SSH</code> environment variable is defined and points to the RHOSP Controller node. Replace the following example values with values that are correct for your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH="ssh -i &lt;path to SSH key&gt; root@&lt;node IP&gt;"</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Copy the configuration file from RHOSP 17.1 for reference:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/manila/etc/manila/manila.conf | awk '!/^ *#/ &amp;&amp; NF' &gt; ~/manila.conf</pre>
</div>
</div>
</li>
<li>
<p>Review the configuration file for configuration changes that were made since RHOSP 17.1. For more information on preparing this file for Red&#160;Hat OpenStack Services on OpenShift (RHOSO), see <a href="#preparing-the-shared-file-systems-service-configuration_adopting-shared-file-systems">Guidelines for preparing the Shared File Systems service configuration</a>.</p>
</li>
<li>
<p>Create a patch file for the <code>OpenStackControlPlane</code> CR to deploy the Shared File Systems service. The following example <code>manila.patch</code> file uses native CephFS:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: manila
      secret: osp-secret
      manilaAPI:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_share_protocols = cephfs
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;
              spec:
                type: LoadBalancer
      manilaScheduler:
        replicas: 3
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = tripleo_ceph
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=cephfs
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_volume_mode=0755
            cephfs_protocol_helper_type=CEPHFS
          networkAttachments:
              - storage
      extraMounts:
      - name: v1
        region: r1
        extraVol:
          - propagation:
            - ManilaShare
          extraVolType: Ceph
          volumes:
          - name: ceph
            secret:
              secretName: ceph-conf-files
          mounts:
          - name: ceph
            mountPath: "/etc/ceph"
            readOnly: true
__EOF__</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>metallb.universe.tf/loadBalancerIPs:&lt;172.17.0.80&gt;</code> specifies the load balancer IP in your environment. If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, <code>metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80</code>.</p>
</li>
<li>
<p><code>share_backend_name</code> specifies the names of the back ends to use in Red&#160;Hat OpenStack Services on OpenShift (RHOSO). Ensure that the names of the back ends are the same as they were in RHOSP 17.1.</p>
</li>
<li>
<p><code>networkAttachments</code> specifies the appropriate storage management network. For example, the <code>manilaShares</code> instance with the CephFS back-end driver is connected to the <code>storage</code> network.</p>
</li>
<li>
<p><code>extraMounts</code> specifies additional files to add to any of the services. For example, when using Red Hat Ceph Storage, you can add the Shared File Systems service Ceph user&#8217;s keyring file as well as the <code>ceph.conf</code> configuration file.</p>
<div class="paragraph">
<p>The following example patch file uses CephFS through NFS:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      secret: osp-secret
      manilaAPI:
        replicas: 3
        customServiceConfig: |
          [DEFAULT]
          enabled_share_protocols = cephfs
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: &lt;172.17.0.80&gt;
              spec:
                type: LoadBalancer
      manilaScheduler:
        replicas: 3
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = cephfs
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=tripleo_ceph
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_protocol_helper_type=NFS
            cephfs_nfs_cluster_id=cephfs
            cephfs_ganesha_server_ip=172.17.5.47
          networkAttachments:
              - storage
__EOF__</pre>
</div>
</div>
</li>
<li>
<p>Prior to adopting the <code>manilaShares</code> service for CephFS through NFS, ensure that you create a clustered Ceph NFS service. The name of the service must be <code>cephfs_nfs_cluster_id</code>. The <code>cephfs_nfs_cluster_id</code> option is set with the name of the NFS cluster created on Red Hat Ceph Storage.</p>
</li>
<li>
<p>The <code>cephfs_ganesha_server_ip</code> option is preserved from the configuration on the RHOSP 17.1 environment.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=~/&lt;manila.patch&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;manila.patch&gt;</code> with the name of your patch file.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Inspect the resulting Shared File Systems service pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l service=manila</pre>
</div>
</div>
</li>
<li>
<p>Check that the Shared File Systems API service is registered in the Identity service (keystone):</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack service list | grep manila</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep manila

| 1164c70045d34b959e889846f9959c0e | regionOne | manila       | share        | True    | internal  | http://manila-internal.openstack.svc:8786/v1/%(project_id)s        |
| 63e89296522d4b28a9af56586641590c | regionOne | manilav2     | sharev2      | True    | public    | https://manila-public-openstack.apps-crc.testing/v2                |
| af36c57adcdf4d50b10f484b616764cc | regionOne | manila       | share        | True    | public    | https://manila-public-openstack.apps-crc.testing/v1/%(project_id)s |
| d655b4390d7544a29ce4ea356cc2b547 | regionOne | manilav2     | sharev2      | True    | internal  | http://manila-internal.openstack.svc:8786/v2                       |</pre>
</div>
</div>
</li>
<li>
<p>Test the health of the service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack share service list
$ openstack share pool list --detail</pre>
</div>
</div>
</li>
<li>
<p>Check existing workloads:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack share list
$ openstack share snapshot list</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="decommissioning-RHOSP-standalone-Ceph-NFS-service_adopting-shared-file-systems">4.14.3. Decommissioning the Red&#160;Hat OpenStack Platform standalone Ceph NFS service</h4>
<div class="paragraph _abstract">
<p>If your deployment uses CephFS through NFS, you must decommission the Red&#160;Hat OpenStack Platform(RHOSP) standalone NFS service. Since future software upgrades do not support the previous NFS service, ensure that the decommissioning period is short.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You identified the new export locations for your existing shares by querying the Shared File Systems API.</p>
</li>
<li>
<p>You unmounted and remounted the shared file systems on each client to stop using the previous NFS server.</p>
</li>
<li>
<p>If you are consuming the Shared File Systems service shares with the Shared File Systems service CSI plugin for Red Hat OpenShift Container Platform (RHOCP), you migrated the shares by scaling down the application pods and scaling them back up.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Clients that are creating new workloads cannot use share exports through the previous NFS service. The Shared File Systems service no longer communicates with the previous NFS service, and cannot apply or alter export rules on the previous NFS service.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Remove the <code>cephfs_ganesha_server_ip</code> option from the <code>manila-share</code> service configuration:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This restarts the <code>manila-share</code> process and removes the export locations that applied to the previous NFS service from all the shares.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; __EOF__ &gt; ~/manila.patch
spec:
  manila:
    enabled: true
    apiOverride:
      route: {}
    template:
      manilaShares:
        cephfs:
          replicas: 1
          customServiceConfig: |
            [DEFAULT]
            enabled_share_backends = cephfs
            host = hostgroup
            [cephfs]
            driver_handles_share_servers=False
            share_backend_name=cephfs
            share_driver=manila.share.drivers.cephfs.driver.CephFSDriver
            cephfs_conf_path=/etc/ceph/ceph.conf
            cephfs_auth_id=openstack
            cephfs_cluster_name=ceph
            cephfs_protocol_helper_type=NFS
            cephfs_nfs_cluster_id=cephfs
          networkAttachments:
              - storage
__EOF__</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> custom resource:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=~/&lt;manila.patch&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;manila.patch&gt;</code> with the name of your patch file.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Clean up the standalone <code>ceph-nfs</code> service from the RHOSP control plane nodes by disabling and deleting the Pacemaker resources associated with the service:</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
You can defer this step until after RHOSO 18.0 is operational. During this time, you cannot decommission the Controller nodes.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs resource disable ceph-nfs
$ sudo pcs resource disable ip-&lt;VIP&gt;
$ sudo pcs resource unmanage ceph-nfs
$ sudo pcs resource unmanage ip-&lt;VIP&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;VIP&gt;</code> with the IP address assigned to the <code>ceph-nfs</code> service in your environment.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-orchestration-service_hsm-integration">4.15. Adopting the Orchestration service</h3>
<div class="paragraph _abstract">
<p>To adopt the Orchestration service (heat), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR), where the Orchestration service
is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment.</p>
</div>
<div class="paragraph">
<p>After you complete the adoption process, you have CRs for <code>Heat</code>, <code>HeatAPI</code>, <code>HeatEngine</code>, and <code>HeatCFNAPI</code>, and endpoints within the Identity service (keystone) to facilitate these services.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The source director environment is running.</p>
</li>
<li>
<p>The target Red Hat OpenShift Container Platform (RHOCP) environment is running.</p>
</li>
<li>
<p>You adopted MariaDB and the Identity service.</p>
</li>
<li>
<p>If your existing Orchestration service stacks contain resources from other services such as Networking service (neutron), Compute service (nova), Object Storage service (swift), and so on, adopt those sevices before adopting the Orchestration service.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Retrieve the existing <code>auth_encryption_key</code> and <code>service</code> passwords. You use these passwords to patch the <code>osp-secret</code>. In the following example, the <code>auth_encryption_key</code> is used as <code>HeatAuthEncryptionKey</code> and the <code>service</code> password is used as <code>HeatPassword</code>:</p>
<div class="listingblock">
<div class="content">
<pre>[stack@rhosp17 ~]$ grep -E 'HeatPassword|HeatAuth|HeatStackDomainAdmin' ~/overcloud-deploy/overcloud/overcloud-passwords.yaml
  HeatAuthEncryptionKey: Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2
  HeatPassword: dU2N0Vr2bdelYH7eQonAwPfI3
  HeatStackDomainAdminPassword: dU2N0Vr2bdelYH7eQonAwPfI3</pre>
</div>
</div>
</li>
<li>
<p>Log in to a Controller node and verify the <code>auth_encryption_key</code> value in use:</p>
<div class="listingblock">
<div class="content">
<pre>[stack@rhosp17 ~]$ ansible -i overcloud-deploy/overcloud/config-download/overcloud/tripleo-ansible-inventory.yaml overcloud-controller-0 -m shell -a "grep auth_encryption_key /var/lib/config-data/puppet-generated/heat/etc/heat/heat.conf | grep -Ev '^#|^$'" -b
overcloud-controller-0 | CHANGED | rc=0 &gt;&gt;
auth_encryption_key=Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2</pre>
</div>
</div>
</li>
<li>
<p>Encode the password to Base64 format:</p>
<div class="listingblock">
<div class="content">
<pre>$ echo Q60Hj8PqbrDNu2dDCbyIQE2dibpQUPg2 | base64
UTYwSGo4UHFickROdTJkRENieUlRRTJkaWJwUVVQZzIK</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>osp-secret</code> to update the <code>HeatAuthEncryptionKey</code> and <code>HeatPassword</code> parameters. These values must match the values in the director Orchestration service configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch secret osp-secret --type='json' -p='[{"op" : "replace" ,"path" : "/data/HeatAuthEncryptionKey" ,"value" : "UTYwSGo4UHFickROdTJkRENieUlRRTJkaWJwUVVQZzIK"}]'
secret/osp-secret patched</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the Orchestration service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  heat:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: heat
      secret: osp-secret
      memcachedInstance: memcached
      passwordSelectors:
        authEncryptionKey: HeatAuthEncryptionKey
        service: HeatPassword
        stackDomainAdminPassword: HeatStackDomainAdminPassword
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Ensure that the statuses of all the CRs are <code>Setup complete</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get Heat,HeatAPI,HeatEngine,HeatCFNAPI
NAME                           STATUS   MESSAGE
heat.heat.openstack.org/heat   True     Setup complete

NAME                                  STATUS   MESSAGE
heatapi.heat.openstack.org/heat-api   True     Setup complete

NAME                                        STATUS   MESSAGE
heatengine.heat.openstack.org/heat-engine   True     Setup complete

NAME                                        STATUS   MESSAGE
heatcfnapi.heat.openstack.org/heat-cfnapi   True     Setup complete</pre>
</div>
</div>
</li>
<li>
<p>Check that the Orchestration service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it openstackclient -- openstack service list -c Name -c Type
+------------+----------------+
| Name       | Type           |
+------------+----------------+
| heat       | orchestration  |
| glance     | image          |
| heat-cfn   | cloudformation |
| ceilometer | Ceilometer     |
| keystone   | identity       |
| placement  | placement      |
| cinderv3   | volumev3       |
| nova       | compute        |
| neutron    | network        |
+------------+----------------+</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it openstackclient -- openstack endpoint list --service=heat -f yaml
- Enabled: true
  ID: 1da7df5b25b94d1cae85e3ad736b25a5
  Interface: public
  Region: regionOne
  Service Name: heat
  Service Type: orchestration
  URL: http://heat-api-public-openstack-operators.apps.okd.bne-shift.net/v1/%(tenant_id)s
- Enabled: true
  ID: 414dd03d8e9d462988113ea0e3a330b0
  Interface: internal
  Region: regionOne
  Service Name: heat
  Service Type: orchestration
  URL: http://heat-api-internal.openstack-operators.svc:8004/v1/%(tenant_id)s</pre>
</div>
</div>
</li>
<li>
<p>Check that the Orchestration service engine services are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it openstackclient -- openstack orchestration service list -f yaml
- Binary: heat-engine
  Engine ID: b16ad899-815a-4b0c-9f2e-e6d9c74aa200
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:01.000000'
- Binary: heat-engine
  Engine ID: 887ed392-0799-4310-b95c-ac2d3e6f965f
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:00.000000'
- Binary: heat-engine
  Engine ID: 26ed9668-b3f2-48aa-92e8-2862252485ea
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:00.000000'
- Binary: heat-engine
  Engine ID: 1011943b-9fea-4f53-b543-d841297245fd
  Host: heat-engine-6d47856868-p7pzz
  Hostname: heat-engine-6d47856868-p7pzz
  Status: up
  Topic: engine
  Updated At: '2023-10-11T21:48:01.000000'</pre>
</div>
</div>
</li>
<li>
<p>Verify that you can see your Orchestration service stacks:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack stack list -f yaml
- Creation Time: '2023-10-11T22:03:20Z'
  ID: 20f95925-7443-49cb-9561-a1ab736749ba
  Project: 4eacd0d1cab04427bc315805c28e66c9
  Stack Name: test-networks
  Stack Status: CREATE_COMPLETE
  Updated Time: null</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-the-loadbalancer-service_hsm-integration">4.16. Adopting the Load-balancing service</h3>
<div class="paragraph _abstract">
<p>To adopt the Load-balancing service (octavia), you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Load-balancing service is disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) environment. After completing the data plane adoption, you must trigger a failover of existing load balancers to upgrade their amphora virtual machines to use the new image and to establish connectivity with the new control plane.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Migrate the server certificate authority (CA) passphrase from the previous deployment:</p>
<div class="listingblock">
<div class="content">
<pre>SERVER_CA_PASSPHRASE=$($CONTROLLER1_SSH grep ^ca_private_key_passphrase /var/lib/config-data/puppet-generated/octavia/etc/octavia/octavia.conf)

oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: octavia-ca-passphrase
type: Opaque
data:
  server-ca-passphrase: $(echo -n $SERVER_CA_PASSPHRASE | base64 -w0)
EOF</pre>
</div>
</div>
</li>
<li>
<p>To isolate the management network, add the network interface for the VLAN base interface:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get --no-headers nncp | cut -f 1 -d ' ' | grep -v nncp-dns | while read; do

interfaces=$(oc get nncp $REPLY -o jsonpath="{.spec.desiredState.interfaces[*].name}")

(echo $interfaces | grep -w -q "octbr\|enp6s0.24") || \
        oc patch nncp $REPLY --type json --patch '
[{
    "op": "add",
    "path": "/spec/desiredState/interfaces/-",
    "value": {
      "description": "Octavia VLAN host interface",
      "name": "enp6s0.24",
      "state": "up",
      "type": "vlan",
      "vlan": {
        "base-iface": "&lt;enp6s0&gt;",
        "id": 24
        }
    }
},
{
    "op": "add",
    "path": "/spec/desiredState/interfaces/-",
    "value": {
      "description": "Octavia Bridge",
      "mtu": &lt;mtu&gt;,
      "state": "up",
      "type": "linux-bridge",
      "name": "octbr",
      "bridge": {
        "options": { "stp": { "enabled": "false" } },
        "port": [ { "name": "enp6s0.24" } ]
        }
    }
}]'

done</pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;enp6s0&gt;</dt>
<dd>
<p>Specifies the name of the network interface in your RHOCP setup.</p>
</dd>
<dt class="hdlist1">&lt;mtu&gt;</dt>
<dd>
<p>Specifies the <code>mtu</code> value in your environment.</p>
</dd>
</dl>
</div>
</li>
<li>
<p>To connect pods that manage load balancer virtual machines (amphorae) and the OpenvSwitch pods the OVN operator manages, configure the Load-balancing service network attachment definition:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat  octavia-nad.yaml &lt;&lt; EOF_CAT
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  labels:
    osp/net: octavia
  name: octavia
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "octavia",
      "type": "bridge",
      "bridge": "octbr",
      "ipam": {
        "type": "whereabouts",
        "range": "172.23.0.0/24",
        "range_start": "172.23.0.30",
        "range_end": "172.23.0.70",
        "routes": [
           {
             "dst": "172.24.0.0/16",
             "gw" : "172.23.0.150"
           }
         ]
      }
    }
EOF_CAT</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>NetworkAttachmentDefinition</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f octavia-nad.yaml</pre>
</div>
</div>
</li>
<li>
<p>Enable the Load-balancing service in RHOCP:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  ovn:
    template:
      ovnController:
        networkAttachment: tenant
        nicMappings:
          octavia: octbr
  octavia:
    enabled: true
    template:
      amphoraImageContainerImage: quay.io/gthiemonge/octavia-amphora-image
      octaviaHousekeeping:
        networkAttachments:
          - octavia
      octaviaHealthManager:
        networkAttachments:
          - octavia
      octaviaWorker:
        networkAttachments:
          - octavia
'</pre>
</div>
</div>
</li>
<li>
<p>Wait for the Load-balancing service control plane services CRs to be ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=600s octavia.octavia.openstack.org/octavia</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the Load-balancing service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"
$ openstack service list | grep load-balancer
| bd078ca6f90c4b86a48801f45eb6f0d7 | octavia   | load-balancer |
$ openstack endpoint list --service load-balancer
+----------------------------------+-----------+--------------+---------------+---------+-----------+---------------------------------------------------+
| ID                               | Region    | Service Name | Service Type  | Enabled | Interface | URL                                               |
+----------------------------------+-----------+--------------+---------------+---------+-----------+---------------------------------------------------+
| f1ae7756b6164baf9cb82a1a670067a2 | regionOne | octavia      | load-balancer | True    | public    | https://octavia-public-openstack.apps-crc.testing |
| ff3222b4621843669e89843395213049 | regionOne | octavia      | load-balancer | True    | internal  | http://octavia-internal.openstack.svc:9876        |
+----------------------------------+-----------+--------------+---------------+---------+-----------+---------------------------------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>After you complete the data plane adoption, you must upgrade existing load balancers and remove old resources. For more information, see <a href="#performing-post-adoption-cleanup-of-load-balancers_data-plane">Post-adoption tasks for the Load-balancing service</a>.</p>
</div>
</div>
<div class="sect2">
<h3 id="adopting-telemetry-services_hsm-integration">4.17. Adopting Telemetry services</h3>
<div class="paragraph _abstract">
<p>To adopt Telemetry services, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) that has Telemetry services disabled to start the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 environment.</p>
</div>
<div class="paragraph">
<p>If you adopt Telemetry services, the observability solution that is used in the RHOSP 17.1 environment, Service Telemetry Framework, is removed from the cluster. The new solution is deployed in the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) environment, allowing for metrics, and optionally logs, to be retrieved and stored in the new back ends.</p>
</div>
<div class="paragraph">
<p>You cannot automatically migrate old data because different back ends are used. Metrics and logs are considered short-lived data and are not intended to be migrated to the RHOSO environment. For information about adopting legacy autoscaling stack templates to the RHOSO environment, see <a href="#adopting-autoscaling_adopt-control-plane">Adopting Autoscaling services</a>.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The director environment is running (the source cloud).</p>
</li>
<li>
<p>The Single Node OpenShift or OpenShift Local is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>Previous adoption steps are completed.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy <code>cluster-observability-operator</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create -f - &lt;&lt;EOF
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: cluster-observability-operator
  namespace: openshift-operators
spec:
  channel: stable
  installPlanApproval: Automatic
  name: cluster-observability-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</pre>
</div>
</div>
</li>
<li>
<p>Wait for the installation to succeed:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for jsonpath="{.status.phase}"=Succeeded csv --namespace=openshift-operators -l operators.coreos.com/cluster-observability-operator.openshift-operators</pre>
</div>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy Ceilometer services:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    enabled: true
    template:
      ceilometer:
        passwordSelector:
          ceilometerService: CeilometerPassword
        enabled: true
        secret: osp-secret
        serviceUser: ceilometer
'</pre>
</div>
</div>
</li>
<li>
<p>Enable the metrics storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    template:
      metricStorage:
        enabled: true
        monitoringStack:
          alertingEnabled: true
          scrapeInterval: 30s
          storage:
            strategy: persistent
            retention: 24h
            persistent:
              pvcStorageRequest: 20G
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the <code>alertmanager</code> and <code>prometheus</code> pods are available:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pods -l alertmanager=metric-storage
NAME                            READY   STATUS    RESTARTS   AGE
alertmanager-metric-storage-0   2/2     Running   0          46s
alertmanager-metric-storage-1   2/2     Running   0          46s

$ oc get pods -l prometheus=metric-storage
NAME                          READY   STATUS    RESTARTS   AGE
prometheus-metric-storage-0   3/3     Running   0          46s</pre>
</div>
</div>
</li>
<li>
<p>Inspect the resulting Ceilometer pods:</p>
<div class="listingblock">
<div class="content">
<pre>CEILOMETETR_POD=`oc get pods -l service=ceilometer | tail -n 1 | cut -f 1 -d' '`
oc exec -t $CEILOMETETR_POD -c ceilometer-central-agent -- cat /etc/ceilometer/ceilometer.conf</pre>
</div>
</div>
</li>
<li>
<p>Inspect enabled pollsters:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get secret ceilometer-config-data -o jsonpath="{.data['polling\.yaml\.j2']}"  | base64 -d</pre>
</div>
</div>
</li>
<li>
<p>Optional: Override default pollsters according to the requirements of your environment:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane controlplane --type=merge --patch '
spec:
  telemetry:
    template:
      ceilometer:
          defaultConfigOverwrite:
            polling.yaml.j2: |
              ---
              sources:
                - name: pollsters
                  interval: 100
                  meters:
                    - volume.*
                    - image.size
          enabled: true
          secret: osp-secret
'</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Next steps</div>
<ol class="arabic">
<li>
<p>Optional: Patch the <code>OpenStackControlPlane</code> CR to include <code>logging</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    template:
      logging:
        enabled: false
        ipaddr: 172.17.0.80
        port: 10514
        cloNamespace: openshift-logging
'</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="adopting-autoscaling_hsm-integration">4.18. Adopting autoscaling services</h3>
<div class="paragraph _abstract">
<p>To adopt services that enable autoscaling, you patch an existing <code>OpenStackControlPlane</code> custom resource (CR) where the Alarming services (aodh) are disabled. The patch starts the service with the configuration parameters that are provided by the Red&#160;Hat OpenStack Platform environment.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The source director environment is running.</p>
</li>
<li>
<p>A Single Node OpenShift or OpenShift Local is running in the Red Hat OpenShift Container Platform (RHOCP) cluster.</p>
</li>
<li>
<p>You have adopted the following services:</p>
<div class="ulist">
<ul>
<li>
<p>MariaDB</p>
</li>
<li>
<p>Identity service (keystone)</p>
</li>
<li>
<p>Orchestration service (heat)</p>
</li>
<li>
<p>Telemetry service</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to deploy the autoscaling services:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  telemetry:
    enabled: true
    template:
      autoscaling:
        enabled: true
        aodh:
          passwordSelector:
            aodhService: AodhPassword
          databaseAccount: aodh
          databaseInstance: openstack
          secret: osp-secret
          serviceUser: aodh
        heatInstance: heat
'</pre>
</div>
</div>
</li>
<li>
<p>Inspect the aodh pods:</p>
<div class="listingblock">
<div class="content">
<pre>$ AODH_POD=`oc get pods -l service=aodh | tail -n 1 | cut -f 1 -d' '`
$ oc exec -t $AODH_POD -c aodh-api -- cat /etc/aodh/aodh.conf</pre>
</div>
</div>
</li>
<li>
<p>Check whether the aodh API service is registered in the Identity service:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack endpoint list | grep aodh
| d05d120153cd4f9b8310ac396b572926 | regionOne | aodh  | alarming  | True    | internal  | http://aodh-internal.openstack.svc:8042  |
| d6daee0183494d7a9a5faee681c79046 | regionOne | aodh  | alarming  | True    | public    | http://aodh-public.openstack.svc:8042    |</pre>
</div>
</div>
</li>
<li>
<p>Optional: Create aodh alarms with the <code>PrometheusAlarm</code> alarm type:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must use the <code>PrometheusAlarm</code> alarm type instead of <code>GnocchiAggregationByResourcesAlarm</code>.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ openstack alarm create --name high_cpu_alarm \
--type prometheus \
--query "(rate(ceilometer_cpu{resource_name=~'cirros'})) * 100" \
--alarm-action 'log://' \
--granularity 15 \
--evaluation-periods 3 \
--comparison-operator gt \
--threshold 7000000000</pre>
</div>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Verify that the alarm is enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack alarm list
+--------------------------------------+------------+------------------+-------------------+----------+
| alarm_id                             | type       | name             | state  | severity | enabled  |
+--------------------------------------+------------+------------------+-------------------+----------+
| 209dc2e9-f9d6-40e5-aecc-e767ce50e9c0 | prometheus | prometheus_alarm |   ok   |    low   |   True   |
+--------------------------------------+------------+------------------+-------------------+----------+</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="pulling-configuration-from-tripleo-deployment_hsm-integration">4.19. Pulling the configuration from a director deployment</h3>
<div class="paragraph _abstract">
<p>Before you start the data plane adoption workflow, back up the configuration from the Red&#160;Hat OpenStack Platform (RHOSP) services and director. You can then use the files during the configuration of the adopted services to ensure that nothing is missed or misconfigured.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The os-diff tool is installed and configured. For more information, see
<a href="#comparing-configuration-files-between-deployments_configuring-network">Comparing configuration files between deployments</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Update your ssh parameters according to your environment in the <code>os-diff.cfg</code>. Os-diff uses the ssh parameters to connect to your director node, and then query and download the configuration files:</p>
<div class="listingblock">
<div class="content">
<pre>ssh_cmd=ssh -F ssh.config standalone
container_engine=podman
connection=ssh
remote_config_path=/tmp/tripleo</pre>
</div>
</div>
<div class="paragraph">
<p>Ensure that the ssh command you provide in <code>ssh_cmd</code> parameter is correct and includes key authentication.</p>
</div>
</li>
<li>
<p>Enable the services that you want to include in the <code>/etc/os-diff/config.yaml</code> file, and disable the services that you want to exclude from the file. Ensure that you have the correct permissions to edit the file:</p>
<div class="listingblock">
<div class="content">
<pre>$ chown ospng:ospng /etc/os-diff/config.yaml</pre>
</div>
</div>
<div class="paragraph">
<p>The following example enables the default Identity service (keystone) to be included in the <code>/etc/os-diff/config.yaml</code> file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># service name and file location
services:
  # Service name
  keystone:
    # Bool to enable/disable a service (not implemented yet)
    enable: true
    # Pod name, in both OCP and podman context.
    # It could be strict match or will only just grep the podman_name
    # and work with all the pods which matched with pod_name.
    # To enable/disable use strict_pod_name_match: true/false
    podman_name: keystone
    pod_name: keystone
    container_name: keystone-api
    # pod options
    # strict match for getting pod id in TripleO and podman context
    strict_pod_name_match: false
    # Path of the config files you want to analyze.
    # It could be whatever path you want:
    # /etc/&lt;service_name&gt; or /etc or /usr/share/&lt;something&gt; or even /
    # @TODO: need to implement loop over path to support multiple paths such as:
    # - /etc
    # - /usr/share
    path:
      - /etc/
      - /etc/keystone
      - /etc/keystone/keystone.conf
      - /etc/keystone/logging.conf</code></pre>
</div>
</div>
<div class="paragraph">
<p>Repeat this step for each RHOSP service that you want to disable or enable.</p>
</div>
</li>
<li>
<p>If you use non-containerized services, such as the <code>ovs-external-ids</code>, pull the configuration or the command output. For example:</p>
<div class="listingblock">
<div class="content">
<pre>services:
  ovs_external_ids:
    hosts:
      - standalone
    service_command: "ovs-vsctl list Open_vSwitch . | grep external_ids | awk -F ': ' '{ print $2; }'"
    cat_output: true
    path:
      - ovs_external_ids.json
    config_mapping:
      ovn-bridge-mappings: edpm_ovn_bridge_mappings
      ovn-bridge: edpm_ovn_bridge
      ovn-encap-type: edpm_ovn_encap_type
      ovn-monitor-all: ovn_monitor_all
      ovn-remote-probe-interval: edpm_ovn_remote_probe_interval
      ovn-ofctrl-wait-before-clear: edpm_ovn_ofctrl_wait_before_clear</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You must correctly configure an SSH configuration file or equivalent for non-standard services, such as OVS. The <code>ovs_external_ids</code> service does not run in a container, and the OVS data is stored on each host of your cloud, for example, <code>controller_1/controller_2/</code>, and so on.
</td>
</tr>
</table>
</div>
<div class="ulist">
<ul>
<li>
<p><code>hosts</code> specifies the list of hosts, for example, <code>compute-1</code>, <code>compute-2</code>.</p>
</li>
<li>
<p><code>service_command: "ovs-vsctl list Open_vSwitch . | grep external_ids | awk -F ': ' '{ print $2; }'"</code> runs against the hosts.</p>
</li>
<li>
<p><code>cat_output: true</code> provides os-diff with the output of the command and stores the output in a file that is specified by the key path.</p>
</li>
<li>
<p><code>config_mapping</code> provides a mapping between, in this example, the data plane custom resource definition and the <code>ovs-vsctl</code> output.</p>
</li>
<li>
<p><code>ovn-bridge-mappings: edpm_ovn_bridge_mappings</code> must be a list of strings, for example, <code>["datacentre:br-ex"]</code>.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Compare the values:</p>
<div class="listingblock">
<div class="content">
<pre>$ os-diff diff ovs_external_ids.json edpm.crd --crd --service ovs_external_ids</pre>
</div>
</div>
<div class="paragraph">
<p>For example, to check the <code>/etc/yum.conf</code> on every host, you must put the following statement in the <code>config.yaml</code> file. The following example uses a file called <code>yum_config</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>services:
  yum_config:
    hosts:
      - undercloud
      - controller_1
      - compute_1
      - compute_2
    service_command: "cat /etc/yum.conf"
    cat_output: true
    path:
      - yum.conf</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Pull the configuration:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The following command pulls all the configuration files that are included in the <code>/etc/os-diff/config.yaml</code> file. You can configure os-diff to update this file automatically according to your running environment by using the <code>--update</code> or <code>--update-only</code> option. These options set the podman information into the <code>config.yaml</code> for all running containers. The podman information can be useful later, when all the Red&#160;Hat OpenStack Platform services are turned off.</p>
</div>
<div class="paragraph">
<p>Note that when the <code>config.yaml</code> file is populated automatically you must provide the configuration paths manually for each service.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># will only update the /etc/os-diff/config.yaml
os-diff pull --update-only</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># will update the /etc/os-diff/config.yaml and pull configuration
os-diff pull --update</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># will update the /etc/os-diff/config.yaml and pull configuration
os-diff pull</code></pre>
</div>
</div>
<div class="paragraph">
<p>The configuration is pulled and stored by default in the following directory:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>/tmp/tripleo/</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>Verify that you have a directory for each service configuration in your local path:</p>
<div class="listingblock">
<div class="content">
<pre>   tmp/
     tripleo/
       glance/
       keystone/</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="rolling-back-control-plane-adoption_hsm-integration">4.20. Rolling back the control plane adoption</h3>
<div class="paragraph _abstract">
<p>If you encountered a problem and are unable to complete the adoption of the Red&#160;Hat OpenStack Platform (RHOSP) control plane services, you can roll back the control plane adoption.</p>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Do not attempt the rollback if you altered the data plane nodes in any way.
You can only roll back the control plane adoption if you altered the control plane.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>During the control plane adoption, services on the RHOSP control plane are stopped but not removed. The databases on the RHOSP control plane are not edited during the adoption procedure. The Red&#160;Hat OpenStack Services on OpenShift (RHOSO) control plane receives a copy of the original control plane databases. The rollback procedure assumes that the data plane has not yet been modified by the adoption procedure, and it is still connected to the RHOSP control plane.</p>
</div>
<div class="paragraph">
<p>The rollback procedure consists of the following steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Restoring the functionality of the RHOSP control plane.</p>
</li>
<li>
<p>Removing the partially or fully deployed RHOSO control plane.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>To restore the source cloud to a working state, start the RHOSP
control plane services that you previously stopped during the adoption
procedure:</p>
<div class="listingblock">
<div class="content">
<pre>ServicesToStart=("tripleo_horizon.service"
                 "tripleo_keystone.service"
                 "tripleo_barbican_api.service"
                 "tripleo_barbican_worker.service"
                 "tripleo_barbican_keystone_listener.service"
                 "tripleo_cinder_api.service"
                 "tripleo_cinder_api_cron.service"
                 "tripleo_cinder_scheduler.service"
                 "tripleo_cinder_volume.service"
                 "tripleo_cinder_backup.service"
                 "tripleo_designate_api.service"
                 "tripleo_designate_backend_bind9.service"
                 "tripleo_designate_central.service"
                 "tripleo_designate_mdns.service"
                 "tripleo_designate_producer.service"
                 "tripleo_designate_worker.service"
                 "tripleo_glance_api.service"
                 "tripleo_manila_api.service"
                 "tripleo_manila_api_cron.service"
                 "tripleo_manila_scheduler.service"
                 "tripleo_neutron_api.service"
                 "tripleo_placement_api.service"
                 "tripleo_nova_api_cron.service"
                 "tripleo_nova_api.service"
                 "tripleo_nova_conductor.service"
                 "tripleo_nova_metadata.service"
                 "tripleo_nova_scheduler.service"
                 "tripleo_nova_vnc_proxy.service"
                 "tripleo_aodh_api.service"
                 "tripleo_aodh_api_cron.service"
                 "tripleo_aodh_evaluator.service"
                 "tripleo_aodh_listener.service"
                 "tripleo_aodh_notifier.service"
                 "tripleo_ceilometer_agent_central.service"
                 "tripleo_ceilometer_agent_compute.service"
                 "tripleo_ceilometer_agent_ipmi.service"
                 "tripleo_ceilometer_agent_notification.service"
                 "tripleo_ovn_cluster_north_db_server.service"
                 "tripleo_ovn_cluster_south_db_server.service"
                 "tripleo_ovn_cluster_northd.service"
                 "tripleo_octavia_api.service"
                 "tripleo_octavia_health_manager.service"
                 "tripleo_octavia_rsyslog.service"
                 "tripleo_octavia_driver_agent.service"
                 "tripleo_octavia_housekeeping.service"
                 "tripleo_octavia_worker.service"
                 "tripleo_unbound.service")

PacemakerResourcesToStart=("galera-bundle"
                           "haproxy-bundle"
                           "rabbitmq-bundle"
                           "openstack-cinder-volume"
                           "openstack-cinder-backup"
                           "openstack-manila-share")

echo "Starting systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &amp;&gt; /dev/null; then
                echo "Starting the $service in controller $i"
                ${!SSH_CMD} sudo systemctl start $service
            fi
        fi
    done
done

echo "Checking systemd OpenStack services"
for service in ${ServicesToStart[*]}; do
    for i in {1..3}; do
        SSH_CMD=CONTROLLER${i}_SSH
        if [ ! -z "${!SSH_CMD}" ]; then
            if ${!SSH_CMD} sudo systemctl is-enabled $service &amp;&gt; /dev/null; then
                if ! ${!SSH_CMD} systemctl show $service | grep ActiveState=active &gt;/dev/null; then
                    echo "ERROR: Service $service is not running on controller $i"
                else
                    echo "OK: Service $service is running in controller $i"
                fi
            fi
        fi
    done
done

echo "Starting pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStart[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                echo "Starting $resource"
                ${!SSH_CMD} sudo pcs resource enable $resource
            else
                echo "Service $resource not present"
            fi
        done
        break
    fi
done

echo "Checking pacemaker OpenStack services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource &amp;&gt;/dev/null; then
                if ${!SSH_CMD} sudo pcs resource status $resource | grep Started &gt;/dev/null; then
                    echo "OK: Service $resource is started"
                else
                    echo "ERROR: Service $resource is stopped"
                fi
            fi
        done
        break
    fi
done</pre>
</div>
</div>
</li>
<li>
<p>If the Ceph NFS service is running on the deployment as a Shared File Systems service (manila) back end, you must restore the Pacemaker order and colocation constraints for the <code>openstack-manila-share</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo pcs constraint order start ceph-nfs then openstack-manila-share kind=Optional id=order-ceph-nfs-openstack-manila-share-Optional
$ sudo pcs constraint colocation add openstack-manila-share with ceph-nfs score=INFINITY id=colocation-openstack-manila-share-ceph-nfs-INFINITY</pre>
</div>
</div>
</li>
<li>
<p>Verify that the source cloud is operational again, for example, you
can run <code>openstack</code> CLI commands such as <code>openstack server list</code>, or check that you can access the Dashboard service (horizon).</p>
</li>
<li>
<p>Remove the partially or fully deployed control plane so that you can attempt the adoption again later:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc delete --ignore-not-found=true --wait=false openstackcontrolplane/openstack
$ oc patch openstackcontrolplane openstack --type=merge --patch '
&gt; metadata:
&gt;   finalizers: []
&gt; ' || true
&gt;
&gt;while oc get pod | grep rabbitmq-server-0; do
&gt;    sleep 2
&gt;done
&gt;while oc get pod | grep openstack-galera-0; do
&gt;    sleep 2
&gt;done

$ oc delete --ignore-not-found=true --wait=false pod mariadb-copy-data
$ oc delete --ignore-not-found=true --wait=false pvc mariadb-data
$ oc delete --ignore-not-found=true --wait=false pod ovn-copy-data
$ oc delete --ignore-not-found=true secret osp-secret</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>After you restore the RHOSP control plane services, their internal
state might have changed. Before you retry the adoption procedure, verify that all the control plane resources are removed and that there are no leftovers which could affect the following adoption procedure attempt. You must not use previously created copies of the database contents in another adoption attempt. You must make a new copy of the latest state of the original source database contents. For more information about making new copies of the database, see <a href="#migrating-databases-to-the-control-plane_configuring-network">Migrating databases to the control plane</a>.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="adopting-data-plane_hsm-integration">5. Adopting the data plane</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Adopting the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane involves the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop any remaining services on the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 control plane.</p>
</li>
<li>
<p>Deploy the required custom resources.</p>
</li>
<li>
<p>Perform a fast-forward upgrade on Compute services from RHOSP 17.1 to RHOSO 18.0.</p>
</li>
<li>
<p>Adopt Networker services to the RHOSO data plane.</p>
</li>
</ol>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
After the RHOSO control plane manages the newly deployed data plane, you must not re-enable services on the RHOSP 17.1 control plane and data plane. If you re-enable services, workloads are managed by two control planes or two data planes, resulting in data corruption, loss of control of existing workloads, inability to start new workloads, or other issues.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="stopping-infrastructure-management-and-compute-services_data-plane">5.1. Stopping infrastructure management and Compute services</h3>
<div class="paragraph _abstract">
<p>You must stop cloud database nodes and messaging nodes on the Red&#160;Hat OpenStack Platform 17.1 control plane. Do not stop nodes that are running the following roles:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Compute</p>
</li>
<li>
<p>Storage</p>
</li>
<li>
<p>Networker</p>
</li>
<li>
<p>Controller if running <code>OVN Controller Gateway agent</code> network agent</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The following procedure applies to a standalone director deployment. You must stop the Pacemaker services on your host so that you can install libvirt packages when the Compute roles are adopted as data plane nodes. Modular libvirt daemons no longer run in podman containers on data plane nodes.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the shell variables. Replace the following example values with values that apply to your environment:</p>
<div class="listingblock">
<div class="content">
<pre>CONTROLLER1_SSH="ssh -i &lt;path_to_SSH_key&gt; root@&lt;controller-1 IP&gt;"
# ...
# ...
EDPM_PRIVATEKEY_PATH="&lt;path_to_SSH_key&gt;"</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>CONTROLLER&lt;X&gt;_SSH</code> defines the SSH connection details for all Controller nodes, including cell Controller nodes, of the source director cloud.</p>
</li>
<li>
<p><code>&lt;path_to_SSH_key&gt;</code> defines the path to your SSH key.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>Stop the Pacemaker services:</p>
<div class="listingblock">
<div class="content">
<pre>PacemakerResourcesToStop=(
                "galera-bundle"
                "haproxy-bundle"
                "rabbitmq-bundle")

echo "Stopping pacemaker services"
for i in {1..3}; do
    SSH_CMD=CONTROLLER${i}_SSH
    if [ ! -z "${!SSH_CMD}" ]; then
        echo "Using controller $i to run pacemaker commands"
        for resource in ${PacemakerResourcesToStop[*]}; do
            if ${!SSH_CMD} sudo pcs resource config $resource; then
                ${!SSH_CMD} sudo pcs resource disable $resource
            fi
        done
        break
    fi
done</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="adopting-compute-services-to-the-data-plane_data-plane">5.2. Adopting Compute services to the RHOSO data plane</h3>
<div class="paragraph _abstract">
<p>Adopt your Compute (nova) services to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have stopped the remaining control plane nodes, repositories, and packages on the Compute service (nova) hosts. For more information, see <a href="#stopping-infrastructure-management-and-compute-services_data-plane">Stopping infrastructure management and Compute services</a>.</p>
</li>
<li>
<p>You have configured the Ceph back end for the <code>NovaLibvirt</code> service. For more information, see <a href="#configuring-a-ceph-backend_migrating-databases">Configuring a Ceph back end</a>.</p>
</li>
<li>
<p>You have configured IP Address Management (IPAM):</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      - end: 192.168.122.200
        start: 192.168.122.150
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: External
    dnsDomain: external.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 10.0.0.250
        start: 10.0.0.100
      cidr: 10.0.0.0/24
      gateway: 10.0.0.1
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: storagemgmt
    dnsDomain: storagemgmt.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.20.0.250
        start: 172.20.0.100
      cidr: 172.20.0.0/24
      vlan: 23
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
EOF</pre>
</div>
</div>
</li>
<li>
<p>If <code>neutron-sriov-nic-agent</code> is running on your Compute service nodes, ensure that the physical device mappings match the values that are defined in the <code>OpenStackDataPlaneNodeSet</code> custom resource (CR). For more information, see <a href="#pulling-configuration-from-tripleo-deployment_adopt-control-plane">Pulling the configuration from a director deployment</a>.</p>
</li>
<li>
<p>You have defined the shell variables to run the script that runs the upgrade:</p>
<div class="listingblock">
<div class="content">
<pre>$ CEPH_FSID=$(oc get secret ceph-conf-files -o json | jq -r <em>.data."ceph.conf"</em> | base64 -d | grep fsid | sed -e <em>s/fsid = //</em>)

$ alias openstack="oc exec -t openstackclient -- openstack"

$ <strong>DEFAULT_CELL_NAME="cell3"</strong>
$ RENAMED_CELLS="cell1 cell2 $DEFAULT_CELL_NAME"

$ declare -A COMPUTES_CELL1
$ <strong>export COMPUTES_CELL1=(</strong>
&gt; <strong>["standalone.localdomain"]="192.168.122.100"</strong>
&gt; # <strong>&lt;compute1&gt;</strong>
&gt; # <strong>&lt;compute2&gt;</strong>
&gt; # <strong>&lt;compute3&gt;</strong>
&gt;)
$ declare -A COMPUTES_CELL2
$ export COMPUTES_CELL2=(
&gt; # <strong>&lt;compute1&gt;</strong>
&gt;)
$ declare -A COMPUTES_CELL3
$<strong>export COMPUTES_CELL3=(</strong>
&gt; # <strong>&lt;compute1&gt;</strong>
&gt; # <strong>&lt;compute2&gt;</strong>
&gt;)

$ declare -A COMPUTES_API_CELL1
$<strong>export COMPUTES_API_CELL1=(</strong>
&gt; ["standalone.localdomain"]="172.17.0.100"
&gt; ["standalone2.localdomain"]="172.17.0.101"
&gt;)

$ NODESETS=""
$ for CELL in $(echo $RENAMED_CELLS); do
&gt; ref="COMPUTES_$(echo ${CELL}|tr <em>[:lower:]</em> <em>[:upper:]</em>)"
&gt; eval names=\${!${ref}[@]}
&gt; [ -z "$names" ] &amp;&amp; continue
&gt; NODESETS="<em>openstack-${CELL}</em>, $NODESETS"
&gt;done
$ NODESETS="[${NODESETS%,*}]"</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>DEFAULT_CELL_NAME="cell3"</code> defines the source cloud <code>default</code> cell that acquires a new <code>DEFAULT_CELL_NAME</code> on the destination cloud after adoption.
In a multi-cell adoption scenario, you can retain the original name, <code>default</code>, or create a new cell default name by providing the incremented index of the last cell in the source cloud. For example, if the incremented index of the last cell is <code>cell5</code>, the new cell default name is <code>cell6</code>.</p>
</li>
<li>
<p><code>export COMPUTES_CELL1=</code> For each cell, update the <code>&lt;["standalone.localdomain"]="x.x.x.x"&gt;</code> value and the <code>COMPUTES_CELL&lt;X&gt;</code> value with the names and IP addresses of the Compute service nodes that are connected to the <code>ctlplane</code> and <code>internalapi</code> networks. Do not specify a real FQDN defined for each network. Always use the same hostname for each connected network of a Compute node. Provide the IP addresses and the names of the hosts on the remaining networks of the source cloud as needed, or you can manually adjust the files that you generate in step 9 of this procedure.</p>
</li>
<li>
<p><code>&lt;compute1&gt;</code>, <code>&lt;compute2&gt;</code>, and <code>&lt;compute3&gt;</code> specifies the names of your Compute service nodes for each cell. Assign all Compute service nodes from the source cloud <code>cell1</code> cell into <code>COMPUTES_CELL1</code>, and so on.</p>
</li>
<li>
<p><code>export COMPUTES_CELL&lt;X&gt;=(</code> specifies all Compute service nodes that you assign from the source cloud <code>default</code> cell into <code>COMPUTES_CELL&lt;X&gt;</code> and <code>COMPUTES_API_CELL&lt;X&gt;</code>, where <code>&lt;X&gt;</code> is the <code>DEFAULT_CELL_NAME</code> environment variable value. In this example, the <code>DEFAULT_CELL_NAME</code> environment variable value equals <code>cell3</code>.</p>
</li>
<li>
<p><code>export COMPUTES_API_CELL1=(</code> For each cell, update the <code>&lt;["standalone.localdomain"]="192.168.122.100"&gt;</code> value and the <code>COMPUTES_API_CELL&lt;X&gt;</code> value with the names and IP addresses of the Compute service nodes that are connected to the <code>ctlplane</code> and <code>internalapi</code> networks. <code>["standalone.localdomain"]="192.168.122.100"</code> defines the custom DNS domain in the FQDN value of the nodes. This value is used in the data plane node set <code>spec.nodes.&lt;NODE NAME&gt;.hostName</code>. Do not specify a real FQDN defined for each network. Use the same hostname for each of its connected networks. Provide the IP addresses and the names of the hosts on the remaining networks of the source cloud as needed, or you can manually adjust the files that you generate in step 9 of this procedure.</p>
</li>
<li>
<p><code>NODESETS="'openstack-${CELL}', $NODESETS"</code> specifies the cells that contain Compute nodes. Cells that do not contain Compute nodes are omitted from this template because no node sets are created for the cells.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you deployed the source cloud with a <code>default</code> cell, and want to rename it during adoption, define the new name that you want to use, as shown in the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ DEFAULT_CELL_NAME="cell1"
$ RENAMED_CELLS="cell1"</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Do not set a value for the <code>CEPH_FSID</code> parameter if the local storage back end is configured by the Compute service for libvirt. The storage back end must match the source cloud storage back end. You cannot change the storage back end during adoption.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create an SSH authentication secret for the data plane nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
    name: dataplane-adoption-secret
data:
    ssh-privatekey: |
$(cat &lt;path_to_SSH_key&gt; | base64 | sed 's/^/        /')
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;path_to_SSH_key&gt;</code> with the path to your SSH key.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Generate an ssh key-pair <code>nova-migration-ssh-key</code> secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ cd "$(mktemp -d)"
$ ssh-keygen -f ./id -t ecdsa-sha2-nistp521 -N ''
$ oc get secret nova-migration-ssh-key || oc create secret generic nova-migration-ssh-key \
  --from-file=ssh-privatekey=id \
  --from-file=ssh-publickey=id.pub \
  --type kubernetes.io/ssh-auth
$ rm -f id*
$ cd -</pre>
</div>
</div>
</li>
<li>
<p>If TLS Everywhere is enabled, set <code>LIBVIRT_PASSWORD</code> to match the existing RHOSP deployment password:</p>
<div class="listingblock">
<div class="content">
<pre>declare -A TRIPLEO_PASSWORDS
TRIPLEO_PASSWORDS[default]="$HOME/overcloud-passwords.yaml"
LIBVIRT_PASSWORD=$(cat ${TRIPLEO_PASSWORDS[default]} | grep ' LibvirtTLSPassword:' | awk -F ': ' '{ print $2; }')
LIBVIRT_PASSWORD_BASE64=$(echo -n "$LIBVIRT_PASSWORD" | base64)</pre>
</div>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create libvirt-secret when TLS-e is enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Secret
metadata:
  name: libvirt-secret
type: Opaque
data:
  LibvirtPassword: ${LIBVIRT_PASSWORD_BASE64}
EOF</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create a configuration map to use for all cells to configure a local storage back end for libvirt:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-cells-global-config
data:
  99-nova-compute-cells-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>data</code> provides the configuration files for all the cells.</p>
</li>
<li>
<p><code>99-nova-compute-cells-workarounds.conf: |</code> specifies the index of the <code>&lt;*.conf&gt;</code> files. There is a requirement to index the <code>&lt;*.conf&gt;</code> files from <em>03</em> to <em>99</em>, based on precedence. A <code>&lt;99-*.conf&gt;</code> file takes the highest precedence, while indexes below <em>03</em> are reserved for internal use.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you adopt a live cloud, you might be required to carry over additional configurations for the default <code>nova</code> data plane services that are stored in the cell1 default <code>nova-extra-config</code> configuration map. Do not delete or overwrite the existing configuration in the <code>cell1</code> default <code>nova-extra-config</code> configuration map that is assigned to <code>nova</code>. Overwriting the configuration can break the data place services that rely on specific contents of the <code>nova-extra-config</code> configuration map.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Configure a Red Hat Ceph Storage back end for libvirt:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-cells-global-config
data:
  99-nova-compute-cells-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
  03-ceph-nova.conf: |
    [libvirt]
    images_type=rbd
    images_rbd_pool=vms
    images_rbd_ceph_conf=/etc/ceph/ceph.conf
    images_rbd_glance_store_name=default_backend
    images_rbd_glance_copy_poll_interval=15
    images_rbd_glance_copy_timeout=600
    rbd_user=openstack
    rbd_secret_uuid=$CEPH_FSID
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
For Red Hat Ceph Storage environments with multi-cell configurations, you must name configuration maps and Red&#160;Hat OpenStack Platform data plane services similar to the following examples: <code>nova-custom-ceph-cellX</code> and <code>nova-compute-extraconfig-cellX</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create the data plane services for Compute service cells to enable pre-upgrade workarounds, and to configure the Compute services for your chosen storage back end:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
 name: nova-$CELL
spec:
 dataSources:
   - secretRef:
       name: nova-$CELL-compute-config
   - secretRef:
       name: nova-migration-ssh-key
   - configMapRef:
       name: nova-cells-global-config
 playbook: osp.edpm.nova
 caCerts: combined-ca-bundle
 edpmServiceType: nova
 containerImageFields:
 - NovaComputeImage
 - EdpmIscsidImage
EOF
 done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.dataSources.secretRef</code> specifies an additional auto-generated <code>nova-cell&lt;X&gt;-metadata-neutron-config</code> secret to enable a local metadata service for cell&lt;X&gt;. You should also set
<code>spec.nova.template.cellTemplates.cell&lt;X&gt;.metadataServiceTemplate.enable</code> in the <code>OpenStackControlPlane/openstack</code> CR, as described in <a href="#adopting-the-compute-service_adopt-control-plane">Adopting the Compute service</a>. You can configure a single top-level metadata, or define the metadata per cell.</p>
</li>
<li>
<p><code>nova-$CELL-compute-config</code> specifies the secret that auto-generates for each <code>cell&lt;X&gt;</code>. You must append the <code>nova-cell&lt;X&gt;-compute-config</code> for each custom <code>OpenStackDataPlaneService</code> CR that is related to the Compute service.</p>
</li>
<li>
<p><code>nova-migration-ssh-key</code> specifies the secret that you must append for each custom <code>OpenStackDataPlaneService</code> CR that is related to the Compute service.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When creating your data plane services for Compute service cells, review the following considerations:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In this example, the same <code>nova-migration-ssh-key</code> key is shared across cells. However, you should use different keys for different cells.</p>
</li>
<li>
<p>For simple configuration overrides, you do not need a custom data plane service. However, to reconfigure the cell, <code>cell1</code>,
the safest option is to create a custom service and a dedicated configuration map for it.</p>
</li>
<li>
<p>The cell, <code>cell1</code>, is already managed with the default <code>OpenStackDataPlaneService</code> CR called <code>nova</code> and its <code>nova-extra-config</code> configuration map. Do not change the default data plane service <code>nova</code> definition. The changes are lost when the RHOSO operator is updated with OLM.</p>
</li>
<li>
<p>When a cell spans multiple node sets, give the custom <code>OpenStackDataPlaneService</code> resources a name that relates to the node set, for example, <code>nova-cell1-nfv</code> and <code>nova-cell1-enterprise</code>. The auto-generated configuration maps are then named <code>nova-cell1-nfv-extra-config</code> and <code>nova-cell1-enterprise-extra-config</code>.</p>
</li>
<li>
<p>Different configurations for nodes in multiple node sets of the same cell are also supported, but are not covered in this guide.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>If TLS Everywhere is enabled, append the following content to the <code>OpenStackDataPlaneService</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>  tlsCerts:
    contents:
      - dnsnames
      - ips
    networks:
      - ctlplane
    issuer: osp-rootca-issuer-internal
    edpmRoleServiceName: nova
  caCerts: combined-ca-bundle
  edpmServiceType: nova</pre>
</div>
</div>
</li>
<li>
<p>Create a secret for the subscription manager:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic subscription-manager \
--from-literal rhc_auth='{"login": {"username": "&lt;subscription_manager_username&gt;", "password": "&lt;subscription_manager_password&gt;"}}'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;subscription_manager_username&gt;</code> with the applicable username.</p>
</li>
<li>
<p>Replace <code>&lt;subscription_manager_password&gt;</code> with the applicable password.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create a secret for the Red Hat registry:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc create secret generic redhat-registry \
--from-literal edpm_container_registry_logins='{"registry.redhat.io": {"&lt;registry_username&gt;": "&lt;registry_password&gt;"}}'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;registry_username&gt;</code> with the applicable username.</p>
</li>
<li>
<p>Replace <code>&lt;registry_password&gt;</code> with the applicable password.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You do not need to reference the <code>subscription-manager</code> secret in the <code>dataSources</code> field of the <code>OpenStackDataPlaneService</code> CR.
The secret is already passed in with a node-specific <code>OpenStackDataPlaneNodeSet</code> CR in the <code>ansibleVarsFrom</code> property in the <code>nodeTemplate</code> field.
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Create the data plane node set definitions for each cell:</p>
<div class="listingblock">
<div class="content">
<pre>$ declare -A names
$ for CELL in $(echo $RENAMED_CELLS); do
  ref="COMPUTES_$(echo ${CELL}|tr <em>[:lower:]</em> <em>[:upper:]</em>)"
  eval names=\${!${ref}[@]}
  ref_api="COMPUTES_API_$(echo ${CELL}|tr <em>[:lower:]</em> <em>[:upper:]</em>)"
  [ -z "$names" ] &amp;&amp; continue
  ind=0
  rm -f computes-$CELL
  for compute in $names; do
    ip="${ref}[<em>$compute</em>]"
    ip_api="${ref_api}[<em>$compute</em>]"
    cat &gt;&gt; computes-$CELL &lt;&lt; EOF
    ${compute}:
     <strong>hostName: $compute</strong>
      ansible:
        ansibleHost: $compute
     <strong>networks:</strong>
      - defaultRoute: true
        fixedIP: ${!ip}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
        fixedIP: ${!ip_api}
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
EOF
    ind=$(( ind + 1 ))
  done

  test -f computes-$CELL || continue
  cat &gt; nodeset-${CELL}.yaml &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
 <strong>name: openstack-$CELL</strong>
spec:
  <strong>tlsEnabled: false</strong>
  networkAttachments:
      - ctlplane
  preProvisioned: true
  <strong>services</strong>:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - install-certs
    - ovn
    - neutron-metadata
    - libvirt
    - nova-$CELL
    - telemetry
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
    - name: ANSIBLE_VERBOSITY
      value: <em>3</em>
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVarsFrom:
      - secretRef:
          name: subscription-manager
      - secretRef:
          name: redhat-registry
      ansibleVars:
        rhc_release: 9.2
        rhc_repositories:
            - {name: "<strong>", state: disabled}
            - {name: "rhel-9-for-x86_64-baseos-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-appstream-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-highavailability-eus-rpms", state: enabled}
            - {name: "rhoso-18.0-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "fast-datapath-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "rhceph-7-tools-for-rhel-9-x86_64-rpms", state: enabled}
        edpm_bootstrap_release_version_package: []
        # edpm_network_config
        # Default nic config template for a EDPM node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {% set _ = mtu_list.append(lookup(<em>vars</em>, networks_lower[network] ~ <em>_mtu</em>)) %}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_mtu</em>) }}
               vlan_id: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_vlan_id</em>) }}
               addresses:
               - ip_netmask:
                   {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_ip</em>) }}/{{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_cidr</em>) }}
               routes: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_host_routes</em>) }}
           {% endfor %}

        edpm_network_config_nmstate: false
        # Control resolv.conf management by NetworkManager
        # false = disable NetworkManager resolv.conf update (default)
        # true = enable NetworkManager resolv.conf update
        edpm_bootstrap_network_resolvconf_update: false
        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        *neutron_physical_bridge_name: br-ctlplane</strong>
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        <strong>edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt;</strong>
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com

        edpm_bootstrap_command: |
          set -euxo pipefail
          dnf -y upgrade openstack-selinux
          rm -f /run/virtlogd.pid

        gather_facts: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: [<em>192.168.122.0/24</em>]

        # Do not attempt OVS major upgrades here
        edpm_ovs_packages:
        - openvswitch3.3
        edpm_default_mounts:
          - <strong>path: /dev/hugepages&lt;size&gt;</strong>
            <strong>opts: pagesize=&lt;size&gt;</strong>
            fstype: hugetlbfs
            group: hugetlbfs
  nodes:
EOF
  cat computes-$CELL &gt;&gt; nodeset-${CELL}.yaml
done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>${compute}.hostName</code> specifies the FQDN for the node if your deployment has a custom DNS Domain.</p>
</li>
<li>
<p><code>${compute}.networks</code> specifies the network composition. The network composition must match the source cloud configuration to avoid data plane connectivity downtime. The <code>ctlplane</code> network must come first. The commands only retain IP addresses for the hosts on the <code>ctlplane</code> and <code>internalapi</code> networks. Repeat this step for other isolated networks, or update the resulting files manually.</p>
</li>
<li>
<p><code>metadata.name:</code> specifies the node set names for each cell, for example, <code>openstack-cell1</code>, <code>openstack-cell2</code>. Only create node sets for cells that contain Compute nodes.</p>
</li>
<li>
<p><code>spec.tlsEnabled</code> specifies whether TLS Everywhere is enabled. If it is enabled, change <code>tlsEnabled</code> to <code>true</code>.</p>
</li>
<li>
<p><code>spec.services</code> specifies the services to be adopted. If you are not adopting telemetry services, omit it from the services list.</p>
</li>
<li>
<p><code>neutron_physical_bridge_name: br-ctlplane</code> specifies the bridge name. The bridge name and other OVN and Networking service-specific values must match the source cloud configuration to avoid data plane connectivity downtime.
*<code>edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt;</code> specifies the value of the bridge mappings in your configuration, for example, <code>"datacentre:br-ctlplane"</code>.</p>
</li>
<li>
<p><code>path: /dev/hugepages&lt;size&gt;</code> and <code>opts: pagesize=&lt;size&gt;</code> configures huge pages. Replace <code>&lt;size&gt;</code> with the size of the page. To configure multi-sized huge pages, create more items in the list. Note that the mount points must match the source cloud configuration.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Ensure that you use the same <code>ovn-controller</code> settings in the <code>OpenStackDataPlaneNodeSet</code> CR that you used in the Compute service nodes before adoption. This configuration is stored in the <code>external_ids</code> column in the <code>Open_vSwitch</code> table in the Open vSwitch database:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ ovs-vsctl list Open .
...
external_ids        : {hostname=standalone.localdomain, ovn-bridge=br-int, ovn-bridge-mappings=&lt;bridge_mappings&gt;, ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneNodeSet</code> CRs for each Compute cell:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
test -f nodeset-${CELL}.yaml || continue
oc apply -f nodeset-${CELL}.yaml
done</pre>
</div>
</div>
</li>
<li>
<p>If you use a Red Hat Ceph Storage back end for Block Storage service (cinder), prepare the adopted data plane workloads:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
test -f nodeset-${CELL}.yaml || continue
oc patch osdpns/openstack-$CELL --type=merge --patch "
spec:
  services:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - ceph-client
    - install-certs
    - ovn
    - neutron-metadata
    - libvirt
    - nova-$CELL
    - telemetry
  nodeTemplate:
    extraMounts:
    - extraVolType: Ceph
      volumes:
      - name: ceph
        secret:
          secretName: ceph-conf-files
      mounts:
      - name: ceph
        mountPath: "/etc/ceph"
        readOnly: true
  "
done</pre>
</div>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-sriov-nic-agent</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
test -f nodeset-${CELL}.yaml || continue
oc patch openstackdataplanenodeset openstack-$CELL --type='json' --patch='[
{
  "op": "add",
  "path": "/spec/services/-",
  "value": "neutron-sriov"
}, {
  "op": "add",
  "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_physical_device_mappings",
  "value": "dummy_sriov_net:dummy-dev"
}, {
  "op": "add",
  "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_bandwidths",
  "value": "dummy-dev:40000000:40000000"
}, {
  "op": "add",
  "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_hypervisors",
  "value": "dummy-dev:standalone.localdomain"
}]'
done</pre>
</div>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-dhcp</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
test -f nodeset-${CELL}.yaml || continue
oc patch openstackdataplanenodeset openstack-$CELL --type='json' --patch='[
{
  "op": "add",
  "path": "/spec/services/-",
  "value": "neutron-dhcp"
}]'
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>To use <code>neutron-dhcp</code> with OVN for the Bare Metal Provisioning service (ironic), you must set the <code>disable_ovn_dhcp_for_baremetal_ports</code> configuration option for the Networking service (neutron)  to <code>true</code>.  You can set this configuration in the <code>NeutronAPI</code> spec:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>..
spec:
  serviceUser: neutron
   ...
      customServiceConfig: |
          [DEFAULT]
          dhcp_agent_notification = True
          [ovn]
          disable_ovn_dhcp_for_baremetal_ports = true</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run the pre-adoption validation:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create the validation service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: pre-adoption-validation
spec:
  playbook: osp.edpm.pre_adoption_validation
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create a <code>OpenStackDataPlaneDeployment</code> CR that runs only the validation:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption
spec:
  nodeSets: $NODESETS
  servicesOverride:
  - pre-adoption-validation
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you created different migration SSH keys for different <code>OpenStackDataPlaneService</code> CRs, you should also define a separate <code>OpenStackDataPlaneDeployment</code> CR for each node set or node sets that represent a cell.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>When the validation is finished, confirm that the status of the Ansible EE pods is <code>Completed</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the deployment to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption --timeout=10m</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If any openstack-pre-adoption validations fail, you must reference the Ansible logs to determine which ones were unsuccessful, and then try the following troubleshooting options:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the hostname validation failed, check that the hostname of the data plane
node is correctly listed in the <code>OpenStackDataPlaneNodeSet</code> CR.</p>
</li>
<li>
<p>If the kernel argument check failed, ensure that the kernel argument configuration in the <code>edpm_kernel_args</code> and <code>edpm_kernel_hugepages</code> variables in the <code>OpenStackDataPlaneNodeSet</code> CR is the same as the kernel argument configuration that you used in the Red&#160;Hat OpenStack Platform (RHOSP) 17.1 node.</p>
</li>
<li>
<p>If the tuned profile check failed, ensure that the
<code>edpm_tuned_profile</code> variable in the <code>OpenStackDataPlaneNodeSet</code> CR is configured
to use the same profile as the one set on the RHOSP 17.1 node.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Remove the remaining director services:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create an <code>OpenStackDataPlaneService</code> CR to clean up the data plane services you are adopting:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: tripleo-cleanup
spec:
  playbook: osp.edpm.tripleo_cleanup
EOF</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>OpenStackDataPlaneDeployment</code> CR to run the clean-up:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: tripleo-cleanup
spec:
  nodeSets: $NODESETS
  servicesOverride:
  - tripleo-cleanup
EOF</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>When the clean-up is finished, deploy the <code>OpenStackDataPlaneDeployment</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack
spec:
  nodeSets: $NODESETS
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you have other node sets to deploy, such as Networker nodes, you can
add them in the <code>nodeSets</code> list in this step, or create separate <code>OpenStackDataPlaneDeployment</code> CRs later. You cannot add new node sets to an <code>OpenStackDataPlaneDeployment</code> CR after deployment.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the data plane node sets to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
oc wait --for condition=Ready osdpns/openstack-$CELL --timeout=30m
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Networking service (neutron) agents are running:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                   | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+
| 174fc099-5cc9-4348-b8fc-59ed44fcfb0e | DHCP agent                   | standalone.localdomain | nova              | :-)   | UP    | neutron-dhcp-agent         |
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.localdomain |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller agent         | standalone.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>After you remove all the services from the director cell controllers, you can decomission the cell controllers.
To create new cell Compute nodes, you re-provision the decomissioned controllers as new data plane hosts and add them to the node sets of corresponding or new cells.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="ulist">
<div class="title">Next steps</div>
<ul>
<li>
<p>You must perform a fast-forward upgrade on your Compute services. For more information, see <a href="#performing-a-fast-forward-upgrade-on-compute-services_data-plane">Performing a fast-forward upgrade on Compute services</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="configuring-dcn-data-plane-nodesets_data-plane">5.3. Configuring data plane node sets for DCN sites</h3>
<div class="paragraph _abstract">
<p>If you are adopting a Distributed Compute Node (DCN) deployment, you must create separate <code>OpenStackDataPlaneNodeSet</code> custom resources (CRs) for each site. Each node set requires site-specific configuration for network subnets, OVN bridge mappings, and inter-site routes.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have adopted the Red&#160;Hat OpenStack Platform (RHOSP) control plane to Red&#160;Hat OpenStack Services on OpenShift (RHOSO).</p>
</li>
<li>
<p>You have configured control plane networking for your spine-leaf topology, including multi-subnet <code>NetConfig</code> and <code>NetworkAttachmentDefinition</code> CRs with routes to remote sites. For more information, see <a href="#configuring-control-plane-networking-for-spine-leaf_adopt-control-plane">Configuring control plane networking for spine-leaf topologies</a>.</p>
</li>
<li>
<p>You have the network configuration information for each DCN site:</p>
<div class="ulist">
<ul>
<li>
<p>IP addresses and hostnames for all Compute nodes</p>
</li>
<li>
<p>VLAN IDs for each service network</p>
</li>
<li>
<p>Gateway addresses for inter-site routing</p>
</li>
</ul>
</div>
</li>
<li>
<p>You have identified the OVN bridge mappings (physnets) for each site.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Define the OVN bridge mappings for each site. Each site requires a unique physnet that maps to the local provider network bridge:</p>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 5. Example OVN bridge mappings</caption>
<colgroup>
<col style="width: 50%;"/>
<col style="width: 50%;"/>
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Site</th>
<th class="tableblock halign-left valign-top">OVN bridge mapping</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">Central</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>leaf0:br-ex</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DCN1</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>leaf1:br-ex</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">DCN2</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>leaf2:br-ex</code></p></td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Configure OVN for DCN sites. The default OVN controller configuration uses the Kubernetes ClusterIP (<code>ovsdbserver-sb.openstack.svc</code>), which is not routable from remote DCN sites. You must create a DCN-specific configuration that uses direct <code>internalapi</code> IP addresses.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Get the OVN Southbound database <code>internalapi</code> IP addresses:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get pod -l service=ovsdbserver-sb -o jsonpath='{range .items[*]}{.metadata.annotations.k8s\.v1\.cni\.cncf\.io/network-status}{"\n"}{end}' | jq -r '.[] | select(.name=="openstack/internalapi") | .ips[0]'</pre>
</div>
</div>
<div class="paragraph">
<p>Example output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.17.0.34
172.17.0.35
172.17.0.36</pre>
</div>
</div>
</li>
<li>
<p>Create a ConfigMap with the OVN SB direct IPs for DCN sites:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: ovncontroller-config-dcn
  namespace: openstack
data:
  ovsdb-config: |
    <strong>ovn-remote: tcp:172.17.0.34:6642,tcp:172.17.0.35:6642,tcp:172.17.0.36:6642</strong>
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace the IP addresses with the actual <code>internalapi</code> IPs from the previous step.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create an <code>OpenStackDataPlaneService</code> CR for DCN OVN configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: ovn-dcn
  namespace: openstack
spec:
  addCertMounts: false
  caCerts: combined-ca-bundle
  containerImageFields:
  - OvnControllerImage
  dataSources:
  - configMapRef:
      name: ovncontroller-config-dcn
  edpmServiceType: ovn
  playbook: osp.edpm.ovn
  tlsCerts:
    default:
      contents:
      - dnsnames
      - ips
      issuer: osp-rootca-issuer-ovn
      keyUsages:
      - digital signature
      - key encipherment
      - server auth
      - client auth
      networks:
      - ctlplane
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>ovn-dcn</code> service uses the <code>ovncontroller-config-dcn</code> ConfigMap (through <code>dataSources</code>), which contains the direct <code>internalapi</code> IPs instead of the <code>ClusterIP</code>. DCN node sets must use this service instead of the default <code>ovn</code> service.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create an <code>OpenStackDataPlaneNodeSet</code> CR for the central site Compute nodes:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack-edpm
spec:
  tlsEnabled: false
  networkAttachments:
    - ctlplane
  preProvisioned: true
  services:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - install-certs
    - ovn
    - neutron-metadata
    - libvirt
    - nova-cell1
    - telemetry
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVars:
        <strong>edpm_ovn_bridge_mappings: ["leaf0:br-ex"]</strong>
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        # Network configuration template for central site
        edpm_network_config_template: |
          ---
          network_config:
          - type: ovs_bridge
            name: {{ neutron_physical_bridge_name }}
            use_dhcp: false
            dns_servers: {{ ctlplane_dns_nameservers }}
            addresses:
            - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
            routes: {{ ctlplane_host_routes }}
            members:
            - type: interface
              name: nic1
              primary: true
          {% for network in nodeset_networks %}
            - type: vlan
              vlan_id: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_vlan_id</em>) }}
              addresses:
              - ip_netmask:
                  {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_ip</em>) }}/{{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_cidr</em>) }}
              routes: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_host_routes</em>) }}
          {% endfor %}
  nodes:
    compute-0:
      hostName: compute-0.example.com
      ansible:
        ansibleHost: compute-0.example.com
      networks:
      - defaultRoute: true
        fixedIP: 192.168.122.100
        name: ctlplane
        <strong>subnetName: subnet1</strong>
      - name: internalapi
        <strong>subnetName: subnet1</strong>
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>The OVN bridge mapping uses the central site physnet <code>leaf0</code>.</p>
</li>
<li>
<p>Central site nodes reference <code>subnet1</code> for all networks.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create an <code>OpenStackDataPlaneNodeSet</code> CR for DCN1 edge site compute nodes. You must add inter-site routes to the network configuration template and use the <code>ovn-dcn</code> service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack-edpm-dcn1
spec:
  tlsEnabled: false
  networkAttachments:
    - ctlplane
  preProvisioned: true
  services:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - reboot-os
    - install-certs
    - ovn-dcn
    - neutron-metadata
    - libvirt
    - nova-cell1
    - telemetry
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVars:
        <strong>edpm_ovn_bridge_mappings: ["leaf1:br-ex"]</strong>
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        # Network configuration template for DCN1 site with inter-site routes
        edpm_network_config_template: |
          ---
          network_config:
          - type: ovs_bridge
            name: {{ neutron_physical_bridge_name }}
            use_dhcp: false
            dns_servers: {{ ctlplane_dns_nameservers }}
            addresses:
            - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
            routes:  <i class="conum" data-value="3"></i><b>(3)</b>
              {{ ctlplane_host_routes }}
              <strong>- ip_netmask: 192.168.122.0/24</strong>
                <strong>next_hop: 192.168.133.1</strong>
              - ip_netmask: 192.168.144.0/24
                next_hop: 192.168.133.1
            members:
            - type: interface
              name: nic1
              primary: true
          {% for network in nodeset_networks %}
            - type: vlan
              vlan_id: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_vlan_id</em>) }}
              addresses:
              - ip_netmask:
                  {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_ip</em>) }}/{{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_cidr</em>) }}
              routes:
                {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_host_routes</em>) }}
                {% if network == <em>internalapi</em> %}
                <strong>- ip_netmask: 172.17.0.0/24</strong>
                  <strong>next_hop: 172.17.10.1</strong>
                - ip_netmask: 172.17.20.0/24
                  next_hop: 172.17.10.1
                {% endif %}
                {% if network == <em>storage</em> %}
                - ip_netmask: 172.18.0.0/24
                  next_hop: 172.18.10.1
                - ip_netmask: 172.18.20.0/24
                  next_hop: 172.18.10.1
                {% endif %}
                {% if network == <em>tenant</em> %}
                - ip_netmask: 172.19.0.0/24
                  next_hop: 172.19.10.1
                - ip_netmask: 172.19.20.0/24
                  next_hop: 172.19.10.1
                {% endif %}
          {% endfor %}
  nodes:
    dcn1-compute-0:
      hostName: dcn1-compute-0.example.com
      ansible:
        ansibleHost: dcn1-compute-0.example.com
      networks:
      - defaultRoute: true
        fixedIP: 192.168.133.100
        name: ctlplane
        <strong>subnetName: ctlplanedcn1</strong>
      - name: internalapi
        <strong>subnetName: internalapidcn1</strong>
      - name: storage
        <strong>subnetName: storagedcn1</strong>
      - name: tenant
        <strong>subnetName: tenantdcn1</strong></code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>ovn</code> with <code>ovn-dcn</code> under spec:services. This ensures OVN controller connects to the OVN Southbound database using direct internalapi IPs instead of the unreachable ClusterIP.</p>
</li>
<li>
<p>DCN1 uses the <code>leaf1</code> physnet,  for its OVN bridge mapping under <code>spec:nodeTemplate:ansible:ansibleVars:edpm_ovn_bridge_mappings</code>.</p>
</li>
<li>
<p>Inter-site routes must be added to the network configuration template. These routes enable DCN1 compute nodes to reach the central site (192.168.122.0/24) and other DCN sites (192.168.144.0/24 for DCN2). Similar routes are added for each service network (internalapi, storage, tenant).</p>
</li>
<li>
<p>DCN1 nodes reference site-specific subnet names like <code>ctlplanedcn1</code> and <code>internalapidcn1</code>. These subnet names must match those defined in the <code>NetConfig</code> CR.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Repeat step 3 for all other DCN sites. Adjust site specific parameters:</p>
<div class="ulist">
<ul>
<li>
<p>The nodeset name, for example: <code>openstack-edpm-dcn2</code></p>
</li>
<li>
<p>The OVN bridge mapping, for example: <code>leaf2:br-ex</code></p>
</li>
<li>
<p>The subnet names, for example: <code>ctlplanedcn2</code>, and <code>internalapidcn2</code></p>
</li>
<li>
<p>The inter-site routes. The routes from DCN2 should point to the central site subnets and the DCN1 site subnets.</p>
</li>
<li>
<p>The compute node definitions with site-appropriate IP addresses.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Deploy all nodesets by creating an <code>OpenStackDataPlaneDeployment</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-edpm-deployment
spec:
  nodeSets:
    - openstack-edpm
    - openstack-edpm-dcn1
    - openstack-edpm-dcn2</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>All nodesets can be deployed in parallel once the control plane adoption is complete.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Wait for the deployment to complete:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployment/openstack-edpm-deployment --timeout=40m</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that all node sets reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get openstackdataplanenodeset
NAME                  STATUS   MESSAGE
openstack-edpm        True     Ready
openstack-edpm-dcn1   True     Ready
openstack-edpm-dcn2   True     Ready</pre>
</div>
</div>
</li>
<li>
<p>Verify that Compute services are running across all sites. Ensure that all <code>nova-compute</code> services show <code>State=up</code> for nodes in all availability zones:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack compute service list</pre>
</div>
</div>
</li>
<li>
<p>Verify inter-site connectivity by checking routes on a DCN Compute node:</p>
<div class="listingblock">
<div class="content">
<pre>$ ssh dcn1-compute-0 ip route show | grep 172.17.0
172.17.0.0/24 via 172.17.10.1 dev internalapi</pre>
</div>
</div>
</li>
<li>
<p>Test that DCN Compute nodes can reach the control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ ssh dcn1-compute-0 ping -c 3 172.17.0.30</pre>
</div>
</div>
<div class="paragraph">
<p>Replace <code>172.17.0.30</code> with an IP address of a control plane service on the internalapi network.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="performing-a-fast-forward-upgrade-on-compute-services_data-plane">5.4. Performing a fast-forward upgrade on Compute services</h3>
<div class="paragraph _abstract">
<p>You must upgrade the Compute services from Red&#160;Hat OpenStack Platform 17.1 to Red&#160;Hat OpenStack Services on OpenShift (RHOSO) 18.0 on the control plane and data plane by completing the following tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Update the cell1 Compute data plane services version.</p>
</li>
<li>
<p>Remove pre-fast-forward upgrade workarounds from the Compute control plane services and Compute data plane services.</p>
</li>
<li>
<p>Run Compute database online migrations to update live data.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the shell variables necessary to apply the fast-forward upgrade commands for each Compute service cell.</p>
<div class="listingblock">
<div class="content">
<pre>DEFAULT_CELL_NAME="cell1"
RENAMED_CELLS="$DEFAULT_CELL_NAME"

declare -A PODIFIED_DB_ROOT_PASSWORD
for CELL in $(echo "super $RENAMED_CELLS"); do
  PODIFIED_DB_ROOT_PASSWORD[$CELL]=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)
done</pre>
</div>
</div>
</li>
<li>
<p>Complete the steps in <a href="#adopting-compute-services-to-the-data-plane_data-plane">Adopting Compute services to the RHOSO data plane</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Wait for the Compute service data plane services version to update for all the cells:</p>
<div class="listingblock">
<div class="content">
<pre>for CELL in $(echo $RENAMED_CELLS); do
oc exec openstack-$CELL-galera-0 -c galera -- mysql -rs -uroot -p"{PODIFIED_DB_ROOT_PASSWORD[$CELL]}" \
  -e "select a.version from nova_${CELL}.services a join nova_${CELL}.services b where a.version!=b.version and a.binary='nova-compute' and a.deleted=0;"
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The query returns an empty result when the update is completed. No downtime is expected for virtual machine (VM) workloads.</p>
</div>
<div class="paragraph">
<p>Review any errors in the nova Compute agent logs on the data plane, and the <code>nova-conductor</code> journal records on the control plane.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Patch the <code>OpenStackControlPlane</code> CR to remove the pre-fast-forward upgrade workarounds from the Compute control plane services:</p>
<div class="listingblock">
<div class="content">
<pre>$ rm -f celltemplates
$ for CELL in $(echo $RENAMED_CELLS); do
$ cat &gt;&gt; celltemplates &lt;&lt; EOF
        ${CELL}:
          metadataServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
EOF
done

$ cat &gt; oscp-patch.yaml &lt;&lt; EOF
spec:
  nova:
    template:
      apiServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      metadataServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      schedulerServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      cellTemplates:
        cell0:
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
EOF
$ cat celltemplates &gt;&gt; oscp-patch.yaml</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>If you are adopting the Compute service with the Bare Metal Provisioning service (ironic), append the following <code>novaComputeTemplates</code> in the <code>cell&lt;X&gt;</code> section of the Compute service CR patch:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">        cell&lt;X&gt;:
          novaComputeTemplates:
            &lt;hostname&gt;:
              customServiceConfig: |
                [DEFAULT]
                host = &lt;hostname&gt;
                [workarounds]
                disable_compute_service_check_for_ffu=true
              computeDriver: ironic.IronicDriver
        ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>where:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">&lt;hostname&gt;</dt>
<dd>
<p>Specifies the hostname of the node that is running the <code>ironic</code> Compute driver in the source cloud of <code>cell&lt;X&gt;</code>.</p>
</dd>
</dl>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the patch file:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge --patch-file=oscp-patch.yaml</pre>
</div>
</div>
</li>
<li>
<p>Wait until the Compute control plane services CRs are ready:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready --timeout=300s Nova/nova</pre>
</div>
</div>
</li>
<li>
<p>Remove the pre-fast-forward upgrade workarounds from the Compute data plane services:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch cm nova-cells-global-config --type=json -p='[{"op": "replace", "path": "/data/99-nova-compute-cells-workarounds.conf", "value": "[workarounds]\n"}]'
$ for CELL in $(echo $RENAMED_CELLS); do
$ oc get Openstackdataplanenodeset openstack-${CELL} || continue
$ oc apply -f - &lt;&lt;EOF
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-nova-compute-ffu-$CELL
spec:
  nodeSets:
    - openstack-${CELL}
  servicesOverride:
    - nova-${CELL}
backoffLimit: 3
EOF
done</pre>
</div>
</div>
</li>
<li>
<p>Wait for the Compute data plane services to be ready for all the cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployments --all --timeout=5m</pre>
</div>
</div>
</li>
<li>
<p>Run Compute database online migrations to complete the upgrade:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec -it nova-cell0-conductor-0 -- nova-manage db online_data_migrations
$ for CELL in $(echo $RENAMED_CELLS); do
$ oc exec -it nova-${CELL}-conductor-0 -- nova-manage db online_data_migrations
done</pre>
</div>
</div>
</li>
<li>
<p>Discover the Compute hosts in the cells:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh nova-cell0-conductor-0 nova-manage cell_v2 discover_hosts --verbose</pre>
</div>
</div>
</li>
<li>
<p>If you have a test VM that is not a production workload, complete the following verification steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Verify if the existing test VM instance is running:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test 2&gt;&amp;1 || echo FAIL</pre>
</div>
</div>
</li>
<li>
<p>Verify if the Compute services can stop the existing test VM instance:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test ACTIVE" &amp;&amp; ${BASH_ALIASES[openstack]} server stop test || echo PASS
${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test SHUTOFF" || echo FAIL
${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test 2&gt;&amp;1 || echo PASS</pre>
</div>
</div>
</li>
<li>
<p>Verify if the Compute services can start the existing test VM instance:</p>
<div class="listingblock">
<div class="content">
<pre>${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test SHUTOFF" &amp;&amp; ${BASH_ALIASES[openstack]} server start test || echo PASS
${BASH_ALIASES[openstack]} server list -c Name -c Status -f value | grep -qF "test ACTIVE" &amp;&amp; \
  ${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test --fit-width -f json | jq -r '.state' | grep running || echo FAIL</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>After the data plane adoption, the Compute hosts continue to run Red Hat Enterprise Linux (RHEL) 9.2. To take advantage of RHEL 9.4, perform a minor update procedure after finishing the adoption procedure.</p>
</div>
</div>
<div class="sect2">
<h3 id="adopting-networker-services-to-the-data-plane_data-plane">5.5. Adopting Networker services to the RHOSO data plane</h3>
<div class="paragraph _abstract">
<p>Adopt the Networker services in your existing Red&#160;Hat OpenStack Platform deployment to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) data plane. The <code>Networker</code> services could be running on <code>Conroller</code> nodes or dedicated <code>Networker</code> nodes. You decide which services you want to run on the Networker nodes, and create a separate <code>OpenStackDataPlaneNodeSet</code> custom resource (CR) for the Networker nodes. You might also decide to implement the following options if they apply to your environment:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Depending on your topology, you might need to run the <code>neutron-metadata</code> service on the nodes, specifically when you want to serve metadata to SR-IOV ports that are hosted on Compute nodes.</p>
</li>
<li>
<p>If you want to continue running OVN gateway services on Networker nodes, keep <code>ovn</code> service in the list to deploy.</p>
</li>
<li>
<p>Optional: You can run the <code>neutron-dhcp</code> service on your Networker nodes instead of your Compute nodes. You might not need to use <code>neutron-dhcp</code> with OVN, unless your deployment uses DHCP relays, or advanced DHCP options that are supported by dnsmasq but not by the OVN DHCP implementation.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Adopt each Controller or Networker node in your existing Red&#160;Hat OpenStack Platform deployment to the Red&#160;Hat OpenStack Services on OpenShift (RHOSO) when your node is set as an OVN chassis gateway. Any node with
parameter set to <code>enable-chassis-as-gw</code> is considered OVN gateway chassis. In this case, such nodes will become edpm networker nodes after adoption.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Check for the nodes where <code>OVN Controller Gateway agent</code> agents are running. The list of agents varies depending on the services you enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                     | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| e5075ee0-9dd9-4f0a-a42a-6bbdf1a6111c | OVN Controller Gateway agent | controller-0.localdomain |                   | XXX   | UP    | ovn-controller             |
| f3112349-054c-403a-b00a-e219238192b8 | OVN Controller agent         | compute-0.localdomain    |                   | XXX   | UP    | ovn-controller             |
| af9dae2d-1c1c-55a8-a743-f84719f6406d | OVN Metadata agent           | compute-0.localdomain    |                   | XXX   | UP    | neutron-ovn-metadata-agent |
| 51a11df8-a66e-47a2-aec0-52eb8589626c | OVN Controller Gateway agent | controller-1.localdomain |                   | XXX   | UP    | ovn-controller             |
| bb817e5e-7832-410a-9e67-934dac8c602f | OVN Controller Gateway agent | controller-2.localdomain |                   | XXX   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Define the shell variable. Based on above agent list output,
controller-0, controller-1, controller-2 are our target
hosts. If you have both <code>Controller</code> and <code>Networker</code> nodes running
networker services then add all those hosts below.</p>
<div class="listingblock">
<div class="content">
<pre>declare -A networkers
networkers+=(
  ["controller-0.localdomain"]="192.168.122.100"
  ["controller-1.localdomain"]="192.168.122.101"
  ["controller-2.localdomain"]="192.168.122.102"
  # ...
)</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>["&lt;node-name&gt;"]="192.168.122.100"</code> with the name and IP address of the corresponding Networker or Controller node as per your environment.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Deploy the <code>OpenStackDataPlaneNodeSet</code> CR for your nodes:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You can reuse most of the <code>nodeTemplate</code> section from the <code>OpenStackDataPlaneNodeSet</code> CR that is designated for your Compute nodes. You can omit some of the variables because of the limited set of services that are running on the Networker nodes.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack-networker
spec:
  tlsEnabled: false
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - redhat
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - install-certs
    - ovn
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
  nodes:
    controller-0:
      hostName: controller-0
      ansible:
        ansibleHost: ${networkers[controller-0.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[controller-0.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
    controller-1:
      hostName: controller-1
      ansible:
        ansibleHost: ${networkers[controller-1.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[controller-1.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
    controller-2:
      hostName: controller-2
      ansible:
        ansibleHost: ${networkers[controller-2.localdomain]}
      networks:
      - defaultRoute: true
        fixedIP: ${networkers[controller-2.localdomain]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
      ansibleVarsFrom:
      - secretRef:
          name: subscription-manager
      - secretRef:
          name: redhat-registry
      ansibleVars:
        rhc_release: 9.2
        rhc_repositories:
            - {name: "*", state: disabled}
            - {name: "rhel-9-for-x86_64-baseos-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-appstream-eus-rpms", state: enabled}
            - {name: "rhel-9-for-x86_64-highavailability-eus-rpms", state: enabled}
            - {name: "rhoso-18.0-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "fast-datapath-for-rhel-9-x86_64-rpms", state: enabled}
            - {name: "rhceph-7-tools-for-rhel-9-x86_64-rpms", state: enabled}
        edpm_bootstrap_release_version_package: []
        # edpm_network_config
        # Default nic config template for a EDPM node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {% set _ = mtu_list.append(lookup(<em>vars</em>, networks_lower[network] ~ <em>_mtu</em>)) %}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_mtu</em>) }}
               vlan_id: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_vlan_id</em>) }}
               addresses:
               - ip_netmask:
                   {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_ip</em>) }}/{{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_cidr</em>) }}
               routes: {{ lookup(<em>vars</em>, networks_lower[network] ~ <em>_host_routes</em>) }}
           {% endfor %}
        edpm_network_config_nmstate: false
        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt;
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        # serve as a OVN gateway
        edpm_enable_chassis_gw: true

        timesync_ntp_servers:
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com


        gather_facts: false
        enable_debug: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: [<em>192.168.122.0/24</em>]
        # SELinux module
        edpm_selinux_mode: enforcing

        # Do not attempt OVS major upgrades here
        edpm_ovs_packages:
        - openvswitch3.3
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.tlsEnabled</code> specifies whether TLS Everywhere is enabled. If TLS is enabled, change <code>spec:tlsEnabled</code> to <code>true</code>.</p>
</li>
<li>
<p><code>edpm_ovn_bridge_mappings: &lt;bridge_mappings&gt;</code> specifies the bridge mapping values that you used in your Red&#160;Hat OpenStack Platform 17.1 deployment.</p>
</li>
<li>
<p><code>edpm_enable_chassis_gw</code> specifies whether to run <code>ovn-controller</code> in gateway mode.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Ensure that you use the same <code>ovn-controller</code> settings in the <code>OpenStackDataPlaneNodeSet</code> CR that you used in the Networker nodes before adoption. This configuration is stored in the <code>external_ids</code> column in the <code>Open_vSwitch</code> table in the Open vSwitch database:</p>
<div class="listingblock">
<div class="content">
<pre>ovs-vsctl list Open .
...
external_ids        : {hostname=controller-0.localdomain, ovn-bridge=br-int, ovn-bridge-mappings=&lt;bridge_mappings&gt;, ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-cms-options=enable-chassis-as-gw, ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;bridge_mappings&gt;</code> with the value of the bridge mappings in your configuration, for example, <code>"datacentre:br-ctlplane"</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-metadata</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackdataplanenodeset &lt;networker_CR_name&gt; --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-metadata"
  }]'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;networker_CR_name&gt;</code> with the name of the CR that you deployed for your Networker nodes, for example, <code>openstack-networker</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Enable <code>neutron-dhcp</code> in the <code>OpenStackDataPlaneNodeSet</code> CR:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackdataplanenodeset &lt;networker_CR_name&gt; --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-dhcp"
  }]'</pre>
</div>
</div>
</li>
<li>
<p>Run the <code>pre-adoption-validation</code> service for Networker nodes:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a <code>OpenStackDataPlaneDeployment</code> CR that runs only the validation:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-pre-adoption-networker
spec:
  nodeSets:
  - openstack-networker
  servicesOverride:
  - pre-adoption-validation
EOF</pre>
</div>
</div>
</li>
<li>
<p>When the validation is finished, confirm that the status of the Ansible EE pods is <code>Completed</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the deployment to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready openstackdataplanedeployment/openstack-pre-adoption-networker --timeout=10m</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Deploy the <code>OpenStackDataPlaneDeployment</code> CR for Networker nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f - &lt;&lt;EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-networker
spec:
  nodeSets:
  - openstack-networker
EOF</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Alternatively, you can include the Networker node set in the <code>nodeSets</code> list before you deploy the main <code>OpenStackDataPlaneDeployment</code> CR. You cannot add new node sets to the <code>OpenStackDataPlaneDeployment</code> CR after deployment.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Clean up any Networking service (neutron) agents that are no longer running.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In some cases, agents from the old data plane that are replaced or retired remain in RHOSO. The function these agents provided might be provided by a new agent that is running in RHOSO, or the function might be replaced by other components. For example, DHCP agents might no longer be needed, since OVN DHCP in RHOSO can provide this function.
</td>
</tr>
</table>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>List the agents:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                     | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| e5075ee0-9dd9-4f0a-a42a-6bbdf1a6111c | OVN Controller Gateway agent | controller-0.localdomain |                   | :-)   | UP    | ovn-controller             |
| 856960f0-5530-46c7-a331-6eadcba362da | DHCP agent                   | controller-1.localdomain | nova              | XXX   | UP    | neutron-dhcp-agent         |
| 8bd22720-789f-45b8-8d7d-006dee862bf9 | DHCP agent                   | controller-2.localdomain | nova              | XXX   | UP    | neutron-dhcp-agent         |
| e584e00d-be4c-4e98-a11a-4ecd87d21be7 | DHCP agent                   | controller-0.localdomain | nova              | XXX   | UP    | neutron-dhcp-agent         |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
<li>
<p>If any agent in the list shows <code>XXX</code> in the <code>Alive</code> field, verify the Host and Agent Type, if the functions of this agent is no longer required, and the agent has been permanently stopped on the Red&#160;Hat OpenStack Platform host. Then, delete the agent:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent delete &lt;agent_id&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;agent_id&gt;</code> with the ID of the agent to delete, for example, <code>856960f0-5530-46c7-a331-6eadcba362da</code>.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Confirm that all the Ansible EE pods reach a <code>Completed</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ watch oc get pod -l app=openstackansibleee</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc logs -l app=openstackansibleee -f --max-log-requests 20</pre>
</div>
</div>
</li>
<li>
<p>Wait for the data plane node set to reach the <code>Ready</code> status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait --for condition=Ready osdpns/&lt;networker_CR_name&gt; --timeout=30m</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;networker_CR_name&gt;</code> with the name of the CR that you deployed for your Networker nodes, for example, <code>openstack-networker</code>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Verify that the Networking service (neutron) agents are running. The list of agents varies depending on the services you enabled:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                     | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+
| e5075ee0-9dd9-4f0a-a42a-6bbdf1a6111c | OVN Controller Gateway agent | controller-0.localdomain |                   | :-)   | UP    | ovn-controller             |
| f3112349-054c-403a-b00a-e219238192b8 | OVN Controller agent         | compute-0.localdomain    |                   | :-)   | UP    | ovn-controller             |
| af9dae2d-1c1c-55a8-a743-f84719f6406d | OVN Metadata agent           | compute-0.localdomain    |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| 51a11df8-a66e-47a2-aec0-52eb8589626c | OVN Controller Gateway agent | controller-1.localdomain |                   | :-)   | UP    | ovn-controller             |
| bb817e5e-7832-410a-9e67-934dac8c602f | OVN Controller Gateway agent | controller-2.localdomain |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+--------------------------+-------------------+-------+-------+----------------------------+</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="enabling-high-availability-for-instances_data-plane">5.6. Enabling the high availability for Compute instances service</h3>
<div class="paragraph _abstract">
<p>To enable the high availability for Compute instances (Instance HA) service, you create the following resources:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Fencing secret.</p>
</li>
<li>
<p>Configuration map. You can create the configuration map manually, or the configuration map is created automatically when you deploy the Instance HA resource. However, you must create the configuration map manually if you want to disable the Instance HA service.</p>
</li>
<li>
<p>Instance HA resource.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have created the <code>fencing-secret.yaml</code> configuration file. For more information, see <a href="#maintaining-instance-ha-functionality-after-adoption_preparing-instance-HA">Maintaining the Instance HA functionality after adoption</a>.</p>
</li>
<li>
<p>You have disabled Pacemaker on your Compute nodes. For more information, see <a href="#preventing-pacemaker-from-monitoring-compute-nodes_preparing-instance-HA">Preventing Pacemaker from monitoring Compute nodes</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create the secret:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f fencing-secret.yaml -n openstack</pre>
</div>
</div>
</li>
<li>
<p>Optional: Create the Instance HA configuration map and set the <code>DISABLED</code> parameter to <code>false</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; iha-cm.yaml
kind: ConfigMap
metadata:
  name: instanceha-0-config
  namespace: openstack
apiVersion: v1
data:
  config.yaml: |
    config:
      EVACUABLE_TAG: "evacuable"
      TAGGED_IMAGES: "true"
      TAGGED_FLAVORS: "true"
      TAGGED_AGGREGATES: "true"
      SMART_EVACUATION: "false"
      DELTA: "30"
      DELAY: "0"
      POLL: "45"
      THRESHOLD: "50"
      WORKERS: "4"
      RESERVED_HOSTS: "false"
      LEAVE_DISABLED: "false"
      CHECK_KDUMP: "false"
      LOGLEVEL: "info"
      DISABLED: "false"
EOF</pre>
</div>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Apply the configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f iha-cm.yaml -n openstack</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
If you want to restrict which Compute nodes are evacuated, create host aggregates and set them by using the <code>EVACUABLE_TAG</code> parameter. Alternatively, you can set the <code>TAGGED_AGGREGATES</code> parameter to <code>false</code> to enable monitoring and evacuation of all your Compute nodes. For more information about Instance HA service parameters, see <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_high_availability_for_instances/assembly_deploying-and-configuring-the-high-availability-for-compute-instances-service_instance-ha#proc_editing-the-instance-ha-service-parameters_instance-ha">Editing the Instance HA service parameters</a> in <em>Configuring high availability for instances</em>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Create an Instance HA resource and reference the fencing secret and configuration map. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ cat &lt;&lt; EOF &gt; iha.yaml
apiVersion: instanceha.openstack.org/v1beta1
kind: InstanceHa
metadata:
  name: instanceha-0
  namespace: openstack
spec:
  caBundleSecretName: combined-ca-bundle
  instanceHaConfigMap:
  fencingSecret: fencing-secret
EOF</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>spec.instanceHaConfigMap</code> defines the name of the YAML file containing the Instance HA configuration map that you created. If you do not create this file, then a YAML file called <code>instanceha-config</code> is created automatically when the Instance HA service is installed, providing the default values of the Instance HA service parameters. You can then edit the values as needed.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Deploy the Instance HA resource:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc apply -f iha.yaml -n openstack</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Next steps</div>
<ul>
<li>
<p>After you complete the Red&#160;Hat OpenStack Services on OpenShift adoption, remove the Pacemaker components from the Compute nodes. You must run the following commands on each Compute node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl stop pacemaker_remote
$ sudo systemctl stop pcsd
$ sudo systemctl stop pcsd-ruby.service
$ sudo systemctl disable pacemaker_remote
$ sudo systemctl disable pcsd
$ sudo systemctl disable pcsd-ruby.service
$ sudo dnf remove pacemaker pacemaker-remote pcs pcsd -y</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="performing-post-adoption-cleanup-of-load-balancers_data-plane">5.7. Post-adoption tasks for the Load-balancing service</h3>
<div class="paragraph _abstract">
<p>If you adopted the Load-balancing service (octavia), after you complete the data plane adoption, you must perform the following tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Upgrade the amphorae virtual machines to the new images.</p>
</li>
<li>
<p>Remove obsolete resources from your existing load balancers.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have adopted the Load-balancing service. For more information, see <a href="#adopting-the-loadbalancer-service_troubleshooting-hsm">Adopting the Load-balancing service</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Ensure that the connectivity between the new control plane and the adopted Compute nodes is functional by creating a new load balancer and checking that its <code>provisioning_status</code> becomes <code>ACTIVE</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ alias openstack="oc exec -t openstackclient -- openstack"
$ openstack loadbalancer create --vip-subnet-id public-subnet --name lb-post-adoption --wait</pre>
</div>
</div>
</li>
<li>
<p>Trigger a failover for all existing load balancers to upgrade the amphorae virtual machines to use the new image and to establish connectivity with the new control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack loadbalancer list -f value -c id | \
      xargs -r -n1 -P4 ${BASH_ALIASES[openstack]} loadbalancer failover --wait</pre>
</div>
</div>
</li>
<li>
<p>Delete old flavors that were migrated to the new control plane:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack flavor delete octavia_65
# The following flavors might not exist in OSP 17.1 deployments
$ openstack flavor show octavia_amphora-mvcpu-ha &amp;&amp; \
  openstack flavor delete octavia_amphora-mvcpu-ha
$ openstack loadbalancer flavor show octavia_amphora-mvcpu-ha &amp;&amp; \
  openstack loadbalancer flavor delete octavia_amphora-mvcpu-ha
$ openstack loadbalancer flavorprofile show octavia_amphora-mvcpu-ha_profile &amp;&amp; \
  openstack loadbalancer flavorprofile delete octavia_amphora-mvcpu-ha_profile</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Some flavors might still be used by load balancers and cannot be deleted.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Delete the old management network and its ports:</p>
<div class="listingblock">
<div class="content">
<pre>$ for net_id in $(openstack network list -f value -c ID --name lb-mgmt-net); do \
    desc=$(openstack network show "$net_id" -f value -c description); \
    [ -z "$desc" ] &amp;&amp; WALLABY_LB_MGMT_NET_ID="$net_id" ; \
  done
$ for id in $(openstack port list --network "$WALLABY_LB_MGMT_NET_ID" -f value -c ID); do \
    openstack port delete "$id" ; \
  done
$ openstack network delete "$WALLABY_LB_MGMT_NET_ID"</pre>
</div>
</div>
</li>
<li>
<p>Verify that only one <code>lb-mgmt-net</code> and one <code>lb-mgmt-subnet</code> exists:</p>
<div class="listingblock">
<div class="content">
<pre>$ openstack network list | grep lb-mgmt-net
| fe470c29-0482-4809-9996-6d636e3feea3 | lb-mgmt-net          | 6a881091-097d-441c-937b-5a23f4f243b7 |
$ openstack subnet list | grep lb-mgmt-subnet
| 6a881091-097d-441c-937b-5a23f4f243b7 | lb-mgmt-subnet          | fe470c29-0482-4809-9996-6d636e3feea3 | 172.24.0.0/16   |</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="migrating-the-object-storage-service_hsm-integration">6. Migrating the Object Storage service to Red&#160;Hat OpenStack Services on OpenShift nodes</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>If you are using the Red&#160;Hat OpenStack Platform Object Storage service (swift) as an Object Storage service, you must migrate your Object Storage service to Red&#160;Hat OpenStack Services on OpenShift nodes.</p>
</div>
<div class="paragraph">
<p>If you are using the Object Storage API of the Ceph Object Gateway (RGW), you can skip this chapter and migrate your Red Hat Ceph Storage cluster. For more information, see <a href="#ceph-migration_adopt-control-plane">Migrate the Red Hat Ceph Storage cluster</a>. If you are not planning to migrate Ceph daemons from Controller nodes, you must perform the steps that are described in <a href="#deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">Deploying a Ceph ingress daemon</a> and <a href="#updating-the-object-storage-endpoints_migrating-ceph-rgw">Create or update the Object Storage service endpoints</a>.</p>
</div>
<div class="paragraph">
<p>The data migration happens replica by replica. For example, if you have 3 replicas, move them one at a time to ensure that the other 2 replicas are still operational, which enables you to continue to use the Object Storage service during the migration.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Data migration to the new deployment is a long-running process that executes mostly in the background. The Object Storage service replicators move data from old to new nodes, which might take a long time depending on the amount of storage used. To reduce downtime, you can use the old nodes if they are running and continue with adopting other services while waiting for the migration to complete. Performance might be degraded due to the amount of replication traffic in the network.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service">6.1. Migrating the Object Storage service data from RHOSP to RHOSO nodes</h3>
<div class="paragraph _abstract">
<p>The Object Storage service (swift) migration involves the following steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Add new nodes to the Object Storage service rings.</p>
</li>
<li>
<p>Set weights of existing nodes to 0.</p>
</li>
<li>
<p>Rebalance rings by moving one replica.</p>
</li>
<li>
<p>Copy rings to old nodes and restart services.</p>
</li>
<li>
<p>Check replication status and repeat the previous two steps until the old nodes are drained.</p>
</li>
<li>
<p>Remove the old nodes from the rings.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Adopt the Object Storage service. For more information, see <a href="#adopting-the-object-storage-service_adopt-control-plane">Adopting the Object Storage service</a>.</p>
</li>
<li>
<p>For DNS servers, ensure that all existing nodes are able to resolve the hostnames of the Red Hat OpenShift Container Platform (RHOCP) pods, for example, by using the external IP of the DNSMasq service as the nameserver in <code>/etc/resolv.conf</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc get service dnsmasq-dns -o jsonpath="{.status.loadBalancer.ingress[0].ip}" | $CONTROLLER1_SSH sudo tee /etc/resolv.conf</pre>
</div>
</div>
</li>
<li>
<p>Track the current status of the replication by using the <code>swift-dispersion</code> tool:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-dispersion-populate'</pre>
</div>
</div>
<div class="paragraph">
<p>The command might need a few minutes to complete. It creates 0-byte objects that are distributed across the Object Storage service deployment, and you can use the <code>swift-dispersion-report</code> afterward to show the current replication status:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-dispersion-report'</pre>
</div>
</div>
<div class="paragraph">
<p>The output of the <code>swift-dispersion-report</code> command looks similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Queried 1024 containers for dispersion reporting, 5s, 0 retries
100.00% of container copies found (3072 of 3072)
Sample represents 100.00% of the container partition space
Queried 1024 objects for dispersion reporting, 4s, 0 retries
There were 1024 partitions missing 0 copies.
100.00% of object copies found (3072 of 3072)
Sample represents 100.00% of the object partition space</pre>
</div>
</div>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add new nodes by scaling up the SwiftStorage resource from 0 to 3:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc patch openstackcontrolplane openstack --type=merge -p='{"spec":{"swift":{"template":{"swiftStorage":{"replicas": 3}}}}}'</pre>
</div>
</div>
<div class="paragraph">
<p>This command creates three storage instances on the Red Hat OpenShift Container Platform (RHOCP) cluster that use Persistent Volume Claims.</p>
</div>
</li>
<li>
<p>Wait until all three pods are running and the rings include the new devices:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc wait pods --for condition=Ready -l component=swift-storage
$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-ring-builder object.builder search --device pv'</pre>
</div>
</div>
</li>
<li>
<p>From the current rings, get the storage management IP addresses of the nodes to drain:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-ring-builder object.builder search _' | tail -n +2 | awk '{print $4}' | sort -u</pre>
</div>
</div>
<div class="paragraph">
<p>The output looks similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.20.0.100
swift-storage-0.swift-storage.openstack.svc
swift-storage-1.swift-storage.openstack.svc
swift-storage-2.swift-storage.openstack.svc</pre>
</div>
</div>
</li>
<li>
<p>Drain the old nodes. In the following example, the old node <code>172.20.0.100</code> is drained:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
&gt; swift-ring-tool get
&gt; swift-ring-tool drain 172.20.0.100
&gt; swift-ring-tool rebalance
&gt; swift-ring-tool push'</pre>
</div>
</div>
<div class="paragraph">
<p>Depending on your deployment, you might have more nodes to include in the command.</p>
</div>
</li>
<li>
<p>Copy and apply the updated rings to the original nodes. Run the
ssh commands for your existing nodes that store the Object Storage service data:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc extract --confirm cm/swift-ring-files
$ $CONTROLLER1_SSH "tar -C /var/lib/config-data/puppet-generated/swift/etc/swift/ -xzf -" &lt; swiftrings.tar.gz
$ $CONTROLLER1_SSH "systemctl restart tripleo_swift_*"</pre>
</div>
</div>
</li>
<li>
<p>Track the replication progress by using the <code>swift-dispersion-report</code> tool:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c "swift-ring-tool get &amp;&amp; swift-dispersion-report"</pre>
</div>
</div>
<div class="paragraph">
<p>The output shows less than 100% of copies found. Repeat the command until all container and object copies are found:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Queried 1024 containers for dispersion reporting, 6s, 0 retries
There were 5 partitions missing 1 copy.
99.84% of container copies found (3067 of 3072)
Sample represents 100.00% of the container partition space
Queried 1024 objects for dispersion reporting, 7s, 0 retries
There were 739 partitions missing 1 copy.
There were 285 partitions missing 0 copies.
75.94% of object copies found (2333 of 3072)
Sample represents 100.00% of the object partition space</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The rebalance command moves only one replica at a time to ensure that data is available continuously. This requires running the rebalance command multiple times to complete the full rebalance operation.
Additionally, a minimum wait time of one hour between consecutive rebalance commands is
enforced to prevent moving multiple replicas at the same time. Running the
rebalance again before this period elapses has no effect.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Move the next replica to the new nodes by rebalancing and distributing the rings:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
&gt; swift-ring-tool get
&gt; swift-ring-tool rebalance
&gt; swift-ring-tool push'

$ oc extract --confirm cm/swift-ring-files
$ $CONTROLLER1_SSH "tar -C /var/lib/config-data/puppet-generated/swift/etc/swift/ -xzf -" &lt; swiftrings.tar.gz
$ $CONTROLLER1_SSH "systemctl restart tripleo_swift_*"</pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the <code>swift-dispersion-report</code> output again, wait until all copies are found, and then repeat this step until all your replicas are moved to the new nodes.</p>
</div>
</li>
<li>
<p>Remove the nodes from the rings:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c '
&gt; swift-ring-tool get
&gt; swift-ring-tool remove 172.20.0.100
&gt; swift-ring-tool rebalance
&gt; swift-ring-tool push'</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Even if all replicas are on the new nodes and the <code>swift-dispersion-report</code> command reports 100% of the copies found, there might still be data on the old nodes. The replicators remove this data, but it might take more time.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Check the disk usage of all disks in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-recon -d'</pre>
</div>
</div>
</li>
<li>
<p>Confirm that there are no more <code>\*.db</code> or <code>*.data</code> files in the <code>/srv/node</code> directory on the nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$CONTROLLER1_SSH "find /srv/node/ -type f -name '*.db' -o -name '*.data' | wc -l"</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="troubleshooting-object-storage-migration_migrate-object-storage-service">6.2. Troubleshooting the Object Storage service migration</h3>
<div class="paragraph _abstract">
<p>You can troubleshoot issues with the Object Storage service (swift) migration.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the replication is not working and the <code>swift-dispersion-report</code> is not back to 100% availability, check the replicator progress to help you debug:</p>
<div class="listingblock">
<div class="content">
<pre>$ CONTROLLER1_SSH tail /var/log/containers/swift/swift.log | grep object-server</pre>
</div>
</div>
<div class="paragraph">
<p>The following shows an example of the output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Mar 14 06:05:30 standalone object-server[652216]: &lt;f+++++++++ 4e2/9cbea55c47e243994b0b10d8957184e2/1710395823.58025.data
Mar 14 06:05:30 standalone object-server[652216]: Successful rsync of /srv/node/vdd/objects/626/4e2 to swift-storage-1.swift-storage.openstack.svc::object/d1/objects/626 (0.094)
Mar 14 06:05:30 standalone object-server[652216]: Removing partition: /srv/node/vdd/objects/626
Mar 14 06:05:31 standalone object-server[652216]: &lt;f+++++++++ 85f/cf53b5a048e5b19049e05a548cde185f/1710395796.70868.data
Mar 14 06:05:31 standalone object-server[652216]: Successful rsync of /srv/node/vdb/objects/829/85f to swift-storage-2.swift-storage.openstack.svc::object/d1/objects/829 (0.095)
Mar 14 06:05:31 standalone object-server[652216]: Removing partition: /srv/node/vdb/objects/829</pre>
</div>
</div>
</li>
<li>
<p>You can also check the ring consistency and replicator status:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc debug --keep-labels=true job/swift-ring-rebalance -- /bin/sh -c 'swift-ring-tool get &amp;&amp; swift-recon -r --md5'</pre>
</div>
</div>
<div class="paragraph">
<p>The output might show a md5 mismatch until approximately 2 minutes after pushing the new rings. After the 2 minutes, the output looks similar to the following example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Oldest completion was 2024-03-14 16:53:27 (3 minutes ago) by 172.20.0.100:6000.
Most recent completion was 2024-03-14 16:56:38 (12 seconds ago) by swift-storage-0.swift-storage.openstack.svc:6200.
===============================================================================
[2024-03-14 16:56:50] Checking ring md5sums
4/4 hosts matched, 0 error[s] while checking hosts.</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="ceph-migration_hsm-integration">7. Migrating the Red Hat Ceph Storage cluster</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>In the context of data plane adoption, where the Red&#160;Hat OpenStack Platform
(RHOSP) services are redeployed in Red Hat OpenShift Container Platform (RHOCP), you migrate a
director-deployed Red Hat Ceph Storage cluster by using a process
called externalizing the Red Hat Ceph Storage cluster.</p>
</div>
<div class="paragraph">
<p>There are two deployment topologies that include an internal Red Hat Ceph Storage
cluster:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>RHOSP includes dedicated Red Hat Ceph Storage nodes to host object
storage daemons (OSDs)</p>
</li>
<li>
<p>Hyperconverged Infrastructure (HCI), where Compute and Storage services are
colocated on hyperconverged nodes</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In either scenario, there are some Red Hat Ceph Storage processes that are deployed on
RHOSP Controller nodes: Red Hat Ceph Storage monitors, Ceph Object Gateway (RGW),
Rados Block Device (RBD), Ceph Metadata Server (MDS), Ceph Dashboard, and NFS
Ganesha. To migrate your Red Hat Ceph Storage cluster, you must decommission the
Controller nodes and move the Red Hat Ceph Storage daemons to a set of target nodes that are
already part of the Red Hat Ceph Storage cluster.</p>
</div>
<div class="paragraph">
<p>Before you begin the migration, complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</div>
<div class="sect2">
<h3 id="ceph-daemon-cardinality_migrating-ceph">7.1. Red Hat Ceph Storage daemon cardinality</h3>
<div class="paragraph _abstract">
<p>Red Hat Ceph Storage 7 and later applies strict constraints in the way daemons can be colocated within the same node.
For more information, see the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.
Your topology depends on the available hardware and the amount of Red Hat Ceph Storage services in the Controller nodes that you retire.
The amount of services that you can migrate depends on the amount of available nodes in the cluster. The following diagrams show the distribution of Red Hat Ceph Storage daemons on Red Hat Ceph Storage nodes where at least 3 nodes are required.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The following scenario includes only RGW and RBD, without the Red Hat Ceph Storage dashboard:</p>
<div class="listingblock">
<div class="content">
<pre>|    |                     |             |
|----|---------------------|-------------|
| osd | mon/mgr/crash      | rgw/ingress |
| osd | mon/mgr/crash      | rgw/ingress |
| osd | mon/mgr/crash      | rgw/ingress |</pre>
</div>
</div>
</li>
<li>
<p>With the Red Hat Ceph Storage dashboard, but without Shared File Systems service (manila), at least 4 nodes are required. The Red Hat Ceph Storage dashboard has no failover:</p>
<div class="listingblock">
<div class="content">
<pre>|     |                     |             |
|-----|---------------------|-------------|
| osd | mon/mgr/crash | rgw/ingress       |
| osd | mon/mgr/crash | rgw/ingress       |
| osd | mon/mgr/crash | dashboard/grafana |
| osd | rgw/ingress   | (free)            |</pre>
</div>
</div>
</li>
<li>
<p>With the Red Hat Ceph Storage dashboard and the Shared File Systems service, a minimum of 5 nodes are required, and the Red Hat Ceph Storage dashboard has no failover:</p>
<div class="listingblock">
<div class="content">
<pre>|     |                     |                         |
|-----|---------------------|-------------------------|
| osd | mon/mgr/crash       | rgw/ingress             |
| osd | mon/mgr/crash       | rgw/ingress             |
| osd | mon/mgr/crash       | mds/ganesha/ingress     |
| osd | rgw/ingress         | mds/ganesha/ingress     |
| osd | mds/ganesha/ingress | dashboard/grafana       |</pre>
</div>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-monitoring_migrating-ceph">7.2. Migrating the monitoring stack component to new nodes within an existing Red Hat Ceph Storage cluster</h3>
<div class="paragraph _abstract">
<p>The Red Hat Ceph Storage Dashboard module adds web-based monitoring and administration to the
Ceph Manager. With director-deployed Red Hat Ceph Storage, the Red Hat Ceph Storage Dashboard is enabled as part of the overcloud deploy and is composed of the following components:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Ceph Manager module</p>
</li>
<li>
<p>Grafana</p>
</li>
<li>
<p>Prometheus</p>
</li>
<li>
<p>Alertmanager</p>
</li>
<li>
<p>Node exporter</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The Red Hat Ceph Storage Dashboard containers are included through <code>tripleo-container-image-prepare</code> parameters, and high availability (HA) relies
on <code>HAProxy</code> and <code>Pacemaker</code> to be deployed on the Red&#160;Hat OpenStack Platform (RHOSP) environment. For an external Red Hat Ceph Storage cluster, HA is not supported.</p>
</div>
<div class="paragraph">
<p>You migrate and relocate the Ceph Monitoring components to free Controller nodes. Before you begin the migration, complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</div>
<div class="sect3">
<h4 id="migrating-monitoring-stack-to-target-nodes_migrating-ceph-monitoring">7.2.1. Migrating the monitoring stack to the target nodes</h4>
<div class="paragraph _abstract">
<p>To migrate the monitoring stack to the target nodes, you add the monitoring label to your existing nodes and update the configuration of each daemon. You do not need to migrate node exporters. These daemons are deployed across
the nodes that are part of the Red Hat Ceph Storage cluster (the placement is *).</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Depending on the target nodes and the number of deployed or active daemons, you can either relocate the existing containers to the target nodes, or
select a subset of nodes that host the monitoring stack daemons. High availability (HA) is not supported. Reducing the placement with <code>count: 1</code>  allows you to migrate the existing daemons in a Hyperconverged Infrastructure, or hardware-limited, scenario without impacting other services.
</td>
</tr>
</table>
</div>
<div class="sect4">
<h5 id="migrating-existing-daemons-to-target-nodes_migrating-monitoring-stack">Migrating the existing daemons to the target nodes</h5>
<div class="paragraph _abstract">
<p>The following procedure is an example of an environment with 3 Red Hat Ceph Storage nodes or ComputeHCI nodes. This scenario extends the monitoring labels to all the Red Hat Ceph Storage or ComputeHCI nodes that are part of the cluster. This means that you keep 3 placements for the target nodes.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Confirm that the firewall rules are in place and the ports are open for a given monitoring stack service.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the monitoring label to all the Red Hat Ceph Storage or ComputeHCI nodes in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item monitoring;
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that all the hosts on the target nodes have the monitoring label:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ sudo cephadm shell -- ceph orch host ls

HOST                        ADDR           LABELS
cephstorage-0.redhat.local  192.168.24.11  osd monitoring
cephstorage-1.redhat.local  192.168.24.12  osd monitoring
cephstorage-2.redhat.local  192.168.24.47  osd monitoring
controller-0.redhat.local   192.168.24.35  _admin mon mgr monitoring
controller-1.redhat.local   192.168.24.53  mon _admin mgr monitoring
controller-2.redhat.local   192.168.24.10  mon _admin mgr monitoring</pre>
</div>
</div>
</li>
<li>
<p>Remove the labels from the Controller nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ for i in 0 1 2; do sudo cephadm shell -- ceph orch host label rm "controller-$i.redhat.local" monitoring; done

Removed label monitoring from host controller-0.redhat.local
Removed label monitoring from host controller-1.redhat.local
Removed label monitoring from host controller-2.redhat.local</pre>
</div>
</div>
</li>
<li>
<p>Dump the current monitoring stack spec:</p>
<div class="listingblock">
<div class="content">
<pre>function export_spec {
    local component="$1"
    local target_dir="$2"
    sudo cephadm shell -- ceph orch ls --export "$component" &gt; "$target_dir/$component"
}

SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
mkdir -p ${SPEC_DIR}
for m in grafana prometheus alertmanager; do
    export_spec "$m" "$SPEC_DIR"
done</pre>
</div>
</div>
</li>
<li>
<p>For each daemon, edit the current spec and replace the <code>placement.hosts:</code> section with the <code>placement.label:</code> section, for example:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: grafana
service_name: grafana
placement:
  label: monitoring
networks:
- 172.17.3.0/24
spec:
  port: 3100</code></pre>
</div>
</div>
<div class="paragraph">
<p>This step also applies to Prometheus and Alertmanager specs.</p>
</div>
</li>
<li>
<p>Apply the new monitoring spec to relocate the monitoring stack daemons:</p>
<div class="listingblock">
<div class="content">
<pre>SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
function migrate_daemon {
    local component="$1"
    local target_dir="$2"
    sudo cephadm shell -m "$target_dir" -- ceph orch apply -i /mnt/ceph_specs/$component
}
for m in grafana prometheus alertmanager; do
    migrate_daemon  "$m" "$SPEC_DIR"
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that the daemons are deployed on the expected nodes:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ps | grep -iE "(prome|alert|grafa)"
alertmanager.cephstorage-2  cephstorage-2.redhat.local  172.17.3.144:9093,9094
grafana.cephstorage-0       cephstorage-0.redhat.local  172.17.3.83:3100
prometheus.cephstorage-1    cephstorage-1.redhat.local  172.17.3.53:9092</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
After you migrate the monitoring stack, you lose high availability. The monitoring stack daemons no longer have a Virtual IP address and HAProxy anymore. Node exporters are still running on all the nodes.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Review the Red Hat Ceph Storage configuration to ensure that it aligns with the configuration on the target nodes. In particular, focus on the following configuration entries:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump | grep -i dashboard
...
mgr  advanced  mgr/dashboard/ALERTMANAGER_API_HOST  http://172.17.3.83:9093
mgr  advanced  mgr/dashboard/GRAFANA_API_URL        https://172.17.3.144:3100
mgr  advanced  mgr/dashboard/PROMETHEUS_API_HOST    http://172.17.3.83:9092
mgr  advanced  mgr/dashboard/controller-0.ycokob/server_addr  172.17.3.33
mgr  advanced  mgr/dashboard/controller-1.lmzpuc/server_addr  172.17.3.147
mgr  advanced  mgr/dashboard/controller-2.xpdgfl/server_addr  172.17.3.138</pre>
</div>
</div>
</li>
<li>
<p>Verify that the <code>API_HOST/URL</code> of the <code>grafana</code>, <code>alertmanager</code> and <code>prometheus</code> services points to the IP addresses on the storage network of the node where each daemon is relocated:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch ps | grep -iE "(prome|alert|grafa)"
alertmanager.cephstorage-0  cephstorage-0.redhat.local  172.17.3.83:9093,9094
alertmanager.cephstorage-1  cephstorage-1.redhat.local  172.17.3.53:9093,9094
alertmanager.cephstorage-2  cephstorage-2.redhat.local  172.17.3.144:9093,9094
grafana.cephstorage-0       cephstorage-0.redhat.local  172.17.3.83:3100
grafana.cephstorage-1       cephstorage-1.redhat.local  172.17.3.53:3100
grafana.cephstorage-2       cephstorage-2.redhat.local  172.17.3.144:3100
prometheus.cephstorage-0    cephstorage-0.redhat.local  172.17.3.83:9092
prometheus.cephstorage-1    cephstorage-1.redhat.local  172.17.3.53:9092
prometheus.cephstorage-2    cephstorage-2.redhat.local  172.17.3.144:9092</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump
...
...
mgr  advanced  mgr/dashboard/ALERTMANAGER_API_HOST   http://172.17.3.83:9093
mgr  advanced  mgr/dashboard/PROMETHEUS_API_HOST     http://172.17.3.83:9092
mgr  advanced  mgr/dashboard/GRAFANA_API_URL         https://172.17.3.144:3100</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The Ceph Dashboard, as the service  provided by the Ceph <code>mgr</code>, is not impacted by the relocation. You might experience an impact when the active <code>mgr</code> daemon is migrated or is force-failed. However, you can define 3 replicas in the Ceph Manager configuration to redirect requests to a different instance.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-mds_migrating-ceph-monitoring">7.3. Migrating Red Hat Ceph Storage MDS to new nodes within the existing cluster</h3>
<div class="paragraph _abstract">
<p>You can migrate the MDS daemon when Shared File Systems service (manila), deployed with either a cephfs-native or ceph-nfs back end, is part of the overcloud deployment. The MDS migration is performed by <code>cephadm</code>, and you move the daemons placement from a hosts-based approach to a label-based approach.
This ensures that you can visualize the status of the cluster and where daemons are placed by using the <code>ceph orch host</code> command. You can also have a general view of how the daemons are co-located within a given host, as described in the Red Hat Knowledgebase article <a href="https://access.redhat.com/articles/1548993">Red Hat Ceph Storage: Supported configurations</a>.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>Complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the Red Hat Ceph Storage cluster is healthy and check the MDS status:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs ls
name: cephfs, metadata pool: manila_metadata, data pools: [manila_data ]

$ sudo cephadm shell -- ceph mds stat
cephfs:1 {0=mds.controller-2.oebubl=up:active} 2 up:standby

$ sudo cephadm shell -- ceph fs status cephfs

cephfs - 0 clients
======
RANK  STATE         	MDS           	ACTIVITY 	DNS	INOS   DIRS   CAPS
 0	active  mds.controller-2.oebubl  Reqs:	0 /s   696	196	173  	0
  	POOL     	TYPE 	USED  AVAIL
manila_metadata  metadata   152M   141G
  manila_data  	data	3072M   141G
  	STANDBY MDS
mds.controller-0.anwiwd
mds.controller-1.cwzhog</pre>
</div>
</div>
</li>
<li>
<p>Retrieve more detailed information on the Ceph File System (CephFS) MDS status:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs dump

e8
enable_multiple, ever_enabled_multiple: 1,1
default compat: compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,8=no anchor table,9=file layout v2,10=snaprealm v2}
legacy client fscid: 1

Filesystem 'cephfs' (1)
fs_name cephfs
epoch   5
flags   12 joinable allow_snaps allow_multimds_snaps
created 2024-01-18T19:04:01.633820+0000
modified    	2024-01-18T19:04:05.393046+0000
tableserver 	0
root	0
session_timeout 60
session_autoclose   	300
max_file_size   1099511627776
required_client_features    	{}
last_failure	0
last_failure_osd_epoch  0
compat  compat={},rocompat={},incompat={1=base v0.20,2=client writeable ranges,3=default file layouts on dirs,4=dir inode in separate object,5=mds uses versioned encoding,6=dirfrag is stored in omap,7=mds uses inline data,8=no anchor table,9=file layout v2,10=snaprealm v2}
max_mds 1
in  	0
up  	{0=24553}
failed
damaged
stopped
data_pools  	[7]
metadata_pool   9
inline_data 	disabled
balancer
standby_count_wanted	1
[mds.mds.controller-2.oebubl{0:24553} state up:active seq 2 addr [v2:172.17.3.114:6800/680266012,v1:172.17.3.114:6801/680266012] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.controller-0.anwiwd{-1:14715} state up:standby seq 1 addr [v2:172.17.3.20:6802/3969145800,v1:172.17.3.20:6803/3969145800] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-1.cwzhog{-1:24566} state up:standby seq 1 addr [v2:172.17.3.43:6800/2227381308,v1:172.17.3.43:6801/2227381308] compat {c=[1],r=[1],i=[7ff]}]
dumped fsmap epoch 8</pre>
</div>
</div>
</li>
<li>
<p>Check the OSD blocklist and clean up the client list:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph osd blocklist ls
$ for item in $(sudo cephadm shell -- ceph osd blocklist ls | awk '{print $1}'); do
&gt;     sudo cephadm shell -- ceph osd blocklist rm $item;
&gt; done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When a file system client is unresponsive or misbehaving, the access to the file system might be forcibly terminated. This process is called eviction. Evicting a CephFS client prevents it from communicating further with MDS daemons and OSD daemons.</p>
</div>
<div class="paragraph">
<p>Ordinarily, a blocklisted client cannot reconnect to the servers; you must unmount and then remount the client. However, permitting a client that was evicted to attempt to reconnect can be useful. Because CephFS uses the RADOS OSD blocklist to control client eviction, you can permit CephFS clients to reconnect by removing them from the blocklist.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Get the hosts that are currently part of the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph orch host ls
HOST                        ADDR           LABELS          STATUS
cephstorage-0.redhat.local  192.168.24.25  osd
cephstorage-1.redhat.local  192.168.24.50  osd
cephstorage-2.redhat.local  192.168.24.47  osd
controller-0.redhat.local   192.168.24.24  _admin mgr mon
controller-1.redhat.local   192.168.24.42  mgr _admin mon
controller-2.redhat.local   192.168.24.37  mgr _admin mon
6 hosts in cluster</pre>
</div>
</div>
</li>
<li>
<p>Apply the MDS labels to the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item mds;
done</pre>
</div>
</div>
</li>
<li>
<p>Verify that all the hosts have the MDS label:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host ls

HOST                    	ADDR       	   LABELS
cephstorage-0.redhat.local  192.168.24.11  osd mds
cephstorage-1.redhat.local  192.168.24.12  osd mds
cephstorage-2.redhat.local  192.168.24.47  osd mds
controller-0.redhat.local   192.168.24.35  _admin mon mgr mds
controller-1.redhat.local   192.168.24.53  mon _admin mgr mds
controller-2.redhat.local   192.168.24.10  mon _admin mgr mds</pre>
</div>
</div>
</li>
<li>
<p>Dump the current MDS spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export mds &gt; ${SPEC_DIR}/mds</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and replace the <code>placement.hosts</code> section with
<code>placement.label</code>:</p>
<div class="listingblock">
<div class="content">
<pre>service_type: mds
service_id: mds
service_name: mds.mds
placement:
  label: mds</pre>
</div>
</div>
</li>
<li>
<p>Use the <code>ceph orchestrator</code> to apply the new MDS spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mds -- ceph orch apply -i /mnt/mds

Scheduling new mds deployment ...</pre>
</div>
</div>
<div class="paragraph">
<p>This results in an increased number of MDS daemons.</p>
</div>
</li>
<li>
<p>Check the new standby daemons that are temporarily added to the CephFS:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs dump

Active

standby_count_wanted    1
[mds.mds.controller-0.awzplm{0:463158} state up:active seq 307 join_fscid=1 addr [v2:172.17.3.20:6802/51565420,v1:172.17.3.20:6803/51565420] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.cephstorage-1.jkvomp{-1:463800} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.135:6820/2075903648,v1:172.17.3.135:6821/2075903648] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-2.gfrqvc{-1:475945} state up:standby seq 1 addr [v2:172.17.3.114:6800/2452517189,v1:172.17.3.114:6801/2452517189] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-0.fqcshx{-1:476503} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.92:6820/4120523799,v1:172.17.3.92:6821/4120523799] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-2.gnfhfe{-1:499067} state up:standby seq 1 addr [v2:172.17.3.79:6820/2448613348,v1:172.17.3.79:6821/2448613348] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.controller-1.tyiziq{-1:499136} state up:standby seq 1 addr [v2:172.17.3.43:6800/3615018301,v1:172.17.3.43:6801/3615018301] compat {c=[1],r=[1],i=[7ff]}]</pre>
</div>
</div>
</li>
<li>
<p>To migrate MDS to the target nodes, set the MDS affinity that manages the MDS failover:</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
It is possible to elect a dedicated MDS as "active" for a particular file system. To configure this preference, <code>CephFS</code> provides a configuration option for MDS called <code>mds_join_fs</code>, which enforces this affinity.
When failing over MDS daemons, cluster monitors prefer standby daemons with <code>mds_join_fs</code> equal to the file system name with the failed rank. If no standby exists with <code>mds_join_fs</code> equal to the file system name, it chooses an unqualified standby as a replacement.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph config set mds.mds.cephstorage-0.fqcshx mds_join_fs cephfs</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>mds.mds.cephstorage-0.fqcshx</code> with the daemon deployed on
<code>cephstorage-0</code> that was retrieved from the previous step.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Remove the labels from the Controller nodes and force the MDS failover to the
target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ for i in 0 1 2; do sudo cephadm shell -- ceph orch host label rm "controller-$i.redhat.local" mds; done

Removed label mds from host controller-0.redhat.local
Removed label mds from host controller-1.redhat.local
Removed label mds from host controller-2.redhat.local</pre>
</div>
</div>
<div class="paragraph">
<p>The switch to the target node happens in the background. The new active MDS is the one that you set by using the <code>mds_join_fs</code> command.</p>
</div>
</li>
<li>
<p>Check the result of the failover and the new deployed daemons:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph fs dump


standby_count_wanted    1
[mds.mds.cephstorage-0.fqcshx{0:476503} state up:active seq 168 join_fscid=1 addr [v2:172.17.3.92:6820/4120523799,v1:172.17.3.92:6821/4120523799] compat {c=[1],r=[1],i=[7ff]}]


Standby daemons:

[mds.mds.cephstorage-2.gnfhfe{-1:499067} state up:standby seq 1 addr [v2:172.17.3.79:6820/2448613348,v1:172.17.3.79:6821/2448613348] compat {c=[1],r=[1],i=[7ff]}]
[mds.mds.cephstorage-1.jkvomp{-1:499760} state up:standby seq 1 join_fscid=1 addr [v2:172.17.3.135:6820/452139733,v1:172.17.3.135:6821/452139733] compat {c=[1],r=[1],i=[7ff]}]


$ sudo cephadm shell -- ceph orch ls

NAME                     PORTS   RUNNING  REFRESHED  AGE  PLACEMENT
crash                                6/6  10m ago    10d  *
mds.mds                          3/3  10m ago    32m  label:mds


$ sudo cephadm shell -- ceph orch ps | grep mds


mds.mds.cephstorage-0.fqcshx  cephstorage-0.redhat.local                     running (79m)     3m ago  79m    27.2M        -  17.2.6-100.el9cp  1af7b794f353  2a2dc5ba6d57
mds.mds.cephstorage-1.jkvomp  cephstorage-1.redhat.local                     running (79m)     3m ago  79m    21.5M        -  17.2.6-100.el9cp  1af7b794f353  7198b87104c8
mds.mds.cephstorage-2.gnfhfe  cephstorage-2.redhat.local                     running (79m)     3m ago  79m    24.2M        -  17.2.6-100.el9cp  1af7b794f353  f3cb859e2a15</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-rgw_migrating-ceph-monitoring">7.4. Migrating Red Hat Ceph Storage RGW to external RHEL nodes</h3>
<div class="paragraph _abstract">
<p>For Hyperconverged Infrastructure (HCI) or dedicated Storage nodes, you must migrate the Ceph Object Gateway (RGW) daemons that are included in the Red&#160;Hat OpenStack Platform Controller nodes into the existing external Red Hat Enterprise Linux (RHEL) nodes. The external RHEL nodes typically include the Compute nodes for an HCI environment or Red Hat Ceph Storage nodes. Your environment must have Red Hat Ceph Storage 7 or later and be managed by <code>cephadm</code> or Ceph Orchestrator.</p>
</div>
<div class="paragraph">
<p>Before you begin the migration, complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</div>
<div class="sect3">
<h4 id="migrating-the-rgw-backends_migrating-ceph-rgw">7.4.1. Migrating the Red Hat Ceph Storage RGW back ends</h4>
<div class="paragraph _abstract">
<p>You must migrate your Ceph Object Gateway (RGW) back ends from your Controller nodes to your Red Hat Ceph Storage nodes. To ensure that you distribute the correct amount of services to your available nodes, you use <code>cephadm</code> labels to refer to a group of nodes where a given daemon type is deployed. For more information about the cardinality diagram, see <a href="#ceph-daemon-cardinality_migrating-ceph">Red Hat Ceph Storage daemon cardinality</a>.
The following procedure assumes that you have three target nodes, <code>cephstorage-0</code>, <code>cephstorage-1</code>, <code>cephstorage-2</code>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Add the RGW label to the Red Hat Ceph Storage nodes that you want to migrate your RGW back ends to:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host label add cephstorage-0 rgw;
$ sudo cephadm shell -- ceph orch host label add cephstorage-1 rgw;
$ sudo cephadm shell -- ceph orch host label add cephstorage-2 rgw;

Added label rgw to host cephstorage-0
Added label rgw to host cephstorage-1
Added label rgw to host cephstorage-2

$ sudo cephadm shell -- ceph orch host ls

HOST       	ADDR       	LABELS      	STATUS
cephstorage-0  192.168.24.54  osd rgw
cephstorage-1  192.168.24.44  osd rgw
cephstorage-2  192.168.24.30  osd rgw
controller-0   192.168.24.45  _admin mon mgr
controller-1   192.168.24.11  _admin mon mgr
controller-2   192.168.24.38  _admin mon mgr

6 hosts in cluster</pre>
</div>
</div>
</li>
<li>
<p>Locate the RGW spec and dump in the spec directory:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export rgw &gt; ${SPEC_DIR}/rgw
$ cat ${SPEC_DIR}/rgw</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>networks:
- 172.17.3.0/24
placement:
  hosts:
  - controller-0
  - controller-1
  - controller-2
service_id: rgw
service_name: rgw.rgw
service_type: rgw
spec:
  rgw_frontend_port: 8080
  rgw_realm: default
  rgw_zone: default</pre>
</div>
</div>
<div class="paragraph">
<p>This example assumes that <code>172.17.3.0/24</code> is the <code>storage</code> network.</p>
</div>
</li>
<li>
<p>In the <code>placement</code> section, ensure that the <code>label</code> and <code>rgw_frontend_port</code> values are set:</p>
<div class="listingblock">
<div class="content">
<pre>---
networks:
- 172.17.3.0/24
placement:
  label: rgw
service_id: rgw
service_name: rgw.rgw
service_type: rgw
spec:
  rgw_frontend_port: 8090
  rgw_realm: default
  rgw_zone: default
  rgw_frontend_ssl_certificate: ...
  ssl: true</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>networks</code> defines the storage network where the RGW back ends are deployed.</p>
</li>
<li>
<p><code>placement.label: rgw</code> replaces the Controller nodes with the <code>rgw</code> label.</p>
</li>
<li>
<p><code>spec.rgw_frontend_port</code> specifies the value as <code>8090</code> to avoid conflicts with the Ceph ingress daemon.</p>
</li>
<li>
<p><code>spec.rgw_frontend_ssl_certificate</code> defines the SSL certificate and key concatenation if TLS is enabled as described in <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_persistent_storage/assembly_configuring-red-hat-ceph-storage-as-the-backend-for-rhosp-storage#proc_ceph-configure-rgw-with-tls_ceph-back-end">Configuring RGW with TLS for an external Red Hat Ceph Storage cluster</a> in <em>Configuring persistent storage</em>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the new RGW spec by using the orchestrator CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/rgw -- ceph orch apply -i /mnt/rgw</pre>
</div>
</div>
<div class="paragraph">
<p>This command triggers the redeploy, for example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>...
osd.9                     	cephstorage-2
rgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090   starting
rgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090   starting
rgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090   starting
rgw.rgw.controller-1.eyvrzw   controller-1   172.17.3.146:8080  running (5h)
rgw.rgw.controller-2.navbxa   controller-2   172.17.3.66:8080   running (5h)

...
osd.9                     	cephstorage-2
rgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090  running (19s)
rgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090  running (16s)
rgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090  running (13s)</pre>
</div>
</div>
</li>
<li>
<p>Ensure that the new RGW back ends are reachable on the new ports, so you can enable an ingress daemon on port <code>8080</code> later. Log in to each Red Hat Ceph Storage node that includes RGW and add the <code>iptables</code> rule to allow connections to both 8080 and 8090 ports in the Red Hat Ceph Storage nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ iptables -I INPUT -p tcp -m tcp --dport 8080 -m conntrack --ctstate NEW -m comment --comment "ceph rgw ingress" -j ACCEPT
$ iptables -I INPUT -p tcp -m tcp --dport 8090 -m conntrack --ctstate NEW -m comment --comment "ceph rgw backends" -j ACCEPT
$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
</li>
<li>
<p>If <code>nftables</code> is used in the existing deployment, edit <code>/etc/nftables/tripleo-rules.nft</code>
and add the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># 100 ceph_rgw {'dport': ['8080','8090']}
add rule inet filter TRIPLEO_INPUT tcp dport { 8080,8090 } ct state new counter accept comment "100 ceph_rgw"</code></pre>
</div>
</div>
</li>
<li>
<p>Save the file.</p>
</li>
<li>
<p>Restart the <code>nftables</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl restart nftables</pre>
</div>
</div>
</li>
<li>
<p>Verify that the rules are applied:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo nft list ruleset | grep ceph_rgw</pre>
</div>
</div>
</li>
<li>
<p>From a Controller node, such as <code>controller-0</code>, try to reach the RGW back ends:</p>
<div class="listingblock">
<div class="content">
<pre>$ curl http://cephstorage-0.storage:8090;</pre>
</div>
</div>
<div class="paragraph">
<p>You should observe the following output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;</pre>
</div>
</div>
<div class="paragraph">
<p>Repeat the verification for each node where a RGW daemon is deployed.</p>
</div>
</li>
<li>
<p>If you migrated RGW back ends to the Red Hat Ceph Storage nodes, there is no <code>internalAPI</code> network, except in the case of HCI nodes. You must reconfigure the RGW keystone endpoint to point to the external network that you propagated:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config dump | grep keystone
global   basic rgw_keystone_url  http://172.16.1.111:5000

[ceph: root@controller-0 /]# ceph config set global rgw_keystone_url http://&lt;keystone_endpoint&gt;:5000</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;keystone_endpoint&gt;</code> with the Identity service (keystone) internal endpoint of the service that is deployed in the <code>OpenStackControlPlane</code> CR when you adopt the Identity service. For more information, see <a href="#adopting-the-identity-service_adopt-control-plane">Adopting the Identity service</a>.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="deploying-a-ceph-ingress-daemon_migrating-ceph-rgw">7.4.2. Deploying a Red Hat Ceph Storage ingress daemon</h4>
<div class="paragraph _abstract">
<p>To deploy the Ceph ingress daemon, you perform the following actions:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Remove the existing <code>ceph_rgw</code> configuration.</p>
</li>
<li>
<p>Clean up the configuration created by director.</p>
</li>
<li>
<p>Redeploy the Object Storage service (swift).</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>When you deploy the ingress daemon, two new containers are created:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>HAProxy, which you use to reach the back ends.</p>
</li>
<li>
<p>Keepalived, which you use to own the virtual IP address.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You use the <code>rgw</code> label to distribute the ingress daemon to only the number of nodes that host Ceph Object Gateway (RGW) daemons. For more information about distributing daemons among your nodes, see <a href="#ceph-daemon-cardinality_migrating-ceph">Red Hat Ceph Storage daemon cardinality</a>.</p>
</div>
<div class="paragraph">
<p>After you complete this procedure, you can reach the RGW back end from the ingress daemon and use RGW through the Object Storage service CLI.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Log in to each Controller node and remove the following configuration from the <code>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg</code> file:</p>
<div class="listingblock">
<div class="content">
<pre>listen ceph_rgw
  bind 10.0.0.103:8080 transparent
  mode http
  balance leastconn
  http-request set-header X-Forwarded-Proto https if { ssl_fc }
  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }
  http-request set-header X-Forwarded-Port %[dst_port]
  option httpchk GET /swift/healthcheck
  option httplog
  option forwardfor
   server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2
  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2
  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2</pre>
</div>
</div>
</li>
<li>
<p>Restart <code>haproxy-bundle</code> and confirm that it is started:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 ~]# sudo pcs resource restart haproxy-bundle
haproxy-bundle successfully restarted


[root@controller-0 ~]# sudo pcs status | grep haproxy

  * Container bundle set: haproxy-bundle [undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:
    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-0
    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-1
    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-2</pre>
</div>
</div>
</li>
<li>
<p>Confirm that no process is connected to port 8080:</p>
<div class="listingblock">
<div class="content">
<pre>[root@controller-0 ~]# ss -antop | grep 8080
[root@controller-0 ~]#</pre>
</div>
</div>
<div class="paragraph">
<p>You can expect the Object Storage service (swift) CLI to fail to establish the connection:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>(overcloud) [root@cephstorage-0 ~]# swift list

HTTPConnectionPool(host='10.0.0.103', port=8080): Max retries exceeded with url: /swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fc41beb0430&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))</pre>
</div>
</div>
</li>
<li>
<p>Set the required images for both HAProxy and Keepalived:</p>
<div class="listingblock">
<div class="content">
<pre>[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_haproxy registry.redhat.io/rhceph/rhceph-haproxy-rhel9:latest
[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_keepalived registry.redhat.io/rhceph/keepalived-rhel9:latest</pre>
</div>
</div>
</li>
<li>
<p>Create a file called <code>rgw_ingress</code> in <code>controller-0</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ vim ${SPEC_DIR}/rgw_ingress</pre>
</div>
</div>
</li>
<li>
<p>Paste the following content into the <code>rgw_ingress</code> file:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">---
service_type: ingress
service_id: rgw.rgw
placement:
  label: rgw
spec:
  backend_service: rgw.rgw
  virtual_ip: 10.0.0.89/24
  frontend_port: 8080
  monitor_port: 8898
  virtual_interface_networks:
    - &lt;external_network&gt;
  ssl_cert: ...</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;external_network&gt;</code> with your external network, for example, <code>10.0.0.0/24</code>. For more information, see <a href="#completing-prerequisites-for-migrating-ceph-rgw_ceph-prerequisites">Completing prerequisites for migrating Red Hat Ceph Storage RGW</a>.</p>
</li>
<li>
<p>If TLS is enabled, add the  SSL certificate and key concatenation as described in <a href="https://docs.redhat.com/en/documentation/red_hat_openstack_services_on_openshift/18.0/html/configuring_persistent_storage/assembly_configuring-red-hat-ceph-storage-as-the-backend-for-rhosp-storage#proc_ceph-configure-rgw-with-tls_ceph-back-end">Configuring RGW with TLS for an external Red Hat Ceph Storage cluster</a> in <em>Configuring persistent storage</em>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Apply the <code>rgw_ingress</code> spec by using the Ceph orchestrator CLI:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ cephadm shell -m ${SPEC_DIR}/rgw_ingress -- ceph orch apply -i /mnt/rgw_ingress</pre>
</div>
</div>
</li>
<li>
<p>Wait until the ingress is deployed and query the resulting endpoint:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch ls

NAME                 	PORTS            	RUNNING  REFRESHED  AGE  PLACEMENT
crash                                         	6/6  6m ago 	3d   *
ingress.rgw.rgw      	10.0.0.89:8080,8898  	6/6  37s ago	60s  label:rgw
mds.mds                   3/3  6m ago 	3d   controller-0;controller-1;controller-2
mgr                       3/3  6m ago 	3d   controller-0;controller-1;controller-2
mon                       3/3  6m ago 	3d   controller-0;controller-1;controller-2
osd.default_drive_group   15  37s ago	3d   cephstorage-0;cephstorage-1;cephstorage-2
rgw.rgw   ?:8090          3/3  37s ago	4m   label:rgw</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ curl 10.0.0.89:8080

---
&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[ceph: root@controller-0 /]#
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="updating-the-object-storage-endpoints_migrating-ceph-rgw">7.4.3. Create or update the Object Storage service endpoints</h4>
<div class="paragraph _abstract">
<p>You must create or update the Object Storage service (swift) endpoints to configure the new virtual IP address (VIP) that you reserved on the same network that you used to deploy RGW ingress.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>List the current swift endpoints and service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient openstack endpoint list | grep "swift.*object"</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient openstack service list | grep "swift.*object"</pre>
</div>
</div>
</li>
<li>
<p>If the service and endpoints do not exist, create the missing swift resources:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient openstack service create --name swift --description 'OpenStack Object Storage' object-store
$ oc rsh openstackclient openstack role add --user swift --project service member
$ oc rsh openstackclient openstack role add --user swift --project service admin
&gt; for i in public internal; do
&gt;     oc rsh openstackclient endpoint create --region regionOne  object-store $i http://&lt;RGW_VIP&gt;:8080/swift/v1/AUTH_%\(tenant_id\)s
&gt; done
$ oc rsh openstackclient openstack role add --project admin --user admin swiftoperator</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;RGW_VIP&gt;</code> with the Ceph RGW ingress VIP.</p>
</li>
</ul>
</div>
</li>
<li>
<p>If the endpoints exist, update the endpoints to point to the right RGW ingress VIP:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient openstack endpoint set --url http://&lt;RGW_VIP&gt;:8080/swift/v1/AUTH_%\(tenant_id\)s &lt;swift_public_endpoint_uuid&gt;
$ oc rsh openstackclient openstack endpoint set --url http://&lt;RGW_VIP&gt;:8080/swift/v1/AUTH_%\(tenant_id\)s &lt;swift_internal_endpoint_uuid&gt;
$ oc rsh openstackclient openstack endpoint list | grep object
| 0d682ad71b564cf386f974f90f80de0d | regionOne | swift        | object-store | True    | public    | http://172.18.0.100:8080/swift/v1/AUTH_%(tenant_id)s    |
| b311349c305346f39d005feefe464fb1 | regionOne | swift        | object-store | True    | internal  | http://172.18.0.100:8080/swift/v1/AUTH_%(tenant_id)s    |</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;swift_public_endpoint_uuid&gt;</code> with the UUID of the swift public endpoint.</p>
</li>
<li>
<p>Replace <code>&lt;swift_internal_endpoint_uuid&gt;</code> with the UUID of the swift internal endpoint.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Test the migrated service:</p>
<div class="listingblock">
<div class="content">
<pre>$ oc rsh openstackclient openstack container list --debug

...
...
...
REQ: curl -g -i -X GET http://keystone-public-openstack.apps.ocp.openstack.lab -H "Accept: application/json" -H "User-Agent: openstacksdk/1.0.2 keystoneauth1/5.1.3 python-requests/2.25.1 CPython/3.9.23"
Starting new HTTP connection (1): keystone-public-openstack.apps.ocp.openstack.lab:80
http://keystone-public-openstack.apps.ocp.openstack.lab:80 "GET / HTTP/1.1" 300 298
RESP: [300] content-length: 298 content-type: application/json date: Mon, 14 Jul 2025 17:41:29 GMT location: http://keystone-public-openstack.apps.ocp.openstack.lab/v3/ server: Apache set-cookie: b5697f82cf3c19ece8be533395142512=d5c6a9ee2
267c4b63e9f656ad7565270; path=/; HttpOnly vary: X-Auth-Token x-openstack-request-id: req-452e42c5-e60f-440f-a6e8-fe1b9ea89055
RESP BODY: {"versions": {"values": [{"id": "v3.14", "status": "stable", "updated": "2020-04-07T00:00:00Z", "links": [{"rel": "self", "href": "http://keystone-public-openstack.apps.ocp.openstack.lab/v3/"}], "media-types": [{"base": "applic
ation/json", "type": "application/vnd.openstack.identity-v3+json"}]}]}}
GET call to http://keystone-public-openstack.apps.ocp.openstack.lab/ used request id req-452e42c5-e60f-440f-a6e8-fe1b9ea89055

...

REQ: curl -g -i -X GET "http://172.18.0.100:8080/swift/v1/AUTH_44477474b0dc4b5b8911ceec23a22246?format=json" -H "User-Agent: openstacksdk/1.0.2 keystoneauth1/5.1.3 python-requests/2.25.1 CPython/3.9.23" -H "X-Auth-Token: {SHA256}ec5deca0be37bd8bfe659f132b9cdf396b8f409db5dc16972d50cbf3f28474d4"
Starting new HTTP connection (1): 172.18.0.100:8080
http://172.18.0.100:8080 "GET /swift/v1/AUTH_44477474b0dc4b5b8911ceec23a22246?format=json HTTP/1.1" 200 2
RESP: [200] accept-ranges: bytes content-length: 2 content-type: application/json; charset=utf-8 date: Mon, 14 Jul 2025 17:41:31 GMT x-account-bytes-used: 0 x-account-bytes-used-actual: 0 x-account-container-count: 0 x-account-object-count: 0 x-account-storage-policy-default-placement-bytes-used: 0 x-account-storage-policy-default-placement-bytes-used-actual: 0 x-account-storage-policy-default-placement-container-count: 0 x-account-storage-policy-default-placement-object-count: 0 x-openstack-request-id: tx000001e95361131ccf694-006875414a-7753-default x-timestamp: 1752514891.25991 x-trans-id: tx000001e95361131ccf694-006875414a-7753-default
RESP BODY: []
GET call to http://172.18.0.100:8080/swift/v1/AUTH_44477474b0dc4b5b8911ceec23a22246?format=json used request id tx000001e95361131ccf694-006875414a-7753-default

clean_up ListContainer:
END return value: 0</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="migrating-ceph-rbd_migrating-ceph-monitoring">7.5. Migrating Red Hat Ceph Storage RBD to external RHEL nodes</h3>
<div class="paragraph _abstract">
<p>For Hyperconverged Infrastructure (HCI) or dedicated Storage nodes that are
running Red Hat Ceph Storage 7 or later, you must migrate the daemons that are
included in the Red&#160;Hat OpenStack Platform control plane into the existing external Red
Hat Enterprise Linux (RHEL) nodes. The external RHEL nodes typically include
the Compute nodes for an HCI environment or dedicated storage nodes.</p>
</div>
<div class="paragraph">
<p>Before you begin the migration, complete the tasks in your Red&#160;Hat OpenStack Platform 17.1 environment. For more information, see <a href="#red-hat-ceph-storage-prerequisites_configuring-network">Red Hat Ceph Storage prerequisites</a>.</p>
</div>
<div class="sect3">
<h4 id="migrating-ceph-mgr-daemons-to-ceph-nodes_migrating-ceph-rbd">7.5.1. Migrating Ceph Manager daemons to Red Hat Ceph Storage nodes</h4>
<div class="paragraph _abstract">
<p>You must migrate your Ceph Manager daemons from the Red&#160;Hat OpenStack Platform (RHOSP) Controller nodes to a set of target nodes. Target nodes are either existing Red Hat Ceph Storage nodes, or RHOSP Compute nodes if Red Hat Ceph Storage is deployed by director with a Hyperconverged Infrastructure (HCI) topology.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The following procedure uses <code>cephadm</code> and the Ceph Orchestrator to drive the Ceph Manager migration, and the Ceph spec to modify the placement and reschedule the Ceph Manager daemons. Ceph Manager is run in an active/passive state. It also provides many modules, including the Ceph Orchestrator. Every potential module, such as the Ceph Dashboard, that is provided by <code>ceph-mgr</code> is implicitly migrated with Ceph Manager.
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>SSH into the target node and enable the firewall rules that are required to reach a Ceph Manager service:</p>
<div class="listingblock">
<div class="content">
<pre>dports="6800:7300"
ssh heat-admin@&lt;target_node&gt; sudo iptables -I INPUT \
    -p tcp --match multiport --dports $dports -j ACCEPT;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the hosts that are listed in the Red Hat Ceph Storage environment. Run <code>ceph orch host ls</code> to see the list of the hosts.</p>
<div class="paragraph">
<p>Repeat this step for each target node.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Check that the rules are properly applied to the target node and persist them:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The default dashboard port for <code>ceph-mgr</code> in a greenfield deployment is 8443. With director-deployed Red Hat Ceph Storage, the default port is 8444 because the service ran on the Controller node, and it was necessary to use this port to avoid a conflict. For adoption, update the dashboard port to 8443 in the <code>ceph-mgr</code> configuration and firewall rules.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Log in to <code>controller-0</code> and update the dashboard port in the <code>ceph-mgr</code> configuration to 8443:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell
$ ceph config set mgr mgr/dashboard/server_port 8443
$ ceph config set mgr mgr/dashboard/ssl_server_port 8443
$ ceph mgr module disable dashboard
$ ceph mgr module enable dashboard</pre>
</div>
</div>
</li>
<li>
<p>If <code>nftables</code> is used in the existing deployment, edit <code>/etc/nftables/tripleo-rules.nft</code>
and add the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># 113 ceph_mgr {'dport': ['6800-7300', 8443]}
add rule inet filter TRIPLEO_INPUT tcp dport { 6800-7300,8443 } ct state new counter accept comment "113 ceph_mgr"</code></pre>
</div>
</div>
</li>
<li>
<p>Save the file.</p>
</li>
<li>
<p>Restart the <code>nftables</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl restart nftables</pre>
</div>
</div>
</li>
<li>
<p>Verify that the rules are applied:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo nft list ruleset | grep ceph_mgr</pre>
</div>
</div>
</li>
<li>
<p>Prepare the target node to host the new Ceph Manager daemon, and add the <code>mgr</code>
label to the target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host label add &lt;target_node&gt; mgr</pre>
</div>
</div>
</li>
<li>
<p>Repeat steps 1-7 for each target node that hosts a Ceph Manager daemon.</p>
</li>
<li>
<p>Get the Ceph Manager spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export mgr &gt; ${SPEC_DIR}/mgr</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and add the <code>label: mgr</code> section to the <code>placement</code>
section:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mgr
service_id: mgr
placement:
  label: mgr</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mgr -- ceph orch apply -i /mnt/mgr</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>Verify that the new Ceph Manager daemons are created in the target nodes:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch ps | grep -i mgr
$ sudo cephadm shell -- ceph -s</pre>
</div>
</div>
<div class="paragraph">
<p>The Ceph Manager daemon count should match the number of hosts where the <code>mgr</code> label is added.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The migration does not shrink the Ceph Manager daemons. The count grows by
the number of target nodes, and migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes
decommissions the stand-by Ceph Manager instances. For more information, see
<a href="#migrating-mon-from-controller-nodes_migrating-ceph-rbd">Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="migrating-mon-from-controller-nodes_migrating-ceph-rbd">7.5.2. Migrating Ceph Monitor daemons to Red Hat Ceph Storage nodes</h4>
<div class="paragraph _abstract">
<p>You must move Ceph Monitor daemons from the Red&#160;Hat OpenStack Platform (RHOSP) Controller nodes to a set of target nodes. Target nodes are either existing Red Hat Ceph Storage nodes, or RHOSP Compute nodes if Red Hat Ceph Storage is
deployed by director with a Hyperconverged Infrastructure (HCI) topology. Additional Ceph Monitors are deployed to the target nodes, and they are promoted as <code>_admin</code> nodes that you can use to manage the Red Hat Ceph Storage cluster and perform day 2 operations.</p>
</div>
<div class="paragraph">
<p>To migrate the Ceph Monitor daemons, you must perform the following high-level steps:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><a href="#configuring-target-nodes-for-ceph-monitor-migration_migrating-ceph-mon">Configure the target nodes for Ceph Monitor migration</a>.</p>
</li>
<li>
<p><a href="#draining-the-source-node_migrating-ceph-mon">Drain the source node</a>.</p>
</li>
<li>
<p><a href="#migrating-the-ceph-monitor-ip-address_migrating-ceph-mon">Migrate your Ceph Monitor IP addresses to the target nodes</a>.</p>
</li>
<li>
<p><a href="#redeploying-a-ceph-monitor-on-the-target-node_migrating-ceph-mon">Redeploy the Ceph Monitor on the target node</a>.</p>
</li>
<li>
<p><a href="#verifying-the-cluster-after-ceph-mon-migration_migrating-ceph-mon">Verify that the Red Hat Ceph Storage cluster is healthy</a>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Repeat these steps for any additional Controller node that hosts a Ceph Monitor until you migrate all the Ceph Monitor daemons to the target nodes.</p>
</div>
<div class="sect4">
<h5 id="configuring-target-nodes-for-ceph-monitor-migration_migrating-ceph-mon">Configuring target nodes for Ceph Monitor migration</h5>
<div class="paragraph _abstract">
<p>Prepare the target Red Hat Ceph Storage nodes for the Ceph Monitor migration by performing the following actions:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Enable firewall rules in a target node and persist them.</p>
</li>
<li>
<p>Create a spec that is based on labels and apply it by using <code>cephadm</code>.</p>
</li>
<li>
<p>Ensure that the Ceph Monitor quorum is maintained during the migration process.</p>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>SSH into the target node and enable the firewall rules that are required to
reach a Ceph Monitor service:</p>
<div class="listingblock">
<div class="content">
<pre>$ for port in 3300 6789; {
    ssh heat-admin@&lt;target_node&gt; sudo iptables -I INPUT \
    -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
    -j ACCEPT;
}</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the node that hosts the new Ceph Monitor.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Check that the rules are properly applied to the target node and persist them:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo iptables-save
$ sudo systemctl restart iptables</pre>
</div>
</div>
</li>
<li>
<p>If <code>nftables</code> is used in the existing deployment, edit <code>/etc/nftables/tripleo-rules.nft</code>
and add the following content:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># 110 ceph_mon {'dport': [6789, 3300, '9100']}
add rule inet filter TRIPLEO_INPUT tcp dport { 6789,3300,9100 } ct state new counter accept comment "110 ceph_mon"</code></pre>
</div>
</div>
</li>
<li>
<p>Save the file.</p>
</li>
<li>
<p>Restart the <code>nftables</code> service:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo systemctl restart nftables</pre>
</div>
</div>
</li>
<li>
<p>Verify that the rules are applied:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo nft list ruleset | grep ceph_mon</pre>
</div>
</div>
</li>
<li>
<p>To migrate the existing Ceph Monitors to the target Red Hat Ceph Storage nodes, retrieve the Red Hat Ceph Storage mon spec from the first Ceph Monitor, or the first Controller node:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ mkdir -p ${SPEC_DIR}
$ sudo cephadm shell -- ceph orch ls --export mon &gt; ${SPEC_DIR}/mon</pre>
</div>
</div>
</li>
<li>
<p>Add the <code>label:mon</code> section to the <code>placement</code> section:</p>
<div class="listingblock">
<div class="content">
<pre>service_type: mon
service_id: mon
placement:
  label: mon</pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mon -- ceph orch apply -i /mnt/mon</pre>
</div>
</div>
</li>
<li>
<p>Extend the <code>mon</code> label to the remaining Red Hat Ceph Storage target nodes to ensure that
quorum is maintained during the migration process:</p>
<div class="listingblock">
<div class="content">
<pre>for item in $(sudo cephadm shell --  ceph orch host ls --format json | jq -r '.[].hostname'); do
    sudo cephadm shell -- ceph orch host label add  $item mon;
    sudo cephadm shell -- ceph orch host label add  $item _admin;
done</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Applying the <code>mon</code> spec allows the existing strategy to use <code>labels</code> instead of <code>hosts</code>.
As a result, any node with the <code>mon</code> label can host a Ceph Monitor daemon.
Perform this step only once to avoid multiple iterations when multiple Ceph Monitors are migrated.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Check the status of the Red Hat Ceph Storage and the Ceph Orchestrator daemons list.
Ensure that Ceph Monitors are in a quorum and listed by the <code>ceph orch</code> command:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK

  services:
    mon: 6 daemons, quorum controller-0,controller-1,controller-2,ceph-0,ceph-1,ceph-2 (age 19m)
    mgr: controller-0.xzgtvo(active, since 32m), standbys: controller-1.mtxohd, controller-2.ahrgsk
    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   43 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host ls
HOST              ADDR           LABELS          STATUS
ceph-0        192.168.24.14  osd mon mgr _admin
ceph-1        192.168.24.7   osd mon mgr _admin
ceph-2        192.168.24.8   osd mon mgr _admin
controller-0  192.168.24.15  _admin mgr mon
controller-1  192.168.24.23  _admin mgr mon
controller-2  192.168.24.13  _admin mgr mon</pre>
</div>
</div>
</li>
<li>
<p>Set up a Ceph client on the first Controller node that is used during the rest
of the procedure to interact with Red Hat Ceph Storage. Set up an additional IP address on the
storage network that is used to interact with Red Hat Ceph Storage when the first Controller
node is decommissioned:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Back up the content of <code>/etc/ceph</code> in the <code>ceph_client_backup</code> directory.</p>
<div class="listingblock">
<div class="content">
<pre>$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph/* $HOME/ceph_client_backup</pre>
</div>
</div>
</li>
<li>
<p>Edit <code>/etc/os-net-config/config.yaml</code> and add <code>- ip_netmask: 172.17.3.200</code>
after the IP address on the VLAN that belongs to the storage network. Replace
<code>172.17.3.200</code> with any other available IP address on the storage network
that can be statically assigned to <code>controller-0</code>.</p>
</li>
<li>
<p>Save the file and refresh the <code>controller-0</code> network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the IP address is present in the Controller node:</p>
<div class="listingblock">
<div class="content">
<pre>$ ip -o a | grep 172.17.3.200</pre>
</div>
</div>
</li>
<li>
<p>Ping the IP address and confirm that it is reachable:</p>
<div class="listingblock">
<div class="content">
<pre>$ ping -c 3 172.17.3.200</pre>
</div>
</div>
</li>
<li>
<p>Verify that you can interact with the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -c $HOME/ceph_client_backup/ceph.conf -k $HOME/ceph_client_backup/ceph.client.admin.keyring -- ceph -s</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Proceed to the next step <a href="#draining-the-source-node_migrating-ceph-mon">Draining the source node</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="draining-the-source-node_migrating-ceph-mon">Draining the source node</h5>
<div class="paragraph _abstract">
<p>Drain the source node and remove the source node host from the Red Hat Ceph Storage cluster.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>On the source node, back up the <code>/etc/ceph/</code> directory to run <code>cephadm</code> and get a shell for the Red Hat Ceph Storage cluster from the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph $HOME/ceph_client_backup</pre>
</div>
</div>
</li>
<li>
<p>Identify the active <code>ceph-mgr</code> instance:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr stat</pre>
</div>
</div>
</li>
<li>
<p>Fail the <code>ceph-mgr</code> if it is active on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr fail &lt;mgr_instance&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;mgr_instance&gt;</code> with the Ceph Manager daemon to fail.</p>
</li>
</ul>
</div>
</li>
<li>
<p>From the <code>cephadm</code> shell, remove the labels on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>$ for label in mon mgr _admin; do
    sudo cephadm shell -- ceph orch host label rm &lt;source_node&gt; $label;
done</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;source_node&gt;</code> with the hostname of the source node.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: Ensure that you remove the Ceph Monitor daemon from the source node if it is still running:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch daemon rm mon.&lt;source_node&gt; --force</pre>
</div>
</div>
</li>
<li>
<p>Drain the source node to remove any leftover daemons:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host drain &lt;source_node&gt;</pre>
</div>
</div>
</li>
<li>
<p>Remove the source node host from the Red Hat Ceph Storage cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch host rm &lt;source_node&gt; --force</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The source node is not part of the cluster anymore, and should not appear in
the Red Hat Ceph Storage host list when you run <code>sudo cephadm shell -- ceph orch host ls</code>.
However, if you run <code>sudo podman ps</code> in the source node, the list might show
that both Ceph Monitors and Ceph Managers are still running.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[root@controller-1 ~]# sudo podman ps
CONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES
5c1ad36472bc  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-controller-1
3b14cc7bf4dd  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-controller-1-mtxohd</pre>
</div>
</div>
<div class="paragraph">
<p>To clean up the existing containers and remove the <code>cephadm</code> data from the source node, contact Red Hat Support.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Confirm that mons are still in quorum:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph -s
$ sudo cephadm shell -- ceph orch ps | grep -i mon</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Proceed to the next step <a href="#migrating-the-ceph-monitor-ip-address_migrating-ceph-mon">Migrating the Ceph Monitor IP address</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="migrating-the-ceph-monitor-ip-address_migrating-ceph-mon">Migrating the Ceph Monitor IP address</h5>
<div class="paragraph _abstract">
<p>You must migrate your Ceph Monitor IP addresses to the target Red Hat Ceph Storage nodes. The
IP address migration assumes that the target nodes are originally deployed by
director and that the network configuration is managed by
<code>os-net-config</code>.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Get the original Ceph Monitor IP addresses from <code>$HOME/ceph_client_backup/ceph.conf</code> file on the <code>mon_host</code> line, for example:</p>
<div class="listingblock">
<div class="content">
<pre>mon_host = [v2:172.17.3.60:3300/0,v1:172.17.3.60:6789/0] [v2:172.17.3.29:3300/0,v1:172.17.3.29:6789/0] [v2:172.17.3.53:3300/0,v1:172.17.3.53:6789/0]</pre>
</div>
</div>
</li>
<li>
<p>Match the IP address retrieved in the previous step with the storage network IP addresses on the source node, and find the Ceph Monitor IP address:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ ip -o -4 a | grep 172.17.3
9: vlan30    inet 172.17.3.60/24 brd 172.17.3.255 scope global vlan30\       valid_lft forever preferred_lft forever
9: vlan30    inet 172.17.3.13/32 brd 172.17.3.255 scope global vlan30\       valid_lft forever preferred_lft forever</pre>
</div>
</div>
</li>
<li>
<p>Confirm that the Ceph Monitor IP address is present in the <code>os-net-config</code> configuration that is located in the <code>/etc/os-net-config</code> directory on the source node:</p>
<div class="listingblock">
<div class="content">
<pre>[tripleo-admin@controller-0 ~]$ grep "172.17.3.60" /etc/os-net-config/config.yaml
    - ip_netmask: 172.17.3.60/24</pre>
</div>
</div>
</li>
<li>
<p>Edit the <code>/etc/os-net-config/config.yaml</code> file and remove the <code>ip_netmask</code> line.</p>
</li>
<li>
<p>Save the file and refresh the node network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the IP address is not present in the source node anymore, for example:</p>
<div class="listingblock">
<div class="content">
<pre>[controller-0]$ ip -o a | grep 172.17.3.60</pre>
</div>
</div>
</li>
<li>
<p>SSH into the target node, for example <code>cephstorage-0</code>, and add the IP address
for the new Ceph Monitor.</p>
</li>
<li>
<p>On the target node, edit <code>/etc/os-net-config/config.yaml</code> and
add the <code>- ip_netmask: 172.17.3.60</code> line that you removed in the source node.</p>
</li>
<li>
<p>Save the file and refresh the node network configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo os-net-config -c /etc/os-net-config/config.yaml</pre>
</div>
</div>
</li>
<li>
<p>Verify that the IP address is present in the target node.</p>
<div class="listingblock">
<div class="content">
<pre>$ ip -o a | grep 172.17.3.60</pre>
</div>
</div>
</li>
<li>
<p>From the Ceph client node, <code>controller-0</code>, ping the IP address that is
migrated to the target node and confirm that it is still reachable:</p>
<div class="listingblock">
<div class="content">
<pre>[controller-0]$ ping -c 3 172.17.3.60</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Proceed to the next step <a href="#redeploying-a-ceph-monitor-on-the-target-node_migrating-ceph-mon">Redeploying the Ceph Monitor on the target node</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="redeploying-a-ceph-monitor-on-the-target-node_migrating-ceph-mon">Redeploying a Ceph Monitor on the target node</h5>
<div class="paragraph _abstract">
<p>You use the IP address that you migrated to the target node to redeploy the
Ceph Monitor on the target node.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>From the Ceph client node, for example <code>controller-0</code>, get the Ceph mon spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -- ceph orch ls --export mon &gt; ${SPEC_DIR}/mon</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and add the <code>unmanaged: true</code> keyword:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: true</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mon -- ceph orch apply -i /mnt/mon</pre>
</div>
</div>
<div class="paragraph">
<p>The Ceph Monitor daemons are marked as <code>unmanaged</code>, and you can now redeploy the existing daemon and bind it to the migrated IP address.</p>
</div>
</li>
<li>
<p>Delete the existing Ceph Monitor on the target node:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch daemon rm mon.&lt;target_node&gt; --force</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;target_node&gt;</code> with the hostname of the target node that is included in the Red Hat Ceph Storage cluster.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Redeploy the new Ceph Monitor on the target node by using the migrated IP address:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch daemon add mon &lt;target_node&gt;:&lt;ip_address&gt;</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Replace <code>&lt;ip_address&gt;</code> with the IP address of the migrated IP address.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Get the Ceph Monitor spec:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -- ceph orch ls --export mon &gt; ${SPEC_DIR}/mon</pre>
</div>
</div>
</li>
<li>
<p>Edit the retrieved spec and set the <code>unmanaged</code> keyword to <code>false</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: false</code></pre>
</div>
</div>
</li>
<li>
<p>Save the spec.</p>
</li>
<li>
<p>Apply the spec with <code>cephadm</code> by using the Ceph Orchestrator:</p>
<div class="listingblock">
<div class="content">
<pre>$ SPEC_DIR=${SPEC_DIR:-"$PWD/ceph_specs"}
$ sudo cephadm shell -m ${SPEC_DIR}/mon -- ceph orch apply -i /mnt/mon</pre>
</div>
</div>
<div class="paragraph">
<p>The new Ceph Monitor runs on the target node with the original IP address.</p>
</div>
</li>
<li>
<p>Identify the running <code>mgr</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr stat</pre>
</div>
</div>
</li>
<li>
<p>Refresh the Ceph Manager information by force-failing it:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph mgr fail</pre>
</div>
</div>
</li>
<li>
<p>Refresh the <code>OSD</code> information:</p>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell -- ceph orch reconfig osd.default_drive_group</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Next steps</div>
<p>Repeat the procedure starting from step <a href="#draining-the-source-node_migrating-ceph-mon">Draining the source node</a> for each node that you want to decommission.
Proceed to the next step <a href="#verifying-the-cluster-after-ceph-mon-migration_migrating-ceph-mon">Verifying the Red Hat Ceph Storage cluster after Ceph Monitor migration</a>.</p>
</div>
</div>
<div class="sect4">
<h5 id="verifying-the-cluster-after-ceph-mon-migration_migrating-ceph-mon">Verifying the Red Hat Ceph Storage cluster after Ceph Monitor migration</h5>
<div class="paragraph _abstract">
<p>After you finish migrating your Ceph Monitor daemons to the target nodes, verify that the the Red Hat Ceph Storage cluster is healthy.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Verify that the Red Hat Ceph Storage cluster is healthy:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK
...
...</pre>
</div>
</div>
</li>
<li>
<p>Verify that the Red Hat Ceph Storage mons are running with the old IP addresses. SSH
into the target nodes and verify that the Ceph Monitor daemons are bound to
the expected IP and port:</p>
<div class="listingblock">
<div class="content">
<pre>$ netstat -tulpn | grep 3300</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="updating-the-cluster-dashboard-configuration_migrating-ceph-rbd">7.6. Updating the Red Hat Ceph Storage cluster Ceph Dashboard configuration</h3>
<div class="paragraph _abstract">
<p>If the Ceph Dashboard is part of the enabled Ceph Manager modules, you need to
reconfigure the failover settings.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Regenerate the following Red Hat Ceph Storage configuration keys to point to the right
<code>mgr</code> container:</p>
<div class="listingblock">
<div class="content">
<pre>mgr    advanced  mgr/dashboard/controller-0.ycokob/server_addr  172.17.3.33
mgr    advanced  mgr/dashboard/controller-1.lmzpuc/server_addr  172.17.3.147
mgr    advanced  mgr/dashboard/controller-2.xpdgfl/server_addr  172.17.3.138</pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>$ sudo cephadm shell
$ ceph orch ps | awk '/mgr./ {print $1}'</pre>
</div>
</div>
</li>
<li>
<p>For each retrieved <code>mgr</code> daemon, update the corresponding entry in the Red Hat Ceph Storage
configuration:</p>
<div class="listingblock">
<div class="content">
<pre>$ ceph config set mgr mgr/dashboard/&lt;&gt;/server_addr/&lt;ip addr&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2026-02-27 14:43:57 UTC
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/languages/yaml.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/languages/bash.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code[data-lang]')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
</body>
</html>