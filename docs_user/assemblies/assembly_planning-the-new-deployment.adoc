[id="planning-the-new-deployment_{context}"]

:context: planning

= Planning the new deployment

Just like you did back when you installed your director-deployed {osp_prev_long}, the
upgrade/migration to the control plane requires planning various aspects
of the environment such as node roles, planning your network topology, and
storage.

This document covers some of this planning, but it is recommended to read
the whole adoption guide before actually starting the process to be sure that
there is a global understanding of the whole process.

== Network

With OpenShift, the network is a very important aspect of the deployment, and
it is important to plan it carefully. The general network requirements for the
OpenStack services are not much different from the ones in a director
deployment, but the way you handle them is.

____
Note: More details about the network architecture and configuration can be
found in the
https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/18.0-dev-preview/html/deploying_red_hat_openstack_platform_18.0_development_preview_3_on_red_hat_openshift_container_platform/assembly_preparing-rhocp-for-rhosp#doc-wrapper[general
OpenStack documentation] as well as
https://docs.openshift.com/container-platform/4.14/networking/about-networking.html[OpenShift
Networking guide]. This document will address concerns specific to adoption.
____

// TODO: update the openstack link with the final documentation

// TODO: should we parametrize the version in the links somehow?

When adopting a new OpenStack deployment, it is important to align the network
configuration with the adopted cluster to maintain connectivity for existing
workloads.

The following logical configuration steps will incorporate the existing network
configuration:

- configure **OpenShift worker nodes** to align VLAN tags and IPAM
  configuration with the existing deployment.
- configure **control plane services** to use compatible IP ranges for
  service and load balancing IPs.
- configure **data plane nodes** to use corresponding compatible configuration for
  VLAN tags and IPAM.

Specifically,

- **<<_ipam_planning,IPAM configuration>>** will either be reused from the
  **existing** deployment or, depending on IP address availability in the
  existing allocation pools, **new** ranges will be defined to be used for the
  new control plane services. If so, **IP routing** will be configured between
  the old and new ranges.
- **<<_vlan_tags,VLAN tags>>** will be reused from the existing deployment.

=== Pulling configuration from the existing deployment

Let's first determine which isolated networks are defined in the existing
deployment. You can find the network configuration in the `network_data.yaml`
file. For example,

```
- name: InternalApi
  mtu: 1500
  vip: true
  vlan: 20
  name_lower: internal_api
  dns_domain: internal.mydomain.tld.
  service_net_map_replace: internal
  subnets:
    internal_api_subnet:
      ip_subnet: '172.17.0.0/24'
      allocation_pools: [{'start': '172.17.0.4', 'end': '172.17.0.250'}]
```

You should make a note of the VLAN tag used (`vlan` key) and the IP range
(`ip_subnet` key) for each isolated network. The IP range will later be split
into separate pools for control plane services and load balancer IP addresses.

You should also determine the list of IP addresses already consumed in the
adopted environment. Consult `tripleo-ansible-inventory.yaml` file to find this
information. In the file, for each listed host, note IP and VIP addresses
consumed by the node.

For example,

```
Standalone:
  hosts:
    standalone:
      ...
      internal_api_ip: 172.17.0.100
    ...
  ...
standalone:
  children:
    Standalone: {}
  vars:
    ...
    internal_api_vip: 172.17.0.2
    ...
```

In the example above, note that the `172.17.0.2` and `172.17.0.100` are
consumed and won't be available for the new control plane services, at least
until the adoption is complete.

Repeat the process for each isolated network and each host in the
configuration.

---

At the end of this process, you should have the following information:

- A list of isolated networks used in the existing deployment.
- For each of the isolated networks, the VLAN tag and IP ranges used for
  dynamic address allocation.
- A list of existing IP address allocations used in the environment. You will
  later exclude these addresses from allocation pools available for the new
  control plane services.

=== IPAM planning

The new deployment model puts additional burden on the size of IP allocation
pools available for OpenStack services. This is because each service deployed
on OpenShift worker nodes will now require an IP address from the IPAM pool (in
the previous deployment model, all services hosted on a controller node shared
the same IP address.)

Since the new control plane deployment has different requirements as to the
number of IP addresses available for services, it may even be impossible to
reuse the existing IP ranges used in adopted environment, depending on its
size. Prudent planning is required to determine which options are available in
your particular case.

The total number of IP addresses required for the new control plane services,
in each isolated network, is calculated as a sum of the following:

- The number of OpenShift worker nodes. (Each node will require 1 IP address in
  `NodeNetworkConfigurationPolicy` CRs.)
- The number of IP addresses required for the data plane nodes. (Each node will require
  an IP address from `NetConfig` CRs.)
- The number of IP addresses required for control plane services. (Each service
  will require an IP address from `NetworkAttachmentDefinition` CRs.) This
  number depends on the number of replicas for each service.
- The number of IP addresses required for load balancer IP addresses. (Each
  service will require a VIP address from `IPAddressPool` CRs.)

As of the time of writing, the simplest single worker node OpenShift deployment
(CRC) has the following IP ranges defined (for the `internalapi` network):

- 1 IP address for the single worker node;
- 1 IP address for the data plane node;
- `NetworkAttachmentDefinition` CRs for control plane services:
  `X.X.X.30-X.X.X.70` (41 addresses);
- `IPAllocationPool` CRs for load balancer IPs: `X.X.X.80-X.X.X.90` (11
  addresses).

Which comes to a total of 54 IP addresses allocated to the `internalapi`
allocation pools.

// TODO: update the numbers above for a more realistic multinode cluster.

The exact requirements may differ depending on the list of OpenStack services
to be deployed, their replica numbers, as well as the number of OpenShift
worker nodes and data plane nodes.

Additional IP addresses may be required in future OpenStack releases, so it is
advised to plan for some extra capacity, for each of the allocation pools used
in the new environment.

Once you know the required IP pool size for the new deployment, you can choose
one of the following scenarios to handle IPAM allocation in the new
environment.

The first listed scenario is more general and implies using new IP ranges,
while the second scenario implies reusing the existing ranges. The end state of
the former scenario is using the new subnet ranges for control plane services,
but keeping the old ranges, with their node IP address allocations intact, for
EDP nodes.

==== Scenario 1: Use new subnet ranges

This scenario is compatible with any existing subnet configuration, and can be
used even when the existing cluster subnet ranges don't have enough free IP
addresses for the new control plane services.

The general idea here is to define new IP ranges for control plane services
that belong to a different subnet that was not used in the existing cluster.
Then, configure link local IP routing between the old and new subnets to allow
old and new service deployments to communicate. This involves using TripleO
mechanism on pre-adopted cluster to configure additional link local routes
there. This will allow EDP deployment to reach out to adopted nodes using their
old subnet addresses.

The new subnet should be sized appropriately to accommodate the new control
plane services, but otherwise doesn't have any specific requirements as to the
existing deployment allocation pools already consumed. Actually, the
requirements as to the size of the new subnet are lower than in the second
scenario, as the old subnet ranges are kept for the adopted nodes, which means
they don't consume any IP addresses from the new range.

In this scenario, you will configure `NetworkAttachmentDefinition` CRs to use a
different subnet from what will be configured in `NetConfig` CR for the same
networks. The former range will be used for control plane services,
while the latter will be used to manage IPAM for EDP nodes.

During the process, you will need to make sure that adopted node IP addresses
don't change during the adoption process. This is achieved by listing the
addresses in `fixedIP` fields in `OpenstackDataplaneNodeSet` per-node section.

---

Before proceeding, configure host routes on the adopted nodes for the
control plane subnets.

To achieve this, you will need to re-run `tripleo deploy` with additional
`routes` entries added to `network_config`. (This change should be applied
for every adopted node configuration.) For example, you may add the following
to `net_config.yaml`:

```yaml
network_config:
  - type: ovs_bridge
    name: br-ctlplane
    routes:
    - ip_netmask: 0.0.0.0/0
      next_hop: 192.168.1.1
    - ip_netmask: 172.31.0.0/24  # <- new ctlplane subnet
      next_hop: 192.168.1.100    # <- adopted node ctlplane IP address
```

Do the same for other networks that will need to use different subnets for the
new and old parts of the deployment.

Once done, run `tripleo deploy` to apply the new configuration.

Note that network configuration changes are not applied by default to avoid
risk of network disruption. You will have to enforce the changes by setting the
`StandaloneNetworkConfigUpdate: true` in the TripleO configuration files.

Once `tripleo deploy` is complete, you should see new link local routes to the
new subnet on each node. For example,

```bash
# ip route | grep 172
172.31.0.0/24 via 192.168.122.100 dev br-ctlplane
```

---

The next step is to configure similar routes for the old subnet for control plane
services attached to the networks. This is done by adding `routes` entries to
`NodeNetworkConfigurationPolicy` CRs for each network. For example,

```yaml
      - destination: 192.168.122.0/24
        next-hop-interface: ospbr
```

Once applied, you should eventually see the following route added to your OCP nodes.

```bash
# ip route | grep 192
192.168.122.0/24 dev ospbr proto static scope link
```

---

At this point, you should be able to ping the adopted nodes from OCP nodes
using their old subnet addresses; and vice versa.

---


Finally, during the data plane adoption, you will have to take care of several aspects:

- in network_config, add link local routes to the new subnets, for example:

```yaml
  nodeTemplate:
    ansible:
      ansibleUser: root
      ansibleVars:
        additional_ctlplane_host_routes:
        - ip_netmask: 172.31.0.0/24
          next_hop: '{{ ctlplane_ip }}'
        edpm_network_config_template: |
          network_config:
          - type: ovs_bridge
            routes: {{ ctlplane_host_routes + additional_ctlplane_host_routes }}
            ...
```

- list the old IP addresses as `ansibleHost` and `fixedIP`, for example:

```yaml
  nodes:
    standalone:
      ansible:
        ansibleHost: 192.168.122.100
        ansibleUser: ""
      hostName: standalone
      networks:
      - defaultRoute: true
        fixedIP: 192.168.122.100
        name: ctlplane
        subnetName: subnet1
```

- expand SSH range for the firewall configuration to include both subnets:

```yaml
        edpm_sshd_allowed_ranges:
        - 192.168.122.0/24
        - 172.31.0.0/24
```

This is to allow SSH access from the new subnet to the adopted nodes as well as
the old one.

---

Since you are applying new network configuration to the nodes, consider also
setting `edpm_network_config_update: true` to enforce the changes.

---

Note that the examples above are incomplete and should be incorporated into
your general configuration.

==== Scenario 2: Reuse existing subnet ranges

This scenario is only applicable when the existing subnet ranges have enough IP
addresses for the new control plane services. On the other hand, it allows to
avoid additional routing configuration between the old and new subnets, as in
<<_scenario_1_use_new_subnet_ranges,scenario 1>>.

The general idea here is to instruct the new control plane services to use the
same subnet as in the adopted environment, but define allocation pools used by
the new services in a way that would exclude IP addresses that were already
allocated to existing cluster nodes.

This scenario implies that the remaining IP addresses in the existing subnet is
enough for the new control plane services. If not,
<<_scenario_1_use_new_subnet_ranges,the first scenario>> should be used
instead. Please consult <<_ipam_planning,IPAM planning>> for more details.

No special routing configuration is required in this scenario; the only thing
to pay attention to is to make sure that already consumed IP addresses don't
overlap with the new allocation pools configured for OpenStack control
plane services.

If you are especially constrained by the size of the existing subnet, you may
have to apply elaborate exclusion rules when defining allocation pools for the
new control plane services. You can find details on how to do this in the
appropriate sections below.

=== VLAN tags

Regardless of the IPAM scenario, the VLAN tags used in the existing deployment
will be reused in the new deployment. Depending on the scenario, the IP address
ranges to be used for control plane services will be either reused from the old
deployment or defined anew. Adjust the configuration described below
accordingly.

=== Configuration steps

At this point, you should have a good idea about VLAN and IPAM configuration
you would like to replicate in the new environment.

Before proceeding, you should have a list of the following IP address
allocations to be used for the new control plane services:

- 1 IP address, per isolated network, per OpenShift worker node. (These
  addresses will <<_configure_openshift_worker_nodes,translate>> to
  `NodeNetworkConfigurationPolicy` CRs.)
- IP range, per isolated network, for the data plane nodes. (These ranges will
  <<_configure_data_plane_nodes,translate>> to `NetConfig` CRs.)
- IP range, per isolated network, for control plane services. (These ranges
  will <<_pod_connectivity_to_isolated_networks,translate>> to
  `NetworkAttachmentDefinition` CRs.)
- IP range, per isolated network, for load balancer IP addresses. (These ranges
  will <<_load_balancer_ip_addresses,translate>> to `IPAddressPool` CRs for
  MetalLB.)

____
Make sure you have the information listed above before proceeding with the next
steps!
____

____
Note: The exact list and configuration of isolated networks in the examples
listed below should reflect the actual adopted environment. The number of
isolated networks may differ from the example below. IPAM scheme may differ.
Only relevant parts of the configuration are shown. Examples are incomplete and
should be incorporated into the general configuration for the new deployment,
as described in the general OpenStack documentation.
____

=== Configure OpenShift worker nodes

OCP worker nodes that run OpenStack services need a way to connect the service
pods to isolated networks. This requires physical network configuration on the
hypervisor.

This configuration is managed by the NMState operator, which uses the CRs to
define the desired network configuration for the nodes.

For each node, define a `NodeNetworkConfigurationPolicy` CR that describes the
desired network configuration. See the example below.

```
apiVersion: v1
items:
- apiVersion: nmstate.io/v1
  kind: NodeNetworkConfigurationPolicy
  spec:
      interfaces:
      - description: internalapi vlan interface
        ipv4:
          address:
          - ip: 172.17.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.20
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 20
          reorder-headers: true
      - description: storage vlan interface
        ipv4:
          address:
          - ip: 172.18.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.21
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 21
          reorder-headers: true
      - description: tenant vlan interface
        ipv4:
          address:
          - ip: 172.19.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.22
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 22
          reorder-headers: true
    nodeSelector:
      kubernetes.io/hostname: ocp-worker-0
      node-role.kubernetes.io/worker: ""
```

=== Configure control plane services

==== Pod connectivity to isolated networks

Once NMState operator created the desired hypervisor network configuration for
isolated networks, we need to configure OpenStack services to use configured
interfaces. This is achieved by defining `NetworkAttachmentDefinition` CRs for
each isolated network. (In some clusters, these CRs are managed by
https://docs.openshift.com/container-platform/4.14/networking/cluster-network-operator.html[Cluster
Network Operator], in which case `Network` CRs should be used instead.)

For example,

```
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50"
      }
    }
```

Make sure that the interface name and IPAM range match the configuration used
in `NodeNetworkConfigurationPolicy` CRs.

When reusing existing IP ranges, you may exclude part of the range defined by
`range_start` and `range_end` that was already consumed in the existing
deployment. Please use `exclude` as follows.

```
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50",
        "exclude": [
          "172.17.0.24/32",
          "172.17.0.44/31"
        ]
      }
    }
```

The example above would exclude addresses `172.17.0.24` as well as
`172.17.0.44` and `172.17.0.45` from the allocation pool.

==== Load balancer IP addresses

Some OpenStack services require load balancer IP addresses. These IP addresses
belong to the same IP range as the control plane services, and are managed by
MetalLB. The IP address pool is defined by `IPAllocationPool` CRs. This pool
should also be aligned with the adopted configuration.

For example,

```
- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.70
```

Define `IPAddressPool` CRs for each isolated network that requires load
balancer IP addresses.

When reusing existing IP ranges, you may exclude part of the range by listing
multiple `addresses` entries.

For example,

```
- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.64
    - 172.17.0.66-172.17.0.70
```

The example above would exclude the `172.17.0.65` address from the allocation
pool.

// TODO: is there anything specific to mention about BGP L3 mode here?

=== Configure data plane nodes

A complete OpenStack cluster consists of OpenShift nodes and data plane nodes. The
former use `NodeNetworkConfigurationPolicy` CRs to configure physical
interfaces. Since data plane nodes are not OpenShift nodes, a different approach to
configure their network connectivity is used.

Instead, data plane nodes are configured by `dataplane-operator` and its CRs. The CRs
define desired network configuration for the nodes.

In case of adoption, the configuration should reflect the existing network
setup. You should be able to pull `net_config.yaml` files from each node and
reuse it when defining `OpenstackDataplaneNodeSet`. The format of the
configuration hasn't changed (`os-net-config` is still being used under the
hood), so you should be able to put network templates under
`edpm_network_config_template` variables (either common for all nodes, or
per-node).

To make sure the latest network configuration is used during the data plane adoption, you
should also set `edpm_network_config_update: true` in the `nodeTemplate`.

You will proceed with <<adopting-dataplane_openstack-adoption,Data Plane Adoption
process>> once the OpenStack control plane is deployed in the
OpenShift cluster. When doing so, you will configure `NetConfig` and
`OpenstackDataplaneNodeSet` CRs, using the same VLAN tags and IPAM
configuration as determined in the previous steps.

For example,

```
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
```

List multiple `allocationRanges` entries to exclude some of the IP addresses,
e.g. to accommodate for addresses already consumed by the adopted environment.

```
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.199
        start: 172.17.0.100
      - end: 172.17.0.250
        start: 172.17.0.201
      cidr: 172.17.0.0/24
      vlan: 20
```

The example above would exclude the `172.17.0.200` address from the pool.


include::../modules/con_service-configurations.adoc[leveloffset=+1]

include::../modules/con_node-roles.adoc[leveloffset=+1]

include::../modules/con_about-node-selector.adoc[leveloffset=+1]

include::../modules/con_about-machine-configs.adoc[leveloffset=+1]

include::../modules/con_key-manager-service-support-for-crypto-plugins.adoc[leveloffset=+1]

include::../modules/con_identity-service-authentication.adoc[leveloffset=+1]

include::../assemblies/assembly_storage-requirements.adoc[leveloffset=+1]

include::../modules/con_comparing-configuration-files-between-deployments.adoc[leveloffset=+1]
