[id="adopting-dataplane_{context}"]

//:context: adopting-dataplane
//kgilliga: This module might be converted to an assembly, or a procedure as a standalone chapter.
//Check xref contexts.

= Data plane adoption

== Prerequisites

* Previous Adoption steps completed.
* Remaining source cloud xref:stopping-infrastructure-management-and-compute-services_{context}[Stopping infrastructure management and Compute services] on Compute hosts.

____
*WARNING* This step is a "point of no return" in the data plane adoption
procedure. The source control plane and data plane services must not
be ever enabled back, after the data plane is deployed, and control
plane has taken control over it.
____

== Variables

Define the shell variables used in the Fast-forward upgrade steps below.
Set `FIP` to the floating IP address of the `test` VM pre-created earlier on the source cloud.
Define the map of compute node name, IP pairs.
The values are just illustrative, use values that are correct for your environment:

----
PODIFIED_DB_ROOT_PASSWORD=$(oc get -o json secret/osp-secret | jq -r .data.DbRootPassword | base64 -d)

alias openstack="oc exec -t openstackclient -- openstack"
FIP=192.168.122.20
declare -A computes
export computes=(
  ["standalone.ooo.test"]="192.168.122.100"
  # ...
)
----

== Pre-checks

* Make sure the IPAM is configured

----
oc apply -f - <<EOF
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: ctlplane
    dnsDomain: ctlplane.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 192.168.122.120
        start: 192.168.122.100
      - end: 192.168.122.200
        start: 192.168.122.150
      cidr: 192.168.122.0/24
      gateway: 192.168.122.1
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: External
    dnsDomain: external.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 10.0.0.250
        start: 10.0.0.100
      cidr: 10.0.0.0/24
      gateway: 10.0.0.1
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: storagemgmt
    dnsDomain: storagemgmt.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.20.0.250
        start: 172.20.0.100
      cidr: 172.20.0.0/24
      vlan: 23
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
EOF
----

* In case when neutron-sriov-nic-agent is running on the existing compute nodes,
  physical device mappings needs to be checked and set the same in the
  OpenStackDataPlaneNodeSet CR. Those options will need to be set in the
  OpenStackDataPlaneNodeSet CR.
  For more information, see xref:pulling-the-openstack-configuration_{context}[Pulling the OpenStack configuration].

== Adopting the data plane

* _Temporary fix_ until the OSP 17 https://code.engineering.redhat.com/gerrit/q/topic:stable-compute-uuid[backport of the stable compute UUID feature]
lands.
+
For each compute node grab the UUID of the compute service and write it too
the stable `compute_id` file in `/var/lib/nova/` directory.
+
[subs=+quotes]
----
for name in "${!computes[@]}";
do
  uuid=$(\
    openstack hypervisor show $name \
    -f value -c 'id'\
  )
  echo "Writing $uuid to /var/lib/nova/compute_id on $name"
  ssh \
ifeval::["{build}" != "downstream"]
    -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa \
endif::[]
ifeval::["{build}" == "downstream"]
    -i *<path to SSH key>* \
endif::[]
    root@"${computes[$name]}" \
      "grep -qF $uuid /var/lib/nova/compute_id || (echo $uuid | sudo tee /var/lib/nova/compute_id && sudo chown 42436:42436 /var/lib/nova/compute_id && sudo chcon -t container_file_t /var/lib/nova/compute_id)"
done
----

* Create a https://kubernetes.io/docs/concepts/configuration/secret/#ssh-authentication-secrets[ssh authentication secret] for the data plane nodes:
+
[subs=+quotes]
----
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
    name: dataplane-adoption-secret
    namespace: openstack
data:
    ssh-privatekey: |
ifeval::["{build}" != "downstream"]
$(cat ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa | base64 | sed 's/^/        /')
endif::[]
ifeval::["{build}" == "downstream"]
$(cat *<path to SSH key>* | base64 | sed 's/^/        /')
endif::[]
EOF
----

* Generate an ssh key-pair `nova-migration-ssh-key` secret
+
----
cd "$(mktemp -d)"
ssh-keygen -f ./id -t ecdsa-sha2-nistp521 -N ''
oc get secret nova-migration-ssh-key || oc create secret generic nova-migration-ssh-key \
  -n openstack \
  --from-file=ssh-privatekey=id \
  --from-file=ssh-publickey=id.pub \
  --type kubernetes.io/ssh-auth
rm -f id*
cd -
----

* Create a Nova Compute Extra Config service
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-compute-extraconfig
  namespace: openstack
data:
  19-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=true
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneService
metadata:
  name: nova-compute-extraconfig
  namespace: openstack
spec:
  label: nova.compute.extraconfig
  configMaps:
    - nova-compute-extraconfig
  secrets:
    - nova-cell1-compute-config
    - nova-migration-ssh-key
  playbook: osp.edpm.nova
EOF
----
+
The secret `nova-cell<X>-compute-config` is auto-generated for each
`cell<X>`. That secret, alongside `nova-migration-ssh-key`, should
always be specified for each custom `OpenStackDataPlaneService` related to Nova.

ifeval::["{build}" == "downstream"]
* Create subscription-manager and redhat-registry secrets
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: subscription-manager
data:
  username: <base64 encoded subscription-manager username>
  password: <base64 encoded subscription-manager password>
---
apiVersion: v1
kind: Secret
metadata:
  name: redhat-registry
data:
  username: <base64 encoded registry username>
  password: <base64 encoded registry password>
EOF
----
+
endif::[]

* Deploy OpenStackDataPlaneNodeSet:
+
Make sure that ovn-controller settings configured in the OpenStackDataPlaneNodeSet are the same as were set in the compute nodes before adoption.
This configuration is stored in the "external_ids" column in the "Open_vSwitch" table in ovsdb and can be checked with command:
+
----
ovs-vsctl list Open .
...
external_ids        : {hostname=standalone.ooo.test, ovn-bridge=br-int, ovn-bridge-mappings="datacentre:br-ctlplane", ovn-chassis-mac-mappings="datacentre:1e:0a:bb:e6:7c:ad", ovn-encap-ip="172.19.0.100", ovn-encap-tos="0", ovn-encap-type=geneve, ovn-match-northd-version=False, ovn-monitor-all=True, ovn-ofctrl-wait-before-clear="8000", ovn-openflow-probe-interval="60", ovn-remote="tcp:ovsdbserver-sb.openstack.svc:6642", ovn-remote-probe-interval="60000", rundir="/var/run/openvswitch", system-id="2eec68e6-aa21-4c95-a868-31aeafc11736"}
...
----
In above example bridge mappings are set as "datacentre:br-ctlplane" and it has to be set in the OpenStackDataPlaneNodeSet CR also.
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneNodeSet
metadata:
  name: openstack
spec:
  tlsEnabled: false
  networkAttachments:
      - ctlplane
  preProvisioned: true
  services:
    - bootstrap
    - download-cache
    - configure-network
    - validate-network
    - install-os
    - configure-os
    - ssh-known-hosts
    - run-os
    - install-certs
    - libvirt
    - nova-compute-extraconfig
    - ovn
    - neutron-metadata
  env:
    - name: ANSIBLE_CALLBACKS_ENABLED
      value: "profile_tasks"
    - name: ANSIBLE_FORCE_COLOR
      value: "True"
  nodes:
    standalone:
      hostName: standalone
      ansible:
        ansibleHost: ${computes[standalone.ooo.test]}
      networks:
      - defaultRoute: true
        fixedIP: ${computes[standalone.ooo.test]}
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
  nodeTemplate:
    ansibleSSHPrivateKeySecret: dataplane-adoption-secret
    ansible:
      ansibleUser: root
ifeval::["{build}" == "downstream"]
      ansibleVarsFrom:
      - prefix: subscription_manager_
        secretRef:
          name: subscription-manager
      - prefix: registry_
        secretRef:
          name: redhat-registry
endif::[]
      ansibleVars:
        service_net_map:
          nova_api_network: internalapi
          nova_libvirt_network: internalapi

        # edpm_network_config
        # Default nic config template for a EDPM compute node
        # These vars are edpm_network_config role vars
        edpm_network_config_template: |
           ---
           {% set mtu_list = [ctlplane_mtu] %}
           {% for network in nodeset_networks %}
           {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
           {%- endfor %}
           {% set min_viable_mtu = mtu_list | max %}
           network_config:
           - type: ovs_bridge
             name: {{ neutron_physical_bridge_name }}
             mtu: {{ min_viable_mtu }}
             use_dhcp: false
             dns_servers: {{ ctlplane_dns_nameservers }}
             domain: {{ dns_search_domains }}
             addresses:
             - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
             routes: {{ ctlplane_host_routes }}
             members:
             - type: interface
               name: nic1
               mtu: {{ min_viable_mtu }}
               # force the MAC address of the bridge to this interface
               primary: true
           {% for network in nodeset_networks %}
             - type: vlan
               mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
               vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
               addresses:
               - ip_netmask:
                   {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
               routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
           {% endfor %}

        edpm_network_config_hide_sensitive_logs: false
        #
        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ctlplane
        neutron_public_interface_name: eth0

        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false

        # edpm ovn-controller configuration
        edpm_ovn_bridge_mappings: ['datacentre:br-ctlplane']
        edpm_ovn_bridge: br-int
        edpm_ovn_encap_type: geneve
        ovn_match_northd_version: false
        ovn_monitor_all: true
        edpm_ovn_remote_probe_interval: 60000
        edpm_ovn_ofctrl_wait_before_clear: 8000

        timesync_ntp_servers:
ifeval::["{build}" != "downstream"]
        - hostname: pool.ntp.org
endif::[]
ifeval::["{build}" == "downstream"]
        - hostname: clock.redhat.com
        - hostname: clock2.redhat.com
endif::[]

ifeval::["{build}" != "downstream"]
        edpm_bootstrap_command: |
          # This is a hack to deploy RDO Delorean repos to RHEL as if it were Centos 9 Stream
          set -euxo pipefail
          curl -sL https://github.com/openstack-k8s-operators/repo-setup/archive/refs/heads/main.tar.gz | tar -xz
          python3 -m venv ./venv
          PBR_VERSION=0.0.0 ./venv/bin/pip install ./repo-setup-main
          # This is required for FIPS enabled until trunk.rdoproject.org
          # is not being served from a centos7 host, tracked by
          # https://issues.redhat.com/browse/RHOSZUUL-1517
          dnf -y install crypto-policies
          update-crypto-policies --set FIPS:NO-ENFORCE-EMS
          # FIXME: perform dnf upgrade for other packages in EDPM ansible
          # here we only ensuring that decontainerized libvirt can start
          ./venv/bin/repo-setup current-podified -b antelope -d centos9 --stream
          dnf -y upgrade openstack-selinux
          rm -f /run/virtlogd.pid
          rm -rf repo-setup-main
endif::[]
ifeval::["{build}" == "downstream"]
        edpm_bootstrap_command: |
          subscription-manager register --username {{ subscription_manager_username }} --password {{ subscription_manager_password }}
          subscription-manager release --set=9.2
          subscription-manager repos --disable=*
          subscription-manager repos --enable=rhel-9-for-x86_64-baseos-eus-rpms --enable=rhel-9-for-x86_64-appstream-eus-rpms --enable=rhel-9-for-x86_64-highavailability-eus-rpms --enable=openstack-17.1-for-rhel-9-x86_64-rpms --enable=fast-datapath-for-rhel-9-x86_64-rpms --enable=openstack-dev-preview-for-rhel-9-x86_64-rpms
          # FIXME: perform dnf upgrade for other packages in EDPM ansible
          # here we only ensuring that decontainerized libvirt can start
          dnf -y upgrade openstack-selinux
          rm -f /run/virtlogd.pid
          podman login -u {{ registry_username }} -p {{ registry_password }} registry.redhat.io
endif::[]

        gather_facts: false
        enable_debug: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges: ['192.168.122.0/24']
        # SELinux module
        edpm_selinux_mode: enforcing

        # Do not attempt OVS 3.2 major upgrades here
        edpm_ovs_packages:
        - openvswitch3.1
EOF
----

* Optionally enable neutron-sriov-nic-agent in the OpenStackDataPlaneNodeSet CR
+
[source,yaml]
----
oc patch openstackdataplanenodeset openstack --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-sriov"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_physical_device_mappings",
    "value": "dummy_sriov_net:dummy-dev"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_bandwidths",
    "value": "dummy-dev:40000000:40000000"
  }, {
    "op": "add",
    "path": "/spec/nodeTemplate/ansible/ansibleVars/edpm_neutron_sriov_agent_SRIOV_NIC_resource_provider_hypervisors",
    "value": "dummy-dev:standalone.ooo.test"
  }
]'
----

* Optionally enable neutron-dhcp in the OpenStackDataPlaneNodeSet CR
+
[source,yaml]
----
oc patch openstackdataplanenodeset openstack --type='json' --patch='[
  {
    "op": "add",
    "path": "/spec/services/-",
    "value": "neutron-dhcp"
  }]'
----

* Deploy OpenStackDataPlaneDeployment:
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack
spec:
  nodeSets:
  - openstack
EOF
----

* Adoption of the neutron-ovn-metadata-agent:
+
Neutron-ovn-metadata-agent running on the data plane nodes don't require any
additional actions nor config adjustments to do during the adoption process.
When OpenStackDataPlaneDeployment and OpenStackDataPlaneNodeSet will be ready,
neutron-ovn-metadata-agent should be up and running on the data plane nodes.

== Post-checks

* Check if all the Ansible EE pods reaches `Completed` status:
+
----
# watching the pods
watch oc get pod -l app=openstackansibleee
----
+
----
# following the ansible logs with:
oc logs -l app=openstackansibleee -f --max-log-requests 20
----

* Wait for the dataplane node set to reach the Ready status:
+
----
oc wait --for condition=Ready osdpns/openstack --timeout=30m
----

* Verify that neutron agents are alive:
+
----
oc exec openstackclient -- openstack network agent list
+--------------------------------------+------------------------------+---------------------+-------------------+-------+-------+----------------------------+
| ID                                   | Agent Type                   | Host                | Availability Zone | Alive | State | Binary                     |
+--------------------------------------+------------------------------+---------------------+-------------------+-------+-------+----------------------------+
| 174fc099-5cc9-4348-b8fc-59ed44fcfb0e | DHCP agent                   | standalone.ooo.test | nova              | :-)   | UP    | neutron-dhcp-agent         |
| 10482583-2130-5b0d-958f-3430da21b929 | OVN Metadata agent           | standalone.ooo.test |                   | :-)   | UP    | neutron-ovn-metadata-agent |
| a4f1b584-16f1-4937-b2b0-28102a3f6eaa | OVN Controller agent         | standalone.ooo.test |                   | :-)   | UP    | ovn-controller             |
+--------------------------------------+------------------------------+---------------------+-------------------+-------+-------+----------------------------+
----

== Nova compute services fast-forward upgrade from Wallaby to Antelope

Nova services rolling upgrade cannot be done during adoption,
there is in a lock-step with Nova control plane services, because those
are managed independently by EDPM ansible, and Kubernetes operators.
Nova service operator and OpenStack Dataplane operator ensure upgrading
is done independently of each other, by configuring
`[upgrade_levels]compute=auto` for Nova services. Nova control plane
services apply the change right after CR is patched. Nova compute data plane
services will catch up the same config change with ansible deployment
later on.

____
*NOTE*: Additional orchestration happening around the FFU workarounds
configuration for Nova compute data plane service is a subject of future changes.
____

* Wait for cell1 Nova compute data plane services version updated (it may take some time):
+
----
oc exec openstack-cell1-galera-0 -c galera -- mysql -rs -uroot -p$PODIFIED_DB_ROOT_PASSWORD \
    -e "select a.version from nova_cell1.services a join nova_cell1.services b where a.version!=b.version and a.binary='nova-compute';"
----
+
The above query should return an empty result as a completion criterion.

* Remove pre-FFU workarounds for Nova control plane services:
+
[source,yaml]
----
oc patch openstackcontrolplane openstack -n openstack --type=merge --patch '
spec:
  nova:
    template:
      cellTemplates:
        cell0:
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
        cell1:
          metadataServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
          conductorServiceTemplate:
            customServiceConfig: |
              [workarounds]
              disable_compute_service_check_for_ffu=false
      apiServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      metadataServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
      schedulerServiceTemplate:
        customServiceConfig: |
          [workarounds]
          disable_compute_service_check_for_ffu=false
'
----

* Wait for Nova control plane services' CRs to become ready:
+
----
oc wait --for condition=Ready --timeout=300s Nova/nova
----

* Remove pre-FFU workarounds for Nova compute data plane services:
+
[source,yaml]
----
oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: nova-compute-extraconfig
  namespace: openstack
data:
  20-nova-compute-cell1-workarounds.conf: |
    [workarounds]
    disable_compute_service_check_for_ffu=false
---
apiVersion: dataplane.openstack.org/v1beta1
kind: OpenStackDataPlaneDeployment
metadata:
  name: openstack-nova-compute-ffu
  namespace: openstack
spec:
  nodeSets:
    - openstack
  servicesOverride:
    - nova-compute-extraconfig
EOF
----

* Wait for Nova compute data plane service to become ready:
+
----
oc wait --for condition=Ready osdpd/openstack-nova-compute-ffu --timeout=5m
----

* Run Nova DB online migrations to complete FFU:
+
----
oc exec -it nova-cell0-conductor-0 -- nova-manage db online_data_migrations
oc exec -it nova-cell1-conductor-0 -- nova-manage db online_data_migrations
----

* Verify if Nova services can stop the existing test VM instance:
+
----
${BASH_ALIASES[openstack]} server list | grep -qF '| test | ACTIVE |' && ${BASH_ALIASES[openstack]} server stop test || echo PASS
${BASH_ALIASES[openstack]} server list | grep -qF '| test | SHUTOFF |' || echo FAIL
${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test 2>&1 || echo PASS
----

* Verify if Nova services can start the existing test VM instance:
+
----
${BASH_ALIASES[openstack]} server list | grep -qF '| test | SHUTOFF |' && ${BASH_ALIASES[openstack]} server start test || echo PASS
${BASH_ALIASES[openstack]} server list | grep -F '| test | ACTIVE |' && \
  ${BASH_ALIASES[openstack]} server --os-compute-api-version 2.48 show --diagnostics test --fit-width -f json | jq -r '.state' | grep running || echo FAIL
----
