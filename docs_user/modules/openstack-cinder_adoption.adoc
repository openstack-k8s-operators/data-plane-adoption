[id="adopting-the-block-storage-service_{context}"]

//:context: adopting-block-storage
//kgilliga: This module might be converted to an assembly, or a procedure as a standalone chapter.
//Check xref contexts.

= Adopting the Block Storage service

Adopting a director deployed Cinder service into OpenStack may require some
thought because it's not always a simple process.

Usually the adoption process entails:

* Checking existing limitations.
* Considering the placement of the cinder services.
* Preparing the OpenShift nodes where volume and backup services will run.
* Crafting the manifest based on the existing `cinder.conf` file.
* Deploying Cinder.
* Validating the new deployment.

This guide provides necessary knowledge to complete these steps in most
situations, but it still requires knowledge on how OpenStack services work and
the structure of a Cinder configuration file.

== Limitations

There are currently some limitations that are worth highlighting; some are
related to this guideline while some to the operator:

* There is no global `nodeSelector` for all cinder volumes, so it needs to be
specified per backend.  This may change in the future.
* There is no global `customServiceConfig` or `customServiceConfigSecrets` for
all cinder volumes, so it needs to be specified per backend.  This may change in
the future.
* Adoption of LVM backends, where the volume data is stored in the compute
nodes, is not currently being documented in this process. It may get documented
in the future.
* Support for Cinder backends that require kernel modules not included in RHEL
has not been tested in Operator deployed OpenStack so it is not documented in
this guide.
* Adoption of DCN/Edge deployment is not currently described in this guide.

== Prerequisites

* Previous Adoption steps completed. Notably, cinder service must have been
stopped and the service databases must already be imported into the podified
MariaDB.
* Keystone and Barbican should be already adopted.
* Storage network has been properly configured on the OpenShift cluster.

== Variables

No new environmental variables need to be defined, though you use the
`CONTROLLER1_SSH` that was defined in a previous step for the pre-checks.

== Pre-checks

You need the contents of `cinder.conf` file. Download the file so that you can access it locally:

----
$CONTROLLER1_SSH cat /var/lib/config-data/puppet-generated/cinder/etc/cinder/cinder.conf > cinder.conf
----

== Prepare OpenShift

As explained in xref:planning-the-new-deployment_{context}[Planning the new deployment], before deploying OpenStack in OpenShift, you must ensure that the networks are ready, that you have decided the node selection, and also make sure any necessary changes to the OpenShift nodes have been made. For Cinder volume and backup services all these 3 must be carefully considered.

=== Node Selection

You might need, or want, to restrict the OpenShift nodes where cinder volume and
backup services can run.

The best example of when you need to do node selection for a specific cinder
service is when you deploy Cinder with the LVM driver. In that scenario, the
LVM data where the volumes are stored only exists in a specific host, so you
need to pin the cinder-volume service to that specific OpenShift node. Running
the service on any other OpenShift node would not work.  Since `nodeSelector`
only works on labels, you cannot use the OpenShift host node name to restrict
the LVM backend and you need to identify it using a unique label, an existing label, or new label:

----
$ oc label nodes worker0 lvm=cinder-volumes
----

[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm-iscsi:
          nodeSelector:
            lvm: cinder-volumes
< . . . >
----

As mentioned in the xref:node-selector_{context}[About node selector], an example where you need to use labels is when using FC storage and you do not have HBA cards in all your OpenShift nodes. In this scenario you need to restrict all the cinder volume backends (not only the FC one) as well as the backup services.

Depending on the cinder backends, their configuration, and the usage of Cinder,
you can have network intensive cinder volume services with lots of I/O as well as
cinder backup services that are not only network intensive but also memory and
CPU intensive. This may be a concern for the OpenShift human operators, and
they may want to use the `nodeSelector` to prevent these service from
interfering with their other OpenShift workloads. For more information about node selection, see xref:node-selector_{context}[About node selector].

When selecting the nodes where cinder volume is going to run please remember
that cinder-volume may also use local storage when downloading a glance image
for the create volume from image operation, and it can require a considerable
amount of space when having concurrent operations and not using cinder volume
cache.

If you do not have nodes with enough local disk space for the temporary images, you can use a remote NFS location for the images. You had to manually set this up in Director deployments, but with operators, you can do it
automatically using the extra volumes feature ()`extraMounts`.

=== Transport protocols

Due to the specifics of the storage transport protocols some changes may be
required on the OpenShift side, and although this is something that must be
documented by the Vendor here wer are going to provide some generic
instructions that can serve as a guide for the different transport protocols.

Check the backend sections in your `cinder.conf` file that are listed in the
`enabled_backends` configuration option to figure out the transport storage
protocol used by the backend.

Depending on the backend, you can find the transport protocol:

* Looking at the `volume_driver` configuration option, as it may contain the
protocol itself: RBD, iSCSI, FC...
* Looking at the `target_protocol` configuration option

WARNING: Any time a `MachineConfig` is used to make changes to OpenShift
nodes the node will reboot!!  Act accordingly.

==== NFS

There is nothing to do for NFS. OpenShift can connect to NFS backends without
any additional changes.

==== RBD/Ceph

There is nothing to do for RBD/Ceph in terms of preparing the nodes, OpenShift
can connect to Ceph backends without any additional changes. Credentials and
configuration files will need to be provided to the services though.

==== iSCSI

Connecting to iSCSI volumes requires that the iSCSI initiator is running on the
OpenShift hosts where volume and backup services are going to run, because
the Linux Open iSCSI initiator does not currently support network namespaces, so
you must only run 1 instance of the service for the normal OpenShift usage, plus
the OpenShift CSI plugins, plus the OpenStack services.

If you are not already running `iscsid` on the OpenShift nodes, then you need
to apply a `MachineConfig` similar to this one:

[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-iscsid
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: iscsid.service
----

If you are using labels to restrict the nodes where cinder services are running you need to use a `MachineConfigPool` as described in
the xref:node-selector_{context}[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.

If you are using a toy single node deployment to test the process, you might need to replace `worker` with `master` in the `MachineConfig`.

//For production deployments using iSCSI volumes, we always recommend setting up
//multipathing, please look at the <<multipathing,multipathing section>> to see
//how to configure it. kgilliga: Commented out because multipathing module doesn't exist yet. Update with xref for beta.

//*TODO:* Add, or at least mention, the Nova eDPM side for iSCSI.

==== FC

There is nothing to do for FC volumes to work, but the _cinder volume and cinder
backup services need to run in an OpenShift host that has HBAs_, so if there
are nodes that do not have HBAs then you need to use labels to restrict where
these services can run, as mentioned in the [node selection section]
(#node-selection).

This also means that for virtualized OpenShift clusters using FC you need to
expose the host's HBAs inside the VM.

//For production deployments using FC volumes we always recommend setting up
//multipathing, please look at the <<multipathing,multipathing section>> to see
//how to configure it. kgilliga: Commented out because multipathing module doesn't exist yet. Update with xref for beta.

==== NVMe-oF

Connecting to NVMe-oF volumes requires that the nvme kernel modules are loaded
on the OpenShift hosts.

If you are not already loading the `nvme-fabrics` module on the OpenShift nodes
where volume and backup services are going to run then you need to apply a
`MachineConfig` similar to this one:

----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-load-nvme-fabrics
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modules-load.d/nvme_fabrics.conf
          overwrite: false
          # Mode must be decimal, this is 0644
          mode: 420
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,nvme-fabrics
----

If you are using labels to restrict the nodes where cinder
services are running, you need to use a `MachineConfigPool` as described in
the xref:node-selector_{context}[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.

If you are using a toy single node deployment to test the process you migt need to replace `worker` with `master` in the `MachineConfig`.

You are only loading the `nvme-fabrics` module because it takes care of loading
the transport specific modules (tcp, rdma, fc) as needed.

For production deployments using NVMe-oF volumes it is recommended that you use
multipathing. For NVMe-oF volumes OpenStack uses native multipathing, called
https://nvmexpress.org/faq-items/what-is-ana-nvme-multipathing/[ANA].

Once the OpenShift nodes have rebooted and are loading the `nvme-fabrics` module
you can confirm that the Operating System is configured and supports ANA by
checking on the host:

----
cat /sys/module/nvme_core/parameters/multipath
----

IMPORTANT: ANA doesn't use the Linux Multipathing Device Mapper, but the
*current OpenStack
code requires `multipathd` on compute nodes to be running for Nova to be able to
use multipathing, so please remember to follow the multipathing part for compute
nodes on the <<multipathing,multipathing section>>.

//*TODO:* Add, or at least mention, the Nova eDPM side for NVMe-oF.

==== Multipathing

For iSCSI and FC protocols, using multipathing is recommended, which
has 4 parts:

* Prepare the OpenShift hosts
* Configure the Cinder services
* Prepare the Nova computes
* Configure the Nova service

To prepare the OpenShift hosts, you need to ensure that the Linux Multipath
Device Mapper is configured and running on the OpenShift hosts, and you do
that using `MachineConfig` like this one:

[source,yaml]
----
# Includes the /etc/multipathd.conf contents and the systemd unit changes
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-multipathd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/multipath.conf
          overwrite: false
          # Mode must be decimal, this is 0600
          mode: 384
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,defaults%20%7B%0A%20%20user_friendly_names%20no%0A%20%20recheck_wwid%20yes%0A%20%20skip_kpartx%20yes%0A%20%20find_multipaths%20yes%0A%7D%0A%0Ablacklist%20%7B%0A%7D
    systemd:
      units:
      - enabled: true
        name: multipathd.service
----

If you are using labels to restrict the nodes where cinder
services are running you need to use a `MachineConfigPool` as described in
the xref:node-selector_{context}[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.

If you are using a toy single node deployment to test the process you might need to replace `worker` with `master` in the `MachineConfig`.

To configure the cinder services to use multipathing you need to enable the
`use_multipath_for_image_xfer` configuration option in all the backend sections
and in the `[DEFAULT]` section for the backup service, but in Podified
deployments you do not need to worry about it, because that's the default. So as
long as you do not override it setting `use_multipath_for_image_xfer = false` then multipathing will work as long as the service is running on the OpenShift host.

//*TODO:* Add, or at least mention, the Nova eDPM side for Multipathing once
//it's implemented.

== Configurations

As described in xref:planning-the-new-deployment_{context}[Planning the new deployment], Cinder is configured using
configuration snippets instead of using obscure configuration parameters
defined by the installer.

The recommended way to deploy Cinder volume backends has changed to remove old
limitations, add flexibility, and improve operations in general.

When deploying with Director you used to run a single Cinder volume service with
all your backends (each backend would run on its own process), and even though
that way of deploying is still supported, it is not recommended. It is recommended to use a volume service per backend since it is a superior deployment model.

So for an LVM and a Ceph backend you would have 2 entries in `cinderVolume` and,
as mentioned in the limitations section, you cannot set global defaults for all
volume services, so you have to define it for each of them, like this:

[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    enabled: true
    template:
      cinderVolume:
        lvm:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [lvm]
< . . . >
        ceph:
          customServiceConfig: |
            [DEFAULT]
            debug = True
            [ceph]
< . . . >
----

Reminder that for volume backends that have sensitive information using `Secret`
and the `customServiceConfigSecrets` key is the recommended way to go.

== Prepare the configuration

For adoption instead of using a whole deployment manifest you use a targeted
patch, like you did with other services, and in this patch you will enable the
different cinder services with their specific configurations.

*WARNING:* Check that all configuration options are still valid for the new
OpenStack version, since configuration options may have been deprecated,
removed, or added. This applies to both backend driver specific configuration
options and other generic options.

There are 2 ways to prepare a cinder configuration for adoption, tailor-making
it or doing it quick and dirty. There is no difference in how Cinder will
operate with both methods, though tailor-making it is recommended whenever possible.

The high level explanation of the tailor-made approach is:

. Determine what part of the configuration is generic for all the cinder
services and remove anything that would change when deployed in OpenShift, like
the `connection` in the `[dabase]` section, the `transport_url` and `log_dir` in
`[DEFAULT]`, the whole `[coordination]` and `[barbican]` sections.  This
configuration goes into the `customServiceConfig` (or a `Secret` and then used
in `customServiceConfigSecrets`) at the `cinder: template:` level.
. Determine if there's any scheduler specific configuration and add it to the
`customServiceConfig` section in `cinder: template: cinderScheduler`.
. Determine if there's any API specific configuration and add it to the
`customServiceConfig` section in `cinder: template: cinderAPI`.
. If you have cinder backup deployed, then you get the cinder backup relevant
configuration options and add them to `customServiceConfig` (or a `Secret` and
then used in `customServiceConfigSecrets`) at the `cinder: template:
cinderBackup:` level. You should remove the `host` configuration in the
`[DEFAULT]` section to facilitate supporting multiple replicas in the future.
. Determine the individual volume backend configuration for each of the
drivers. The configuration will not only be the specific driver section, it
should also include the `[backend_defaults]` section and FC zoning sections is
they are being used, because the cinder operator doesn't support a
`customServiceConfig` section global for all volume services.  Each backend
would have its own section under `cinder: template: cinderVolumes` and the
configuration would go in `customServiceConfig` (or a `Secret` and then used in
`customServiceConfigSecrets`).
. Check if any of the cinder volume drivers being used requires a custom vendor
image. If they do, find the location of the image in the vendor's instruction
available in the w https://catalog.redhat.com/software/search?target_platforms=Red%20Hat%20OpenStack%20Platform&p=1&functionalCategories=Data%20storage[OpenStack Cinder ecosystem
page]
and add it under the specific's driver section using the `containerImage` key.
For example, if you had a Pure Storage array and the driver was already certified
for OSP18, then you would have something like this:
+
[source,yaml]
----
spec:
  cinder:
    enabled: true
    template:
      cinderVolume:
        pure:
          containerImage: registry.connect.redhat.com/purestorage/openstack-cinder-volume-pure-rhosp-18-0'
          customServiceConfigSecrets:
            - openstack-cinder-pure-cfg
< . . . >
----

. External files: Cinder services sometimes use external files, for example for
a custom policy, or to store credentials, or SSL CA bundles to connect to a
storage array, and you need to make those files available to the right
containers. To achieve this, you use `Secrets` or `ConfigMap` to store the
information in OpenShift and then the `extraMounts` key. For example, for the
Ceph credentials stored in a `Secret` called `ceph-conf-files` you patch
the top level `extraMounts` in `OpenstackControlPlane`:
+
[source,yaml]
----
spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
----
+
But for a service specific one, like the API policy, you do it directly
on the service itself. In this example, you include the cinder API
configuration that references the policy you are adding from a `ConfigMap`
called `my-cinder-conf` that has a key `policy` with the contents of the
policy:
+
[source,yaml]
----
spec:
  cinder:
    enabled: true
    template:
      cinderAPI:
        customServiceConfig: |
           [oslo_policy]
           policy_file=/etc/cinder/api/policy.yaml
      extraMounts:
      - extraVol:
        - extraVolType: Ceph
          mounts:
          - mountPath: /etc/cinder/api
            name: policy
            readOnly: true
          propagation:
          - CinderAPI
          volumes:
          - name: policy
            projected:
              sources:
              - configMap:
                  name: my-cinder-conf
                  items:
                    - key: policy
                      path: policy.yaml
----

The quick and dirty process is more straightforward:

. Create an agnostic configuration file removing any specifics from the old
deployment's `cinder.conf` file, like the `connection` in the `[dabase]`
section, the `transport_url` and `log_dir` in `[DEFAULT]`, the whole
`[coordination]` and `[barbican]` sections, etc..
. Assuming the configuration has sensitive information, drop the modified
contents of the whole file into a `Secret`.
. Reference this secret in all the services, creating a cinder volumes section
for each backend and just adding the respective `enabled_backends` option.
. Add external files as mentioned in the last bullet of the tailor-made
configuration explanation.

Example of what the quick and dirty configuration patch would look like:

[source,yaml]
----
   spec:
     cinder:
       enabled: true
       template:
         cinderAPI:
           customServiceConfigSecrets:
             - cinder-conf
         cinderScheduler:
           customServiceConfigSecrets:
             - cinder-conf
         cinderBackup:
           customServiceConfigSecrets:
             - cinder-conf
         cinderVolume:
           lvm1:
             customServiceConfig: |
               [DEFAULT]
               enabled_backends = lvm1
             customServiceConfigSecrets:
               - cinder-conf
           lvm2:
             customServiceConfig: |
               [DEFAULT]
               enabled_backends = lvm2
             customServiceConfigSecrets:
               - cinder-conf
----

=== Configuration generation helper tool

Creating the right Cinder configuration files to deploy using Operators may
sometimes be a complicated experience, especially the first times, so you have a
helper tool that can create a draft of the files from a `cinder.conf` file.

This tool is not meant to be a automation tool. It is mostly to help you get the
gist of it, maybe point out some potential pitfalls and reminders.

IMPORTANT: The tools requires `PyYAML` Python package to be installed (`pip
install PyYAML`).

This link:helpers/cinder-cfg.py[cinder-cfg.py script] defaults to reading the
`cinder.conf` file from the current directory (unless `--config` option is used)
and outputs files to the current directory (unless `--out-dir` option is used).

In the output directory you always get a `cinder.patch` file with the Cinder
specific configuration patch to apply to the `OpenStackControlPlane` CR but you might also get an additional file called `cinder-prereq.yaml` file with some
`Secrets` and `MachineConfigs`.

Example of an invocation setting input and output explicitly to the defaults for
a Ceph backend:

----
$ python cinder-cfg.py --config cinder.conf --out-dir ./
WARNING:root:Cinder is configured to use ['/etc/cinder/policy.yaml'] as policy file, please ensure this file is available for the podified cinder services using "extraMounts" or remove the option.

WARNING:root:Deployment uses Ceph, so make sure the Ceph credentials and configuration are present in OpenShift as a asecret and then use the extra volumes to make them available in all the services that would need them.

WARNING:root:You were using user ['nova'] to talk to Nova, but in podified using the service keystone username is preferred in this case ['cinder']. Dropping that configuration.

WARNING:root:ALWAYS REVIEW RESULTS, OUTPUT IS JUST A ROUGH DRAFT!!

Output written at ./: cinder.patch
----

The script outputs some warnings to let you know things that you might need to do
manually -adding the custom policy, provide the ceph configuration files- and
also let you know a change in how the `service_user` has been removed.

A different example when using multiple backends, one of them being a 3PAR FC
could be:

----
$ python cinder-cfg.py --config cinder.conf --out-dir ./
WARNING:root:Cinder is configured to use ['/etc/cinder/policy.yaml'] as policy file, please ensure this file is available for the podified cinder services using "extraMounts" or remove the option.

ERROR:root:Backend hpe_fc requires a vendor container image, but there is no certified image available yet. Patch will use the last known image for reference, but IT WILL NOT WORK

WARNING:root:Deployment uses Ceph, so make sure the Ceph credentials and configuration are present in OpenShift as a asecret and then use the extra volumes to make them available in all the services that would need them.

WARNING:root:You were using user ['nova'] to talk to Nova, but in podified using the service keystone username is preferred, in this case ['cinder']. Dropping that configuration.

WARNING:root:Configuration is using FC, please ensure all your OpenShift nodes have HBAs or use labels to ensure that Volume and Backup services are scheduled on nodes with HBAs.

WARNING:root:ALWAYS REVIEW RESULTS, OUTPUT IS JUST A ROUGH DRAFT!!

Output written at ./: cinder.patch, cinder-prereq.yaml
----

In this case there are additional messages. The following list provides an explanation of each one:

* There is one message mentioning how this backend driver needs external vendor
dependencies so the standard container image will not work. Unfortunately this
image is still not available, so an older image is used in the output patch file
for reference. You can then replace this image with one that you build or
with a Red Hat official image once the image is available. In this case you can see in your `cinder.patch` file:
+
[source,yaml]
----
      cinderVolumes:
      hpe-fc:
        containerImage: registry.connect.redhat.com/hpe3parcinder/openstack-cinder-volume-hpe3parcinder17-0
----

* The FC message reminds you that this transport protocol requires specific HBA
cards to be present on the nodes where cinder services are running.
* In this case it has created the `cinder-prereq.yaml` file and within the file
there is one `MachineConfig` and one `Secret`. The `MachineConfig` is called `99-master-cinder-enable-multipathd` and like the name suggests enables multipathing on all the OCP worker nodes. The `Secret` is
called `openstackcinder-volumes-hpe_fc` and contains the 3PAR backend
configuration because it has sensitive information (credentials). The
`cinder.patch` file uses the following configuration:
+
[source,yaml]
----
   cinderVolumes:
      hpe-fc:
        customServiceConfigSecrets:
        - openstackcinder-volumes-hpe_fc
----

== Procedure - Cinder adoption

Assuming you have already stopped cinder services, prepared the OpenShift nodes,
deployed the OpenStack operators and a bare OpenStack manifest, and migrated the
database, and prepared the patch manifest with the Cinder service configuration,
you must apply the patch and wait for the operator to apply the changes and deploy the Cinder services.
//The above first sentence could be turned into prerequisites.

It is recommended to write the patch manifest into a file, for example
`cinder.patch` and then apply it with something like:

----
oc patch openstackcontrolplane openstack --type=merge --patch-file=cinder.patch
----

For example, for the RBD deployment from the Development Guide the
`cinder.patch` would look like this:

[source,yaml]
----
spec:
  extraMounts:
  - extraVol:
    - extraVolType: Ceph
      mounts:
      - mountPath: /etc/ceph
        name: ceph
        readOnly: true
      propagation:
      - CinderVolume
      - CinderBackup
      - Glance
      volumes:
      - name: ceph
        projected:
          sources:
          - secret:
              name: ceph-conf-files
  cinder:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      databaseAccount: cinder
      secret: osp-secret
      cinderAPI:
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          default_volume_type=tripleo
      cinderScheduler:
        replicas: 1
      cinderBackup:
        networkAttachments:
        - storage
        replicas: 1
        customServiceConfig: |
          [DEFAULT]
          backup_driver=cinder.backup.drivers.ceph.CephBackupDriver
          backup_ceph_conf=/etc/ceph/ceph.conf
          backup_ceph_user=openstack
          backup_ceph_pool=backups
      cinderVolumes:
        ceph:
          networkAttachments:
          - storage
          replicas: 1
          customServiceConfig: |
            [tripleo_ceph]
            backend_host=hostgroup
            volume_backend_name=tripleo_ceph
            volume_driver=cinder.volume.drivers.rbd.RBDDriver
            rbd_ceph_conf=/etc/ceph/ceph.conf
            rbd_user=openstack
            rbd_pool=volumes
            rbd_flatten_volume_from_snapshot=False
            report_discard_supported=True
----

Once the services have been deployed you need to clean up the old scheduler
and backup services which will appear as being down while you have others that
appear as being up:

----
openstack volume service list

+------------------+------------------------+------+---------+-------+----------------------------+
| Binary           | Host                   | Zone | Status  | State | Updated At                 |
+------------------+------------------------+------+---------+-------+----------------------------+
| cinder-backup    | standalone.ooo.test | nova | enabled | down  | 2023-06-28T11:00:59.000000 |
| cinder-scheduler | standalone.ooo.test | nova | enabled | down  | 2023-06-28T11:00:29.000000 |
| cinder-volume    | hostgroup@tripleo_ceph | nova | enabled | up    | 2023-06-28T17:00:03.000000 |
| cinder-scheduler | cinder-scheduler-0     | nova | enabled | up    | 2023-06-28T17:00:02.000000 |
| cinder-backup    | cinder-backup-0        | nova | enabled | up    | 2023-06-28T17:00:01.000000 |
+------------------+------------------------+------+---------+-------+----------------------------+
----

In this case you need to remove services for hosts `standalone.ooo.test`

----
oc exec -it cinder-scheduler-0 -- cinder-manage service remove cinder-backup standalone.ooo.test
oc exec -it cinder-scheduler-0 -- cinder-manage service remove cinder-scheduler standalone.ooo.test
----

The reason why we haven't preserved the name of the backup service is because
we have taken the opportunity to change its configuration to support
Active-Active, even though we are not doing so right now because we have 1
replica.
//kgilliga: The above paragraph is confusing. Who changed the configuration? Unclear on what the replica refers to.

Now that the Cinder services are running, the DB schema migration has been completed and you can proceed to apply the DB data migrations.
While it is not necessary to run these data migrations at this precise moment,
because you can run them right before the next upgrade, for adoption it is best to run them now to make sure there are no issues before running production workloads on the deployment.

The command to run the DB data migrations is:

----
oc exec -it cinder-scheduler-0 -- cinder-manage db online_data_migrations
----

== Post-checks

Before you can run any checks you need to set the right cloud configuration for
the `openstack` command to be able to connect to your OpenShift control plane.

Ensure that the `openstack` alias is defined:

----
alias openstack="oc exec -t openstackclient -- openstack"
----

Now you can run a set of tests to confirm that the deployment is using your
old database contents:

* See that Cinder endpoints are defined and pointing to the podified
FQDNs:
+
----
openstack endpoint list --service cinderv3
----

* Check that the cinder services are running and up. The API won't show but if
you get a response you know it's up as well:
+
----
openstack volume service list
----

* Check that your old volume types, volumes, snapshots, and backups are there:
+
----
openstack volume type list
openstack volume list
openstack volume snapshot list
openstack volume backup list
----

To confirm that the configuration is working, the following basic operations are recommended:

* Create a volume from an image to check that the connection to glance is
working.
+
----
openstack volume create --image cirros --bootable --size 1 disk_new
----

* Backup the old attached volume to a new backup. Example:
+
----
openstack --os-volume-api-version 3.47 volume create --backup backup restored
----

You do not boot a nova instance using the new volume from image or try to detach
the old volume because nova and cinder are still not connected.
