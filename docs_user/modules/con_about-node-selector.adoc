[id="about-node-selector_{context}"]

= About node selector

There are a variety of reasons why you might want to restrict the nodes where
OpenStack services can be placed:

* Hardware requirements: System memory, Disk space, Cores, HBAs
* Limit the impact of the OpenStack services on other OpenShift workloads.
* Avoid collocating OpenStack services.

The mechanism provided by the OpenStack operators to achieve this is through the
use of labels.

You either label the OpenShift nodes or use existing labels, and then use those labels in the OpenStack manifests in the
`nodeSelector` field.

The `nodeSelector` field in the OpenStack manifests follows the standard
OpenShift `nodeSelector` field, please refer to https://docs.openshift.com/container-platform/4.13/nodes/scheduling/nodes-scheduler-node-selectors.html[the OpenShift documentation on
the matter]
additional information.

This field is present at all the different levels of the OpenStack manifests:

* Deployment: The `OpenStackControlPlane` object.
* Component: For example the `cinder` element in the `OpenStackControlPlane`.
* Service: For example the `cinderVolume` element within the `cinder` element
in the `OpenStackControlPlane`.

This allows a fine grained control of the placement of the OpenStack services
with minimal repetition.

Values of the `nodeSelector` are propagated to the next levels unless they are
overwritten. This means that a `nodeSelector` value at the deployment level will
affect all the OpenStack services.

For example, you can add label `type: openstack` to any 3 OpenShift nodes:

----
$ oc label nodes worker0 type=openstack
$ oc label nodes worker1 type=openstack
$ oc label nodes worker2 type=openstack
----

And then in our `OpenStackControlPlane` you can use the label to place all the
services in those 3 nodes:

[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  nodeSelector:
    type: openstack
< . . . >
----

You can use the selector for specific services. For example, you might want to place your the Block Storage service (cinder) volume and backup services on certain nodes if you are using FC and only have HBAs on a subset of
nodes. The following example assumes that you have the label `fc_card: true`:

[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    template:
      cinderVolumes:
          pure_fc:
            nodeSelector:
              fc_card: true
< . . . >
          lvm-iscsi:
            nodeSelector:
              fc_card: true
< . . . >
      cinderBackup:
          nodeSelector:
            fc_card: true
< . . . >
----

The Block Storage service operator does not currently have the possibility of defining
the `nodeSelector` in `cinderVolumes`, so you need to specify it on each of the
backends.

It's possible to leverage labels added by https://docs.openshift.com/container-platform/4.13/hardware_enablement/psap-node-feature-discovery-operator.html[the node feature discovery
operator]
to place OpenStack services.
