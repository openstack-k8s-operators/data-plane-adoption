[id="openshift-preparation-for-block-storage-adoption_{context}"]

= {OpenShift} preparation for {block_storage} adoption

Before deploying {rhos_prev_long} ({OpenStackShort}) in {OpenShift}, you must ensure that the networks are ready, that you have decided the node selection, and also make sure any necessary changes to the {OpenShiftShort} nodes have been made. For {block_storage_first_ref} volume and backup services all these 3 must be carefully considered.

Node Selection::
You might need, or want, to restrict the {OpenShiftShort} nodes where {block_storage} volume and
backup services can run.
+
The best example of when you need to do node selection for a specific {block_storage} is when you deploy the {block_storage} with the LVM driver. In that scenario, the
LVM data where the volumes are stored only exists in a specific host, so you
need to pin the Block Storage-volume service to that specific {OpenShiftShort} node. Running
the service on any other {OpenShiftShort} node would not work.  Since `nodeSelector`
only works on labels, you cannot use the {OpenShiftShort} host node name to restrict
the LVM backend and you need to identify it using a unique label, an existing label, or new label:
+
----
$ oc label nodes worker0 lvm=cinder-volumes
----
+
[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  secret: osp-secret
  storageClass: local-storage
  cinder:
    enabled: true
    template:
      cinderVolumes:
        lvm-iscsi:
          nodeSelector:
            lvm: cinder-volumes
< . . . >
----
+
As mentioned in the xref:about-node-selector_planning[About node selector], an example where you need to use labels is when using FC storage and you do not have HBA cards in all your {OpenShiftShort} nodes. In this scenario you need to restrict all the {block_storage} volume backends (not only the FC one) as well as the backup services.
+
Depending on the {block_storage} backends, their configuration, and the usage of {block_storage},
you can have network intensive {block_storage} volume services with lots of I/O as well as
{block_storage} backup services that are not only network intensive but also memory and
CPU intensive. This may be a concern for the {OpenShiftShort} human operators, and
they may want to use the `nodeSelector` to prevent these service from
interfering with their other {OpenShiftShort} workloads. For more information about node selection, see xref:about-node-selector_planning[About node selector].
+
When selecting the nodes where the {block_storage} volume is going to run remember that {block_storage}-volume may also use local storage when downloading a {image_service_first_ref} image for the create volume from image operation, and it can require a considerable
amount of space when having concurrent operations and not using {block_storage} volume
cache.
+
If you do not have nodes with enough local disk space for the temporary images, you can use a remote NFS location for the images. You had to manually set this up in {OpenStackPreviousInstaller} deployments, but with operators, you can do it
automatically using the extra volumes feature ()`extraMounts`.

Transport protocols::
Due to the specifics of the storage transport protocols some changes may be
required on the {OpenShiftShort} side, and although this is something that must be
documented by the Vendor here wer are going to provide some generic
instructions that can serve as a guide for the different transport protocols.
+
Check the backend sections in your `cinder.conf` file that are listed in the
`enabled_backends` configuration option to figure out the transport storage
protocol used by the backend.
+
Depending on the backend, you can find the transport protocol:
+
* Looking at the `volume_driver` configuration option, as it may contain the
protocol itself: RBD, iSCSI, FC...
* Looking at the `target_protocol` configuration option
+
WARNING: Any time a `MachineConfig` is used to make changes to {OpenShiftShort}
nodes the node will reboot!!  Act accordingly.

NFS::
There is nothing to do for NFS. {OpenShiftShort} can connect to NFS backends without
any additional changes.

RBD/Ceph::
There is nothing to do for RBD/Ceph in terms of preparing the nodes, {OpenShiftShort}
can connect to Ceph backends without any additional changes. Credentials and
configuration files will need to be provided to the services though.

iSCSI::
Connecting to iSCSI volumes requires that the iSCSI initiator is running on the
{OpenShiftShort} hosts where volume and backup services are going to run, because
the Linux Open iSCSI initiator does not currently support network namespaces, so
you must only run 1 instance of the service for the normal {OpenShiftShort} usage, plus
the {OpenShiftShort} CSI plugins, plus the {OpenStackShort} services.
+
If you are not already running `iscsid` on the {OpenShiftShort} nodes, then you need
to apply a `MachineConfig` similar to this one:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-iscsid
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: iscsid.service
----
+
If you are using labels to restrict the nodes where the Block Storage services are running you need to use a `MachineConfigPool` as described in
the xref:about-node-selector_planning[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.
+
If you are using a single node deployment to test the process, replace `worker` with `master` in the `MachineConfig`.

//For production deployments using iSCSI volumes, we always recommend setting up
//multipathing, please look at the <<multipathing,multipathing section>> to see
//how to configure it. kgilliga: Commented out because multipathing module doesn't exist yet. Update with xref for beta.

//*TODO:* Add, or at least mention, the Nova eDPM side for iSCSI.

FC::
There is nothing to do for FC volumes to work, but the {block_storage} volume and {block_storage} backup services need to run in an {OpenShiftShort} host that has HBAs, so if there
are nodes that do not have HBAs then you need to use labels to restrict where
these services can run, as mentioned in xref:about-node-selector_planning[About node selector].
+
This also means that for virtualized {OpenShiftShort} clusters using FC you need to
expose the host's HBAs inside the VM.

//For production deployments using FC volumes we always recommend setting up
//multipathing, please look at the <<multipathing,multipathing section>> to see
//how to configure it. kgilliga: Commented out because multipathing module doesn't exist yet. Update with xref for beta.

NVMe-oF::
Connecting to NVMe-oF volumes requires that the nvme kernel modules are loaded
on the {OpenShiftShort} hosts.
+
If you are not already loading the `nvme-fabrics` module on the {OpenShiftShort} nodes
where volume and backup services are going to run then you need to apply a
`MachineConfig` similar to this one:
+
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-load-nvme-fabrics
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modules-load.d/nvme_fabrics.conf
          overwrite: false
          # Mode must be decimal, this is 0644
          mode: 420
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,nvme-fabrics
----
+
If you are using labels to restrict the nodes where Block Storage
services are running, you need to use a `MachineConfigPool` as described in
the xref:about-node-selector_planning[About node selector] to limit the effects of the
`MachineConfig` to only the nodes where your services may run.
+
If you are using a single node deployment to test the process,replace `worker` with `master` in the `MachineConfig`.
+
You are only loading the `nvme-fabrics` module because it takes care of loading
the transport specific modules (tcp, rdma, fc) as needed.
+
ifeval::["{build}" != "downstream"]
For production deployments using NVMe-oF volumes it is recommended that you use
multipathing. For NVMe-oF volumes {OpenStackShort} uses native multipathing, called
https://nvmexpress.org/faq-items/what-is-ana-nvme-multipathing/[ANA].
endif::[]
ifeval::["{build}" != "upstream"]
For production deployments using NVMe-oF volumes it is recommended that you use
multipathing. For NVMe-oF volumes {OpenStackShort} uses native multipathing, called ANA.
endif::[]
+
Once the {OpenShiftShort} nodes have rebooted and are loading the `nvme-fabrics` module
you can confirm that the Operating System is configured and supports ANA by
checking on the host:
+
----
cat /sys/module/nvme_core/parameters/multipath
----
+
IMPORTANT: ANA doesn't use the Linux Multipathing Device Mapper, but the
*current {OpenStackShort}
code requires `multipathd` on compute nodes to be running for {compute_service_first_ref} to be able to
use multipathing, so please remember to follow the multipathing part for compute
nodes on the <<multipathing,multipathing section>>.

//*TODO:* Add, or at least mention, the Nova eDPM side for NVMe-oF.

Multipathing::
For iSCSI and FC protocols, using multipathing is recommended, which
has 4 parts:

* Prepare the {OpenShiftShort} hosts
* Configure the Block Storage services
* Prepare the {compute_service} computes
* Configure the {compute_service} service
+
To prepare the {OpenShiftShort} hosts, you need to ensure that the Linux Multipath
Device Mapper is configured and running on the {OpenShiftShort} hosts, and you do
that using `MachineConfig` like this one:
+
[source,yaml]
----
# Includes the /etc/multipathd.conf contents and the systemd unit changes
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
    service: cinder
  name: 99-master-cinder-enable-multipathd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/multipath.conf
          overwrite: false
          # Mode must be decimal, this is 0600
          mode: 384
          user:
            name: root
          group:
            name: root
          contents:
            # Source can be a http, https, tftp, s3, gs, or data as defined in rfc2397.
            # This is the rfc2397 text/plain string format
            source: data:,defaults%20%7B%0A%20%20user_friendly_names%20no%0A%20%20recheck_wwid%20yes%0A%20%20skip_kpartx%20yes%0A%20%20find_multipaths%20yes%0A%7D%0A%0Ablacklist%20%7B%0A%7D
    systemd:
      units:
      - enabled: true
        name: multipathd.service
----
+
If you are using labels to restrict the nodes where Block Storage
services are running you need to use a `MachineConfigPool` as described in
the xref:about-node-selector_planning[About node selector] to limit the effects of the 
`MachineConfig` to only the nodes where your services may run.
+
If you are using a single node deployment to test the process, replace `worker` with `master` in the `MachineConfig`.
+
To configure the Block Storage services to use multipathing, enable the
`use_multipath_for_image_xfer` configuration option in all the backend sections
and in the `[DEFAULT]` section for the backup service. This is the default in control plane deployments. Multipathing works as long as the service is running on the {OpenShiftShort} host. Do not override this option by setting `use_multipath_for_image_xfer = false`. 

//*TODO:* Add, or at least mention, the Nova eDPM side for Multipathing once
//it's implemented.
