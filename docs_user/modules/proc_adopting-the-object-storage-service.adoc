:_mod-docs-content-type: PROCEDURE
[id="adopting-the-object-storage-service_{context}"]

= Adopting the {object_storage}

[role="_abstract"]
If you are using Object Storage as a service, adopt the {object_storage_first_ref} to the {rhos_long} environment. If you are using the Object Storage API of the Ceph Object Gateway (RGW), skip the following procedure.

.Prerequisites

* The {object_storage} storage back-end services are running in the {rhos_prev_long} ({OpenStackShort}) deployment.
* The storage network is properly configured on the {rhocp_long} cluster. For more information, see link:{deploying-rhoso}/assembly_preparing-rhocp-for-rhoso#proc_configuring-the-data-plane-network_preparing[Preparing Red Hat OpenShift Container Platform for Red Hat OpenStack Services on OpenShift] in _{deploying-rhoso-t}_.

.Procedure

. Create the `swift-conf` secret that includes the {object_storage} hash path suffix and prefix:
+
----
$ oc apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: swift-conf
type: Opaque
data:
  swift.conf: $($CONTROLLER1_SSH sudo cat /var/lib/config-data/puppet-generated/swift/etc/swift/swift.conf | base64 -w0)
EOF
----

. Create the `swift-ring-files` `ConfigMap` that includes the {object_storage} ring files:
+
----
$ oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: swift-ring-files
binaryData:
  swiftrings.tar.gz: $($CONTROLLER1_SSH "cd /var/lib/config-data/puppet-generated/swift/etc/swift && tar cz *.builder *.ring.gz backups/ | base64 -w0")
  account.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/account.ring.gz")
  container.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/container.ring.gz")
  object.ring.gz: $($CONTROLLER1_SSH "base64 -w0 /var/lib/config-data/puppet-generated/swift/etc/swift/object.ring.gz")
EOF
----

. Patch the `OpenStackControlPlane` custom resource to deploy the {object_storage}:
+
----
$ oc patch openstackcontrolplane openstack --type=merge --patch '
spec:
  swift:
    enabled: true
    template:
      memcachedInstance: memcached
      swiftRing:
        ringReplicas: 1
      swiftStorage:
        replicas: 0
        networkAttachments:
        - storage
        storageClass: local-storage <1>
        storageRequest: 10Gi
      swiftProxy:
        secret: osp-secret
        replicas: 1
        passwordSelectors:
          service: SwiftPassword
        serviceUser: swift
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.80 <2>
              spec:
                type: LoadBalancer
        networkAttachments: <3>
        - storage
'
----
+
<1> Must match the {rhos_acro} deployment storage class.
<2> If you use IPv6, change the load balancer IP to the load balancer IP in your environment, for example, `metallb.universe.tf/loadBalancerIPs: fd00:bbbb::80`.
<3> Must match the network attachment for the previous {object_storage} configuration from the {OpenStackShort} deployment.

.Verification

* Inspect the resulting {object_storage} pods:
+
----
$ oc get pods -l component=swift-proxy
----

* Verify that the Object Storage proxy service is registered in the {identity_service_first_ref}:
+
----
$ openstack service list | grep swift
| b5b9b1d3c79241aa867fa2d05f2bbd52 | swift    | object-store |
----
+
----
$ openstack endpoint list | grep swift
| 32ee4bd555414ab48f2dc90a19e1bcd5 | regionOne | swift        | object-store | True    | public    | https://swift-public-openstack.apps-crc.testing/v1/AUTH_%(tenant_id)s |
| db4b8547d3ae4e7999154b203c6a5bed | regionOne | swift        | object-store | True    | internal  | http://swift-internal.openstack.svc:8080/v1/AUTH_%(tenant_id)s        |
----

* Verify that you are able to upload and download objects:
+
----
$ openstack container create test
+---------------------------------------+-----------+------------------------------------+
| account                               | container | x-trans-id                         |
+---------------------------------------+-----------+------------------------------------+
| AUTH_4d9be0a9193e4577820d187acdd2714a | test      | txe5f9a10ce21e4cddad473-0065ce41b9 |
+---------------------------------------+-----------+------------------------------------+

$ openstack object create test --name obj <(echo "Hello World!")
+--------+-----------+----------------------------------+
| object | container | etag                             |
+--------+-----------+----------------------------------+
| obj    | test      | d41d8cd98f00b204e9800998ecf8427e |
+--------+-----------+----------------------------------+

$ openstack object save test obj --file -
Hello World!
----

[NOTE]
The Object Storage data is still stored on the existing {OpenStackShort} nodes. For more information about migrating the actual data from the {OpenStackShort} deployment to the {rhos_acro} deployment, see xref:migrating-object-storage-data-to-rhoso-nodes_migrate-object-storage-service[Migrating the {object_storage_first_ref} data from {OpenStackShort} to {rhos_long} nodes].
