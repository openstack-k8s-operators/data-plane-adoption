[id="adopting-the-image-service_{context}"]

//:context: adopting-image-service
//kgilliga: This module might be converted to an assembly, or a procedure as a standalone chapter.
//Check xref context.

= Adopting the Image service

Adopting Glance means that an existing `OpenStackControlPlane` CR, where Glance
is supposed to be disabled, should be patched to start the service with the
configuration parameters provided by the source environment.

When the procedure is over, the expectation is to see the `GlanceAPI` service
up and running: the `Keystone endpoints` should be updated and the same backend
of the source Cloud will be available. If the conditions above are met, the
adoption is considered concluded.

This guide also assumes that:

. A `TripleO` environment (the source Cloud) is running on one side;
. A `SNO` / `CodeReadyContainers` is running on the other side;
. (optional) an internal/external `Ceph` cluster is reachable by both `crc` and
`TripleO`

== Prerequisites

* Previous Adoption steps completed. Notably, MariaDB, Keystone and Barbican
should be already adopted.

== Procedure - Glance adoption

As already done for https://github.com/openstack-k8s-operators/data-plane-adoption/blob/main/keystone_adoption.md[Keystone], the Glance Adoption follows the same pattern.

=== Using Swift backend

When Glance is deployed with Swift as a backend in the source environment based
on TripleO, the podified `glanceAPI` instance should be deployed with the following
configuration:

----
..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:swift
          [glance_store]
          default_backend = default_backend
          [default_backend]
          swift_store_create_container_on_put = True
          swift_store_auth_version = 3
          swift_store_auth_address = {{ .KeystoneInternalURL }}
          swift_store_endpoint_type = internalURL
          swift_store_user = service:glance
          swift_store_key = {{ .ServicePassword }}
----

It is recommended to write the patch manifest into a file, for example `glance_swift.patch`.
For example, the Glance deployment with a Swift backend would look like this:

----
spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      storageClass: "local-storage"
      storageRequest: 10G
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:swift
          [glance_store]
          default_backend = default_backend
          [default_backend]
          swift_store_create_container_on_put = True
          swift_store_auth_version = 3
          swift_store_auth_address = {{ .KeystoneInternalURL }}
          swift_store_endpoint_type = internalURL
          swift_store_user = service:glance
          swift_store_key = {{ .ServicePassword }}
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
          networkAttachments:
            - storage
----

Having `Swift` as a backend establishes a dependency between the two services,
and any deployed `GlanceAPI` instance would **not work** if `Glance` is
configured with `Swift` that is still not available in the `OpenStackControlPlane`.
Once Swift, and in particular `SwiftProxy`, has been adopted through the
procedure described in <<adopting-object-storage-service>>, it is possible
to proceed with the `GlanceAPI` adoption.

Verify that `SwiftProxy` is available with the following command:

----
$ oc get pod -l component=swift-proxy | grep Running
swift-proxy-75cb47f65-92rxq   3/3     Running   0
----

If the output is similar to the above, it is possible to move forward and patch
the `GlanceAPI` service deployed in the podified context with the following
command:

----
oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_swift.patch
----

=== Using Cinder backend

When Glance is deployed with Cinder as a backend in the source environment based
on TripleO, the podified `glanceAPI` instance should be deployed with the following
configuration:

----
..
spec
  glance:
   ...
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:cinder
          [glance_store]
          default_backend = default_backend
          [default_backend]
          rootwrap_config = /etc/glance/rootwrap.conf
          description = Default cinder backend
          cinder_store_auth_address = {{ .KeystoneInternalURL }}
          cinder_store_user_name = {{ .ServiceUser }}
          cinder_store_password = {{ .ServicePassword }}
          cinder_store_project_name = service
          cinder_catalog_info = volumev3::internalURL
          cinder_use_multipath = true
----

It is recommended to write the patch manifest into a file, for example `glance_cinder.patch`.
For example, the Glance deployment with a Cinder backend would look like this:

----
spec:
  glance:
    enabled: true
    apiOverride:
      route: {}
    template:
      databaseInstance: openstack
      storageClass: "local-storage"
      storageRequest: 10G
      customServiceConfig: |
          [DEFAULT]
          enabled_backends = default_backend:cinder
          [glance_store]
          default_backend = default_backend
          [default_backend]
          rootwrap_config = /etc/glance/rootwrap.conf
          description = Default cinder backend
          cinder_store_auth_address = {{ .KeystoneInternalURL }}
          cinder_store_user_name = {{ .ServiceUser }}
          cinder_store_password = {{ .ServicePassword }}
          cinder_store_project_name = service
          cinder_catalog_info = volumev3::internalURL
          cinder_use_multipath = true
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
          networkAttachments:
            - storage
----

Having `Cinder` as a backend establishes a dependency between the two services,
and any deployed `GlanceAPI` instance would **not work** if `Glance` is
configured with `Cinder` that is still not available in the `OpenStackControlPlane`.
Once Cinder, and in particular `CinderVolume`, has been adopted through the
procedure described in <<adopting-the-block-storage-service>>, it is possible
to proceed with the `GlanceAPI` adoption.

Verify that `CinderVolume` is available with the following command:

----
$ oc get pod -l component=cinder-volume | grep Running
cinder-volume-75cb47f65-92rxq   3/3     Running   0
----

If the output is similar to the above, it is possible to move forward and patch
the `GlanceAPI` service deployed in the podified context with the following
command:

----
oc patch openstackcontrolplane openstack --type=merge --patch-file=glance_cinder.patch
----

=== Using NFS backend

When the source Cloud based on TripleO uses Glance with a NFS backend, before
patching the OpenStackControlPlane to deploy Glance it is important to validate
a few networking related prerequisites.
In the source cloud, verify the NFS parameters used by the overcloud to configure
the Glance backend.
In particular, find among the TripleO heat templates the following variables that are usually an override of the default content provided by
`/usr/share/openstack-tripleo-heat-templates/environments/storage/glance-nfs.yaml`[glance-nfs.yaml].:

---

**GlanceBackend**: file

**GlanceNfsEnabled**: true

**GlanceNfsShare**: 192.168.24.1:/var/nfs

---

In the example above, as the first variable shows, unlike Cinder, Glance has no
notion of NFS backend: the `File` driver is used in this scenario, and behind the
scenes, the `filesystem_store_datadir` which usually points to `/var/lib/glance/images/`
is mapped to the export value provided by the `GlanceNfsShare` variable.
If the `GlanceNfsShare` is not exported through a network that is supposed to be
propagated to the adopted `OpenStack` control plane, an extra action is required
by the human administrator, who must stop the `nfs-server` and remap the export
to the `storage` network. This action usually happens when the Glance service is
stopped in the source controller nodes.
In the podified control plane, as per the
(https://github.com/openstack-k8s-operators/docs/blob/main/images/network_diagram.jpg)[network isolation diagram],
Glance is attached to the Storage network, propagated via the associated
`NetworkAttachmentsDefinition` CR, and the resulting Pods have already the right
permissions to handle the Image Service traffic through this network.
In a deployed OpenStack control plane, you can verify that the network mapping
matches with what has been deployed in the TripleO based environment by checking
both the `NodeNetworkConfigPolicy` (`nncp`) and the `NetworkAttachmentDefinition`
(`net-attach-def`) with the following commands:

```
$ oc get nncp
NAME                        STATUS      REASON
enp6s0-crc-8cf2w-master-0   Available   SuccessfullyConfigured

$ oc get net-attach-def
NAME
ctlplane
internalapi
storage
tenant

$ oc get ipaddresspool -n metallb-system
NAME          AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES
ctlplane      true          false             ["192.168.122.80-192.168.122.90"]
internalapi   true          false             ["172.17.0.80-172.17.0.90"]
storage       true          false             ["172.18.0.80-172.18.0.90"]
tenant        true          false             ["172.19.0.80-172.19.0.90"]
```

The above represents an example of the output that should be checked in the
openshift environment to make sure there are no issues with the propagated
networks.

The following steps assume that:

1. the Storage network has been propagated to the openstack control plane
2. Glance is able to reach the Storage network and connect to the nfs-server
   through the port `2049`.

If the above conditions are met, it is possible to adopt the `Glance` service
and create a new `default` `GlanceAPI` instance connected with the existing
NFS share.

----
cat << EOF > glance_nfs_patch.yaml

spec:
  extraMounts:
  - extraVol:
    - extraVolType: Nfs
      mounts:
      - mountPath: /var/lib/glance/images
        name: nfs
      propagation:
      - Glance
      volumes:
      - name: nfs
        nfs:
          path: /var/nfs
          server: 172.17.3.20
    name: r1
    region: r1
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
         [DEFAULT]
         enabled_backends = default_backend:file
         [glance_store]
         default_backend = default_backend
         [default_backend]
         filesystem_store_datadir = /var/lib/glance/images/
      storageClass: "local-storage"
      storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 1
          type: single
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
          networkAttachments:
          - storage
EOF
----

*Note*:

Replace in `glance_nfs_patch.yaml` the `nfs/server` ip address with the IP used
to reach the `nfs-server` and make sure the `nfs/path` points to the exported
path in the `nfs-server`.

Patch OpenStackControlPlane to deploy Glance with a NFS backend:

----
oc patch openstackcontrolplane openstack --type=merge --patch-file glance_nfs_patch.yaml
----

When GlanceAPI is active, you can see a single API instance:

```
$ oc get pods -l service=glance
NAME                      READY   STATUS    RESTARTS
glance-default-single-0   3/3     Running   0
```

and the description of the pod must report:

```
Mounts:
...
  nfs:
    Type:      NFS (an NFS mount that lasts the lifetime of a pod)
    Server:    {{ server ip address }}
    Path:      {{ nfs export path }}
    ReadOnly:  false
...
```

It is also possible to double check the mountpoint by running the following:

```
oc rsh -c glance-api glance-default-single-0

sh-5.1# mount
...
...
{{ ip address }}:/var/nfs on /var/lib/glance/images type nfs4 (rw,relatime,vers=4.2,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.5,local_lock=none,addr=172.18.0.5)
...
...
```

You can run an `openstack image create` command and double check, on the NFS
node, the uuid has been created in the exported directory.

For example:

```
$ oc rsh openstackclient
$ openstack image list

sh-5.1$  curl -L -o /tmp/cirros-0.5.2-x86_64-disk.img http://download.cirros-cloud.net/0.5.2/cirros-0.5.2-x86_64-disk.img
...
...

sh-5.1$ openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.5.2-x86_64-disk.img cirros
...
...

sh-5.1$ openstack image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 634482ca-4002-4a6d-b1d5-64502ad02630 | cirros | active |
+--------------------------------------+--------+--------+
```

On the nfs-server node, the same `uuid` is in the exported `/var/nfs`:

```
$ ls /var/nfs/
634482ca-4002-4a6d-b1d5-64502ad02630
```

=== Using Ceph storage backend

If a Ceph backend is used, the `customServiceConfig` parameter should
be used to inject the right configuration to the `GlanceAPI` instance.

Make sure the Ceph-related secret (`ceph-conf-files`) was created in
the `openstack` namespace and that the `extraMounts` property of the
`OpenStackControlPlane` CR has been configured properly. These tasks
are described in an earlier Adoption step xref:configuring-a-ceph-backend_{context}[Configuring a Ceph backend].

----
cat << EOF > glance_patch.yaml
spec:
  glance:
    enabled: true
    template:
      databaseInstance: openstack
      customServiceConfig: |
        [DEFAULT]
        enabled_backends=default_backend:rbd
        [glance_store]
        default_backend=default_backend
        [default_backend]
        rbd_store_ceph_conf=/etc/ceph/ceph.conf
        rbd_store_user=openstack
        rbd_store_pool=images
        store_description=Ceph glance store backend.
      storageClass: "local-storage"
      storageRequest: 10G
      glanceAPIs:
        default:
          replicas: 1
          override:
            service:
              internal:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.80
              spec:
                type: LoadBalancer
          networkAttachments:
          - storage
EOF
----

If you have previously backup your OpenStack services configuration file from the old environment:
xref:pulling-the-openstack-configuration_{context}[Pulling the OpenStack configuration] you can use os-diff to compare and make sure the configuration is correct.

----
pushd os-diff
./os-diff cdiff --service glance -c /tmp/collect_tripleo_configs/glance/etc/glance/glance-api.conf -o glance_patch.yaml
----

This will produce the difference between both ini configuration files.

Patch OpenStackControlPlane to deploy Glance with Ceph backend:

----
oc patch openstackcontrolplane openstack --type=merge --patch-file glance_patch.yaml
----

== Post-checks

=== Test the glance service from the OpenStack CLI


You can compare and make sure the configuration has been correctly applied to the glance pods by running

----
./os-diff cdiff --service glance -c /etc/glance/glance.conf.d/02-config.conf  -o glance_patch.yaml --frompod -p glance-api
----

If no line appear, then the configuration is correctly done.

Inspect the resulting glance pods:

----
GLANCE_POD=`oc get pod |grep glance-default-external-0 | cut -f 1 -d' '`
oc exec -t $GLANCE_POD -c glance-api -- cat /etc/glance/glance.conf.d/02-config.conf

[DEFAULT]
enabled_backends=default_backend:rbd
[glance_store]
default_backend=default_backend
[default_backend]
rbd_store_ceph_conf=/etc/ceph/ceph.conf
rbd_store_user=openstack
rbd_store_pool=images
store_description=Ceph glance store backend.

oc exec -t $GLANCE_POD -c glance-api -- ls /etc/ceph
ceph.client.openstack.keyring
ceph.conf
----

Ceph secrets are properly mounted, at this point let's move to the OpenStack
CLI and check the service is active and the endpoints are properly updated.

----
(openstack)$ service list | grep image

| fc52dbffef36434d906eeb99adfc6186 | glance    | image        |

(openstack)$ endpoint list | grep image

| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |
| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |
| 709859219bc24ab9ac548eab74ad4dd5 | regionOne | glance       | image        | True    | admin     | http://glance-admin-openstack.apps-crc.testing      |
----

Check that the images that you previously listed in the source Cloud are available in the adopted service:

----
(openstack)$ image list
+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |
+--------------------------------------+--------+--------+
----

=== Image upload

You can test that an image can be created on the adopted service.

----
(openstack)$ alias openstack="oc exec -t openstackclient -- openstack"
(openstack)$ curl -L -o /tmp/cirros-0.5.2-x86_64-disk.img http://download.cirros-cloud.net/0.5.2/cirros-0.5.2-x86_64-disk.img
    qemu-img convert -O raw /tmp/cirros-0.5.2-x86_64-disk.img /tmp/cirros-0.5.2-x86_64-disk.img.raw
    openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.5.2-x86_64-disk.img.raw cirros2
    openstack image list
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   273  100   273    0     0   1525      0 --:--:-- --:--:-- --:--:--  1533
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 15.5M  100 15.5M    0     0  17.4M      0 --:--:-- --:--:-- --:--:-- 17.4M

+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+
| Field            | Value                                                                                                                                      |
+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+
| container_format | bare                                                                                                                                       |
| created_at       | 2023-01-31T21:12:56Z                                                                                                                       |
| disk_format      | raw                                                                                                                                        |
| file             | /v2/images/46a3eac1-7224-40bc-9083-f2f0cd122ba4/file                                                                                       |
| id               | 46a3eac1-7224-40bc-9083-f2f0cd122ba4                                                                                                       |
| min_disk         | 0                                                                                                                                          |
| min_ram          | 0                                                                                                                                          |
| name             | cirros                                                                                                                                     |
| owner            | 9f7e8fdc50f34b658cfaee9c48e5e12d                                                                                                           |
| properties       | os_hidden='False', owner_specified.openstack.md5='', owner_specified.openstack.object='images/cirros', owner_specified.openstack.sha256='' |
| protected        | False                                                                                                                                      |
| schema           | /v2/schemas/image                                                                                                                          |
| status           | queued                                                                                                                                     |
| tags             |                                                                                                                                            |
| updated_at       | 2023-01-31T21:12:56Z                                                                                                                       |
| visibility       | shared                                                                                                                                     |
+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+

+--------------------------------------+--------+--------+
| ID                                   | Name   | Status |
+--------------------------------------+--------+--------+
| 46a3eac1-7224-40bc-9083-f2f0cd122ba4 | cirros2| active |
| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |
+--------------------------------------+--------+--------+


(openstack)$ oc rsh ceph
sh-4.4$ ceph -s
r  cluster:
    id:     432d9a34-9cee-4109-b705-0c59e8973983
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum a (age 4h)
    mgr: a(active, since 4h)
    osd: 1 osds: 1 up (since 4h), 1 in (since 4h)

  data:
    pools:   5 pools, 160 pgs
    objects: 46 objects, 224 MiB
    usage:   247 MiB used, 6.8 GiB / 7.0 GiB avail
    pgs:     160 active+clean

sh-4.4$ rbd -p images ls
46a3eac1-7224-40bc-9083-f2f0cd122ba4
c3158cad-d50b-452f-bec1-f250562f5c1f
----
