---
- name: set Neutron services shell vars
  no_log: "{{ use_no_log }}"
  ansible.builtin.set_fact:
    neutron_header: |
      alias openstack="oc exec -t openstackclient -- openstack"
      FIP={{ lookup('env', 'FIP') | default('192.168.122.20', True) }}

- name: verify that neutron-ovn-metadata-agent is alive
  ansible.builtin.shell: |
    {{ shell_header }}
    {{ neutron_header }}
    ${BASH_ALIASES[openstack]} network agent list | grep -F 'neutron-ovn-metadata-agent' | grep -qF 'XXX' || echo PASS
  register: neutron_verify_metadata_agent_result
  until: neutron_verify_metadata_agent_result.stdout == 'PASS'
  # NOTE(slaweq): retries should not be needed but it seems there is some minor
  # bug in Neutron which causes reporting ovn-metadata-agent as DOWN in every first API request after deploying it on host.
  retries: 2
  delay: 1

- name: get neutron-sriov-nic-agent alive state
  when: compute_adoption|bool
  ansible.builtin.shell: |
    {{ shell_header }}
    {{ neutron_header }}
    ${BASH_ALIASES[openstack]} network agent list | grep -F 'neutron-sriov-nic-agent' | grep -qF 'XXX' || echo ALIVE
  register: neutron_verify_sriov_nic_agent_result

- name: verify that neutron-sriov-nic-agent is alive
  when: compute_adoption|bool
  ansible.builtin.assert:
    that:
      - neutron_verify_sriov_nic_agent_result.stdout == 'ALIVE'
    fail_msg: "neutron-sriov-nic-agent is DEAD after adoption"
    success_msg: "neutron-sriov-nic-agent is ALIVE after adoption"

- name: get neutron-dhcp-agent alive state
  ansible.builtin.shell: |
    {{ shell_header }}
    {{ neutron_header }}
    ${BASH_ALIASES[openstack]} network agent list | grep -F 'neutron-dhcp-agent' | grep -qF 'XXX' || echo ALIVE
  register: neutron_verify_dhcp_agent_result

- name: verify that neutron-dhcp-agent is alive
  ansible.builtin.assert:
    that:
      - neutron_verify_dhcp_agent_result.stdout == 'ALIVE'
    fail_msg: "neutron-dhcp-agent is DEAD after adoption"
    success_msg: "neutron-dhcp-agent is ALIVE after adoption"

- name: verify connectivity to the existing test VM instance using Floating IP
  when: prelaunch_test_instance|bool
  ansible.builtin.shell: |
      ping -c4 {{ lookup('env', 'FIP') | default('192.168.122.20', True) }}

- name: tobiko validate pings, create workloads
  when: tobiko_qe_test | default('false') | bool
  environment:
    OS_CLOUD_IP: "{{ standalone_ip | default(edpm_node_ip) }}"
  block:
    # Temporal - tobiko installation task
    - name: tobiko installation
      ansible.builtin.shell: |
          ssh ${OS_CLOUD_IP} "set -o pipefail && mkdir -p ~/src/x && cd ~/src/x && git clone https://github.com/redhat-openstack/tobiko.git"

    - name: oc undercloud installation
      ansible.builtin.shell: >
          ssh ${OS_CLOUD_IP} "curl -s -L https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-client-linux.tar.gz
          | sudo tar -zxvf - -C /usr/local/bin/"

    - name: copy kube conf to undercloud
      ansible.builtin.shell: |
        scp -r ~/.kube/ ${OS_CLOUD_IP}:~

    - name: upload tobiko-playbook.yaml to the undercloud
      delegate_to: "{{ standalone_ip | default(edpm_node_ip) }}"
      vars:
        tobiko_playbook: |
          - name: Playbook to run tobiko from the undercloud pre-adoption
            hosts: localhost
            tasks:
              - name: "run tests"
                ansible.builtin.include_role:
                  name: tobiko-run
      ansible.builtin.copy:
        mode: a+r
        content: "{{ tobiko_playbook }}"
        dest: ~/src/x/tobiko/tobiko-playbook.yaml

    - name: Add tobiko.conf to the undercloud
      delegate_to: "{{ standalone_ip | default(edpm_node_ip) }}"
      vars:
        tobiko_conf_file: |
          [DEFAULT]
          log_dir = /home/zuul/src/x/tobiko/report
          log_file = tobiko.log
          debug = true
          [testcase]
          test_runner_timeout = 14400.0
          timeout = 1800.0
          [advanced_vm]
          image_url = "https://softwarefactory-project.io/ubuntu-minimal-customized-enp3s0"
          username: ubuntu
          [tripleo]
          undercloud_ssh_hostname = undercloud
          undercloud_ssh_username = zuul
          run_background_ping_in_pod = true
          [keystone]
          interface = public
          [neutron]
          custom_mtu_size = 1292
      ansible.builtin.copy:
        mode: a+r
        content: "{{ tobiko_conf_file }}"
        dest: ~/src/x/tobiko/tobiko.conf

    - name: Run Tobiko from the undercloud
      delegate_to: "{{ standalone_ip | default(edpm_node_ip) }}"
      ansible.builtin.shell:
        chdir: ~/src/x/tobiko/
        cmd: ansible-playbook tobiko-playbook.yaml -e test_workflow=create-resources -e pytest_addopts_global="--skipregex='TestFloatingIPLogging|ExtraDhcpOptsPortLoggingTest|NoFipPortTest|StatelessSecurityGroupInstanceTest|StatelessSecurityGroupTest|migrate_server|qos|create_share|extend_share'"

- name: validate pings, new workloads after adoption and cleanup
  when: neutron_qe_test | default('false') | bool
  environment:
    OS_CLOUD_IP: "{{ standalone_ip | default(edpm_node_ip) }}"
  block:
    - name: stop pinger
      ansible.builtin.shell: |
          ssh ${OS_CLOUD_IP} "echo 'exit' > ~/adoption/_pinger_cmd.txt </dev/null"

    - name: get validation pinger logs
      ansible.builtin.shell: |
          ssh ${OS_CLOUD_IP} "set -o pipefail && chmod 744 {{ neutron_qe_dir }}/validate_pinger.sh && \
          {{ neutron_qe_dir }}/validate_pinger.sh > {{ neutron_qe_dir }}/validate_pinger.sh.log 2>&1"

    - name: get validation pinger result
      ansible.builtin.shell: |
          ssh ${OS_CLOUD_IP} "set -o pipefail && grep -r 'res=1' {{ neutron_qe_dir }}/validate_pinger.sh.log"
      register: validation_pinger_result
      ignore_errors: true

    - name: verify pinger result
      ansible.builtin.assert:
        that:
          - validation_pinger_result.rc == 1
        fail_msg: "fail ping"
        success_msg: "success ping"

    - name: create workloads after adoption
      ansible.builtin.shell: |
          {{ shell_header }}
          set -o pipefail && chmod 744 {{ neutron_qe_dir }}/create_resources_after.sh && \
          {{ neutron_qe_dir }}/create_resources_after.sh > {{ neutron_qe_dir }}/create_resources_after.sh.log 2>&1

    - name: reboot workloads after adoption
      ansible.builtin.shell: |
          {{ shell_header }}
          set -o pipefail && chmod 744 {{ neutron_qe_dir }}/validate-workload-operations.sh && \
          export resources_type=granular_poc validate_vm_reboot_p=True validate_vm_shelve_p=True && \
          {{ neutron_qe_dir }}/validate-workload-operations.sh reboot > {{ neutron_qe_dir }}/validate-workload-operations_reboot.log 2>&1

    - name: get validation result_reboot
      ansible.builtin.shell: |
          set -o pipefail && grep -r 'fail' ~/validations_after/connectivity_summary_file_reboot
      register: validation_result_reboot
      ignore_errors: true

    - name: verify result_reboot
      ansible.builtin.assert:
        that:
          - validation_result_reboot.rc == 1
        fail_msg: "fail ping"
        success_msg: "success ping"

    - name: migration workloads after adoption
      ansible.builtin.shell: |
          {{ shell_header }}
          set -o pipefail && unset validate_vm_reboot_p validate_vm_shelve_p && \
          export resources_type=granular_poc ping_during_vm_migration=True validate_vm_migration_p=True validate_vm_live_migration_p=True validate_vm_cold_migration_p=True && \
          {{ neutron_qe_dir }}/validate-workload-operations.sh migration > {{ neutron_qe_dir }}/validate-workload-operations_migration.log 2>&1

    - name: get validation result_migration
      ansible.builtin.shell: |
          set -o pipefail && grep -r 'fail' ~/validations_after/connectivity_summary_file_migration
      register: validation_result_migration
      ignore_errors: true

    - name: verify result migration
      ansible.builtin.assert:
        that:
          - validation_result_migration.rc == 1
        fail_msg: "fail ping"
        success_msg: "success ping"

    - name: cleanup workloads after adoption
      ansible.builtin.shell: |
          {{ shell_header }}
          set -o pipefail && chmod 744 {{ neutron_qe_dir }}/delete-resources.sh && \
          {{ neutron_qe_dir }}/delete-resources.sh > {{ neutron_qe_dir }}/delete-resources.sh.log 2>&1
